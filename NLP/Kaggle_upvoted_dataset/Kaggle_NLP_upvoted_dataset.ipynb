{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_NLP_upvoted_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRl5hpLiB7dd6CCcOu6M5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/Kaggle_NLP_upvoted_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE3HCPOVUhjz",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/canggih/voted-kaggle-dataset/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-vTIH-OUbRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz28thIKUhzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "fc383b41-dcee-4292-92a8-c6e325876aa8"
      },
      "source": [
        "data=pd.read_csv('https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/voted-kaggle-dataset.csv?raw=true')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Subtitle</th>\n",
              "      <th>Owner</th>\n",
              "      <th>Votes</th>\n",
              "      <th>Versions</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Data Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>License</th>\n",
              "      <th>Views</th>\n",
              "      <th>Download</th>\n",
              "      <th>Kernels</th>\n",
              "      <th>Topics</th>\n",
              "      <th>URL</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Credit Card Fraud Detection</td>\n",
              "      <td>Anonymized credit card transactions labeled as...</td>\n",
              "      <td>Machine Learning Group - ULB</td>\n",
              "      <td>1241</td>\n",
              "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
              "      <td>crime\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>442,136 views</td>\n",
              "      <td>53,128 downloads</td>\n",
              "      <td>1,782 kernels</td>\n",
              "      <td>26 topics</td>\n",
              "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
              "      <td>The datasets contains transactions made by cre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>European Soccer Database</td>\n",
              "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
              "      <td>Hugo Mathien</td>\n",
              "      <td>1046</td>\n",
              "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
              "      <td>association football\\neurope</td>\n",
              "      <td>SQLite</td>\n",
              "      <td>299 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>396,214 views</td>\n",
              "      <td>46,367 downloads</td>\n",
              "      <td>1,459 kernels</td>\n",
              "      <td>75 topics</td>\n",
              "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
              "      <td>The ultimate Soccer database for data analysis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TMDB 5000 Movie Dataset</td>\n",
              "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
              "      <td>The Movie Database (TMDb)</td>\n",
              "      <td>1024</td>\n",
              "      <td>Version 2,2017-09-28</td>\n",
              "      <td>film</td>\n",
              "      <td>CSV</td>\n",
              "      <td>44 MB</td>\n",
              "      <td>Other</td>\n",
              "      <td>446,255 views</td>\n",
              "      <td>62,002 downloads</td>\n",
              "      <td>1,394 kernels</td>\n",
              "      <td>46 topics</td>\n",
              "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
              "      <td>Background\\nWhat can we say about the success ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Global Terrorism Database</td>\n",
              "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
              "      <td>START Consortium</td>\n",
              "      <td>789</td>\n",
              "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
              "      <td>crime\\nterrorism\\ninternational relations</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>Other</td>\n",
              "      <td>187,877 views</td>\n",
              "      <td>26,309 downloads</td>\n",
              "      <td>608 kernels</td>\n",
              "      <td>11 topics</td>\n",
              "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
              "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bitcoin Historical Data</td>\n",
              "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
              "      <td>Zielak</td>\n",
              "      <td>618</td>\n",
              "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
              "      <td>history\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>119 MB</td>\n",
              "      <td>CC4</td>\n",
              "      <td>146,734 views</td>\n",
              "      <td>16,868 downloads</td>\n",
              "      <td>68 kernels</td>\n",
              "      <td>13 topics</td>\n",
              "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
              "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Title  ...                                        Description\n",
              "0  Credit Card Fraud Detection  ...  The datasets contains transactions made by cre...\n",
              "1     European Soccer Database  ...  The ultimate Soccer database for data analysis...\n",
              "2      TMDB 5000 Movie Dataset  ...  Background\\nWhat can we say about the success ...\n",
              "3    Global Terrorism Database  ...  Context\\nInformation on more than 170,000 Terr...\n",
              "4      Bitcoin Historical Data  ...  Context\\nBitcoin is the longest running and mo...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEVdPq3QU96N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "797720c0-39cf-454f-d95e-379a03955b92"
      },
      "source": [
        "data.Description[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lzO1bYxV9p3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "383492f9-079b-4257-dac0-6fa602d515e2"
      },
      "source": [
        "data.Description[1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlayers and Teams\\' attributes* sourced from EA Sports\\' FIFA video game series, including the weekly updates\\nTeam line up with squad formation (X, Y coordinates)\\nBetting odds from up to 10 providers\\nDetailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\\n*16th Oct 2016: New table containing teams\\' attributes from FIFA !\\nOriginal Data Source:\\nYou can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from:\\nhttp://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events\\nhttp://www.football-data.co.uk/ : betting odds. Click here to understand the column naming system for betting odds:\\nhttp://sofifa.com/ : players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.\\nWhen you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. I have called those foreign keys \"api_id\".\\nImproving the dataset:\\nYou will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games, national cups, Champion\\'s League and Europa League. Please ask me if you\\'re after a specific tournament.\\nPlease get in touch with me if you want to help improve this dataset.\\nCLICK HERE TO ACCESS THE PROJECT GITHUB\\nImportant note for people interested in using the crawlers: since I first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players (\\'Player Spider\\') will not work until i\\'ve updated it.\\nExploring the data:\\nNow that\\'s the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:\\nThe Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I\\'ve achieved so far using my own SVM. Though it may sound high for such a random sport game, you\\'ve got to know that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.\\nProbabilities vs Odds\\nWhen running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.\\nExplore and visualize features\\nWith access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows, Guardiola himself may hire one of you some day!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhh0YWlxXLKG",
        "colab_type": "text"
      },
      "source": [
        "# Ch06b - Topic Modeling with gensim.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPj-D9NXXXiW",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgVxQFhcXhL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "89604aa2-14bc-48d7-9afd-25237ddda1ee"
      },
      "source": [
        "papers=data.Description\n",
        "print(len(papers))\n",
        "papers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       The datasets contains transactions made by cre...\n",
              "1       The ultimate Soccer database for data analysis...\n",
              "2       Background\\nWhat can we say about the success ...\n",
              "3       Context\\nInformation on more than 170,000 Terr...\n",
              "4       Context\\nBitcoin is the longest running and mo...\n",
              "                              ...                        \n",
              "2145    Context\\nFortnite: Battle Royale has over 20 m...\n",
              "2146    Context\\nThis dataset provides the nationaliti...\n",
              "2147    lem.json\\nThis file contains lementized englis...\n",
              "2148    Context\\nThis data set contains weather data f...\n",
              "2149    Context\\nBirths in U.S during 1994 to 2003.\\nC...\n",
              "Name: Description, Length: 2150, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyQh58zWYHTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "papers=papers.astype(str)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7frNCl40WDfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "42ec6598-b331-4987-f647-236f2f13928a"
      },
      "source": [
        "%%time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(papers):\n",
        "    norm_papers = []\n",
        "    for paper in papers:\n",
        "        paper = paper.lower()\n",
        "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "        paper_tokens = list(filter(None, paper_tokens))\n",
        "        if paper_tokens:\n",
        "            norm_papers.append(paper_tokens)\n",
        "            \n",
        "    return norm_papers\n",
        "    \n",
        "norm_papers = normalize_corpus(papers)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "CPU times: user 3.14 s, sys: 33.9 ms, total: 3.17 s\n",
            "Wall time: 3.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqy2Zt0VXnxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "896cca7c-7482-4632-d971-9afc9b0754f4"
      },
      "source": [
        "print(papers[0])\n",
        "print(norm_papers[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
            "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
            "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
            "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
            "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze66dZ44YlUq",
        "colab_type": "text"
      },
      "source": [
        "## Text Representation with Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO6346G5XpN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') \n",
        "# higher threshold fewer phrases.\n",
        "bigram_model = gensim.models.phrases.Phraser(bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hupfI97NYyY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "412accac-80aa-4f6f-9c39-53b0b40cfa1d"
      },
      "source": [
        "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
        "print(norm_corpus_bigrams[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine_learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please_cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C01Gv8vBZBrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "# Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
        "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHKgFEkuZNoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e50dcada-ffe8-4cbd-f9ca-6cbb94433965"
      },
      "source": [
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, 'ac'), (1, 'account'), (2, 'accuracy'), (3, 'amount'), (4, 'analysed'), (5, 'andrea'), (6, 'area'), (7, 'artml'), (8, 'auprc'), (9, 'available'), (10, 'background'), (11, 'big'), (12, 'bontempi'), (13, 'brufence'), (14, 'bruxelles')]\n",
            "Total Vocabulary Size: 32097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX-lAwjdZPqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "881b100c-30b0-4b6c-9c43-9922692392ca"
      },
      "source": [
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, 'ac'), (1, 'account'), (2, 'accuracy'), (3, 'amount'), (4, 'area'), (5, 'available'), (6, 'background'), (7, 'big'), (8, 'cannot'), (9, 'card'), (10, 'case'), (11, 'class'), (12, 'classification'), (13, 'collected'), (14, 'component')]\n",
            "Total Vocabulary Size: 1821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BufKRboIZVoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "98096c4e-0d0b-4149-9169-3cdfbafbe967"
      },
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
        "print(bow_corpus[0][:50])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 3), (1, 1), (2, 2), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 3), (12, 2), (13, 1), (14, 1), (15, 1), (16, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 7), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbnvw8ROZpX9",
        "colab_type": "text"
      },
      "source": [
        "doc2bow: Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method.\n",
        "\n",
        "If allow_update is set, then also update dictionary in the process: create ids for new words. At the same time, update document frequencies -- for each word appearing in this document, increase its document frequency (self.dfs) by one.\n",
        "\n",
        "If allow_update is not set, this function is const, aka read-only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBqSH8EsZe3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7fd6bd05-cf67-4f3d-e034-fbd7d33e0e3c"
      },
      "source": [
        "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('card', 1), ('case', 1), ('class', 2), ('day', 1), ('european', 1), ('feature', 1), ('first', 1), ('ha', 3), ('http', 2), ('machine_learning', 1), ('original', 2), ('probability', 2), ('project', 1), ('time', 2), ('using', 2), ('value', 1), ('able', 2), ('access', 2), ('achieved', 1), ('across', 1), ('adding', 1), ('algorithm', 1), ('also', 3), ('analysis', 1), ('api', 1), ('appears', 1), ('ask', 1), ('asset', 1), ('attribute', 5), ('away', 1), ('base', 1), ('called', 1), ('changed', 1), ('classifier', 1), ('click', 2), ('co', 1), ('collection', 1), ('column', 1), ('com', 3), ('come', 1), ('commercial', 1), ('compare', 1), ('containing', 1), ('coordinate', 1), ('could', 1), ('country', 1), ('crawling', 2), ('cross', 1), ('database', 2), ('design', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZhG9ywZt_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3417334-7cd0-460d-a962-b77cdad9c47f"
      },
      "source": [
        "print('Total number of papers:', len(bow_corpus))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of papers: 2150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksCjrdIwZxgZ",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meXt3wwNZvr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "360a35f7-7b0e-4250-b0e2-2e98ad64a929"
      },
      "source": [
        "%%time\n",
        "TOTAL_TOPICS = 10\n",
        "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
        "                                 onepass=True, chunksize=1740, power_iters=1000)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 51s, sys: 47.4 s, total: 2min 38s\n",
            "Wall time: 1min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-VT5R5Z2AI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "0278f02c-0f13-42c1-cfa5-579f5599d563"
      },
      "source": [
        "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
        "    print('Topic #'+str(topic_id+1)+':')\n",
        "    print(topic)\n",
        "    print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "0.880*\"university\" + 0.124*\"player\" + 0.102*\"number\" + 0.090*\"college\" + 0.090*\"wa\" + 0.088*\"file\" + 0.087*\"year\" + 0.087*\"time\" + 0.079*\"team\" + 0.075*\"csv\" + 0.068*\"one\" + 0.066*\"name\" + 0.065*\"information\" + 0.063*\"ha\" + 0.059*\"date\" + 0.049*\"value\" + 0.046*\"institute\" + 0.045*\"university_california\" + 0.045*\"used\" + 0.045*\"new\"\n",
            "\n",
            "Topic #2:\n",
            "-0.451*\"university\" + 0.295*\"player\" + 0.206*\"number\" + 0.185*\"team\" + 0.177*\"time\" + 0.176*\"year\" + 0.172*\"wa\" + 0.166*\"file\" + 0.146*\"csv\" + 0.134*\"one\" + 0.129*\"name\" + 0.124*\"date\" + 0.122*\"information\" + 0.121*\"ha\" + 0.101*\"goal\" + 0.098*\"value\" + 0.088*\"game\" + 0.086*\"code\" + 0.085*\"used\" + 0.080*\"use\"\n",
            "\n",
            "Topic #3:\n",
            "-0.677*\"player\" + -0.393*\"team\" + -0.237*\"goal\" + -0.152*\"zone\" + 0.135*\"file\" + -0.134*\"game\" + -0.130*\"allowed\" + 0.112*\"csv\" + 0.102*\"year\" + 0.092*\"one\" + 0.087*\"date\" + 0.081*\"information\" + -0.076*\"percentage\" + 0.073*\"name\" + 0.072*\"value\" + -0.068*\"individual\" + -0.065*\"taken\" + -0.065*\"scored\" + -0.060*\"relative\" + 0.058*\"time\"\n",
            "\n",
            "Topic #4:\n",
            "0.603*\"year\" + -0.264*\"date\" + -0.221*\"file\" + 0.187*\"total\" + -0.160*\"one\" + -0.156*\"registration\" + -0.151*\"zero\" + -0.147*\"start\" + -0.139*\"application\" + -0.138*\"time\" + -0.129*\"csv\" + -0.124*\"position\" + 0.123*\"given\" + -0.120*\"containing\" + -0.118*\"numeric\" + 0.107*\"energy\" + -0.098*\"element\" + -0.097*\"text\" + 0.093*\"state\" + -0.092*\"section\"\n",
            "\n",
            "Topic #5:\n",
            "0.633*\"csv\" + -0.292*\"date\" + -0.221*\"year\" + -0.195*\"number\" + 0.183*\"integer\" + -0.160*\"registration\" + -0.157*\"zero\" + -0.144*\"time\" + -0.135*\"start\" + -0.130*\"application\" + -0.117*\"position\" + -0.113*\"code\" + -0.112*\"one\" + 0.110*\"movie\" + -0.109*\"containing\" + 0.099*\"numeric\" + -0.098*\"element\" + -0.091*\"section\" + 0.085*\"text\" + 0.083*\"user\"\n",
            "\n",
            "Topic #6:\n",
            "0.864*\"integer\" + -0.343*\"csv\" + 0.109*\"movie\" + -0.103*\"year\" + 0.092*\"point\" + 0.092*\"people\" + 0.089*\"categorical\" + 0.086*\"always\" + 0.077*\"music\" + 0.063*\"preference\" + 0.063*\"interest\" + 0.059*\"lot\" + 0.056*\"item\" + 0.049*\"enjoy\" + -0.048*\"numeric\" + -0.046*\"player\" + 0.042*\"time\" + 0.041*\"money\" + 0.039*\"life\" + 0.036*\"often\"\n",
            "\n",
            "Topic #7:\n",
            "0.777*\"numeric\" + 0.499*\"text\" + -0.292*\"csv\" + -0.062*\"integer\" + -0.058*\"date\" + 0.058*\"word\" + -0.053*\"file\" + 0.051*\"open\" + 0.044*\"food\" + -0.042*\"time\" + 0.040*\"product\" + 0.036*\"student\" + -0.035*\"one\" + 0.034*\"database\" + -0.034*\"number\" + -0.033*\"registration\" + -0.033*\"movie\" + -0.033*\"zero\" + 0.032*\"collection\" + -0.032*\"position\"\n",
            "\n",
            "Topic #8:\n",
            "-0.512*\"csv\" + -0.363*\"year\" + -0.278*\"numeric\" + -0.276*\"integer\" + -0.165*\"date\" + 0.132*\"image\" + 0.115*\"word\" + 0.109*\"use\" + -0.107*\"total\" + -0.105*\"registration\" + 0.101*\"user\" + -0.100*\"zero\" + 0.093*\"information\" + 0.088*\"language\" + 0.088*\"wa\" + -0.084*\"position\" + -0.084*\"given\" + -0.082*\"application\" + -0.082*\"start\" + -0.081*\"time\"\n",
            "\n",
            "Topic #9:\n",
            "-0.295*\"value\" + -0.282*\"de\" + -0.276*\"name\" + 0.268*\"year\" + 0.257*\"image\" + 0.205*\"file\" + 0.167*\"total\" + -0.166*\"number\" + -0.131*\"fire\" + -0.116*\"event\" + -0.114*\"per\" + -0.114*\"station\" + 0.114*\"word\" + -0.114*\"monitor\" + -0.113*\"en\" + 0.112*\"movie\" + -0.109*\"code\" + -0.108*\"unit\" + -0.105*\"day\" + 0.101*\"one\"\n",
            "\n",
            "Topic #10:\n",
            "-0.594*\"de\" + -0.307*\"en\" + -0.292*\"per\" + -0.195*\"number\" + -0.191*\"le\" + -0.174*\"la\" + -0.174*\"com\" + 0.139*\"value\" + 0.133*\"name\" + 0.125*\"station\" + 0.103*\"fire\" + 0.091*\"event\" + -0.089*\"word\" + 0.087*\"time\" + -0.080*\"http_www\" + 0.077*\"monitor\" + 0.074*\"wa\" + 0.073*\"unit\" + -0.071*\"ha\" + 0.069*\"feature\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnToggzkaZAj",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a moment to understand these results. A brief recap on the LSI model— it is based on the principle that words that are used in the same contexts tend to have similar meanings. You can observe in this output that each topic is a combination of terms (which basically tend to convey an overall sense of the topic) and weights. Now the problem here is that we have both positive and negative weights. What does that mean?\n",
        "\n",
        "Based on existing research and my interpretations, considering we are reducing the dimensionality here to a 10-dimensional space based on the number of topics, the sign on each term indicates a sense of direction or orientation in the vector space for a particular topic. The higher the weight, the more important the contribution. So similar correlated terms have the same sign or direction. Hence, it is perfectly possible for a topic to have two different sub-themes based on the sign or orientation of terms. Let’s separate these terms and try to interpret the topics again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSdChuYwaLmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff001ae1-ef71-4152-a008-406f0f0b1e50"
      },
      "source": [
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('university', 0.88), ('player', 0.124), ('number', 0.102), ('college', 0.09), ('wa', 0.09), ('file', 0.088), ('year', 0.087), ('time', 0.087), ('team', 0.079), ('csv', 0.075), ('one', 0.068), ('name', 0.066), ('information', 0.065), ('ha', 0.063), ('date', 0.059), ('value', 0.049), ('institute', 0.046), ('university_california', 0.045), ('used', 0.045), ('new', 0.045)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.295), ('number', 0.206), ('team', 0.185), ('time', 0.177), ('year', 0.176), ('wa', 0.172), ('file', 0.166), ('csv', 0.146), ('one', 0.134), ('name', 0.129), ('date', 0.124), ('information', 0.122), ('ha', 0.121), ('goal', 0.101), ('value', 0.098), ('game', 0.088), ('code', 0.086), ('used', 0.085), ('use', 0.08)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('university', -0.451)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('file', 0.135), ('csv', 0.112), ('year', 0.102), ('one', 0.092), ('date', 0.087), ('information', 0.081), ('name', 0.073), ('value', 0.072), ('time', 0.058)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('player', -0.677), ('team', -0.393), ('goal', -0.237), ('zone', -0.152), ('game', -0.134), ('allowed', -0.13), ('percentage', -0.076), ('individual', -0.068), ('taken', -0.065), ('scored', -0.065), ('relative', -0.06)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.603), ('total', 0.187), ('given', 0.123), ('energy', 0.107), ('state', 0.093)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('date', -0.264), ('file', -0.221), ('one', -0.16), ('registration', -0.156), ('zero', -0.151), ('start', -0.147), ('application', -0.139), ('time', -0.138), ('csv', -0.129), ('position', -0.124), ('containing', -0.12), ('numeric', -0.118), ('element', -0.098), ('text', -0.097), ('section', -0.092)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('csv', 0.633), ('integer', 0.183), ('movie', 0.11), ('numeric', 0.099), ('text', 0.085), ('user', 0.083)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('date', -0.292), ('year', -0.221), ('number', -0.195), ('registration', -0.16), ('zero', -0.157), ('time', -0.144), ('start', -0.135), ('application', -0.13), ('position', -0.117), ('code', -0.113), ('one', -0.112), ('containing', -0.109), ('element', -0.098), ('section', -0.091)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('integer', 0.864), ('movie', 0.109), ('point', 0.092), ('people', 0.092), ('categorical', 0.089), ('always', 0.086), ('music', 0.077), ('preference', 0.063), ('interest', 0.063), ('lot', 0.059), ('item', 0.056), ('enjoy', 0.049), ('time', 0.042), ('money', 0.041), ('life', 0.039), ('often', 0.036)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.343), ('year', -0.103), ('numeric', -0.048), ('player', -0.046)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.777), ('text', 0.499), ('word', 0.058), ('open', 0.051), ('food', 0.044), ('product', 0.04), ('student', 0.036), ('database', 0.034), ('collection', 0.032)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.292), ('integer', -0.062), ('date', -0.058), ('file', -0.053), ('time', -0.042), ('one', -0.035), ('number', -0.034), ('registration', -0.033), ('movie', -0.033), ('zero', -0.033), ('position', -0.032)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.132), ('word', 0.115), ('use', 0.109), ('user', 0.101), ('information', 0.093), ('language', 0.088), ('wa', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.512), ('year', -0.363), ('numeric', -0.278), ('integer', -0.276), ('date', -0.165), ('total', -0.107), ('registration', -0.105), ('zero', -0.1), ('position', -0.084), ('given', -0.084), ('application', -0.082), ('start', -0.082), ('time', -0.081)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.268), ('image', 0.257), ('file', 0.205), ('total', 0.167), ('word', 0.114), ('movie', 0.112), ('one', 0.101)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('value', -0.295), ('de', -0.282), ('name', -0.276), ('number', -0.166), ('fire', -0.131), ('event', -0.116), ('per', -0.114), ('station', -0.114), ('monitor', -0.114), ('en', -0.113), ('code', -0.109), ('unit', -0.108), ('day', -0.105)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('value', 0.139), ('name', 0.133), ('station', 0.125), ('fire', 0.103), ('event', 0.091), ('time', 0.087), ('monitor', 0.077), ('wa', 0.074), ('unit', 0.073), ('feature', 0.069)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.594), ('en', -0.307), ('per', -0.292), ('number', -0.195), ('le', -0.191), ('la', -0.174), ('com', -0.174), ('word', -0.089), ('http_www', -0.08), ('ha', -0.071)]\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFiq2ihqa8RA",
        "colab_type": "text"
      },
      "source": [
        "Does this make things better? Well, it’s definitely a lot better than the previous interpretation. Here we can see clear themes of modeling being applied in chips and electronic devices, classification and recognition models, neural models talking about the human brain components like cells, stimuli, neurons, cortical components, and even themes around reinforcement learning! We explore these in detail later in a more structured way. Let’s try to get the three major matrices (U, S, and VT) from our topic model, which uses SVD (based on the foundational concepts mentioned earlier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9E85ziEaaDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "475eaf78-9ec8-465e-f2db-26cc43002ce1"
      },
      "source": [
        "term_topic = lsi_bow.projection.u\n",
        "singular_values = lsi_bow.projection.s\n",
        "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
        "term_topic.shape, singular_values.shape, topic_document.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:502: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1821, 10), (10,), (10, 2150))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRNTimEma9LJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "a4469fc9-bf4f-4899-d0f7-a2e5e0d38d95"
      },
      "source": [
        "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
        "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "print(document_topics.shape)\n",
        "document_topics.head(15)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.020</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.041</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.010</td>\n",
              "      <td>-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.014</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.032</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.019</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.016</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.041</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.019</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.018</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.044</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>0.130</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>0.877</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>-0.066</td>\n",
              "      <td>0.022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
              "0   0.006  0.012  0.009 -0.002  0.001  0.002 -0.000  0.014 -0.003  0.001\n",
              "1   0.020  0.043 -0.041 -0.002  0.015  0.007  0.004  0.027  0.010 -0.006\n",
              "2   0.014  0.027  0.018 -0.007  0.007  0.012  0.000  0.024  0.021 -0.001\n",
              "3   0.032  0.035  0.023 -0.016  0.000  0.004  0.005  0.044  0.008  0.003\n",
              "4   0.007  0.013  0.008 -0.004  0.019 -0.009 -0.008 -0.009 -0.006  0.003\n",
              "5   0.016  0.032  0.020 -0.002  0.041 -0.011 -0.015 -0.010 -0.002  0.015\n",
              "6   0.002  0.004  0.003 -0.001  0.001  0.000  0.000  0.007  0.002  0.002\n",
              "7   0.002  0.004  0.003 -0.000  0.001  0.000  0.000  0.003  0.004  0.000\n",
              "8   0.009  0.017  0.012 -0.009  0.016 -0.011 -0.009 -0.010  0.005  0.006\n",
              "9   0.009  0.017  0.007  0.006  0.004  0.002  0.000  0.015 -0.007 -0.003\n",
              "10  0.004  0.007  0.006 -0.008 -0.001 -0.001 -0.002  0.004  0.007  0.001\n",
              "11  0.010  0.020  0.010  0.009  0.003  0.002  0.007  0.019 -0.009  0.005\n",
              "12  0.009  0.018  0.013  0.001  0.021 -0.014 -0.012 -0.015 -0.008  0.017\n",
              "13  0.018  0.035  0.044 -0.129  0.130 -0.053  0.877 -0.276 -0.066  0.022\n",
              "14  0.007  0.008  0.004  0.023 -0.004  0.000  0.003 -0.009  0.008 -0.002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2RG-WoIcOw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8ba875c3-9e14-49c1-b3e5-23dcfbd74961"
      },
      "source": [
        "print(lsi_bow.show_topic(1, topn=20))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('university', -0.450553573610091), ('player', 0.29506700736267866), ('number', 0.20621154666702776), ('team', 0.18479887219503696), ('time', 0.1766891337550597), ('year', 0.17606283480334114), ('wa', 0.17224652417513153), ('file', 0.16641327395046698), ('csv', 0.14590255454119186), ('one', 0.13429132222503803), ('name', 0.12889782562571633), ('date', 0.12406195789189757), ('information', 0.12192957273946657), ('ha', 0.12087212806902431), ('goal', 0.10073343681324058), ('value', 0.0984630413860938), ('game', 0.08791427352298868), ('code', 0.08565537960353707), ('used', 0.08541151909640803), ('use', 0.08043511731207907)]\n",
            "[('csv', 0.6334792711458185), ('date', -0.29246379413739515), ('year', -0.22142714981716535), ('number', -0.19452716386112245), ('integer', 0.18294111118787237), ('registration', -0.16034625622593435), ('zero', -0.15748096668186842), ('time', -0.14429339858005663), ('start', -0.134933511373109), ('application', -0.12984797663563105), ('position', -0.11668209010919899), ('code', -0.11333145605202852), ('one', -0.11161897604231404), ('movie', 0.10963260087092926), ('containing', -0.10902784307858368), ('numeric', 0.0990507442043553), ('element', -0.09814475741660983), ('section', -0.09086770986409563), ('text', 0.08525738504483706), ('user', 0.08263643637282912)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRCygIUEcHQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72c83b1c-cbc0-4ee9-ff60-f3777df721d5"
      },
      "source": [
        "int(top_topics[0][1])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwK6FPqNa-3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "d67ba0d5-c320-4629-e0a4-1f8f800c7cd3"
      },
      "source": [
        "document_numbers = [1, 5, 10]\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print('Topic model '+top_topics[0][1]+':',lsi_bow.show_topic(int(top_topics[0][1]), topn=20))\n",
        "    print()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document #1:\n",
            "Dominant Topics (top 3): ['T2', 'T3', 'T8']\n",
            "Paper Summary:\n",
            "The ultimate Soccer database for data analysis and machine learning\n",
            "What you get:\n",
            "+25,000 matches\n",
            "+10,000 players\n",
            "11 European Countries with their lead championship\n",
            "Seasons 2008 to 2016\n",
            "Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates\n",
            "Team line up with squad formation (X, Y coordinates)\n",
            "Betting odds from up to 10 providers\n",
            "Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
            "*16th Oct 201\n",
            "Topic model 2: [('player', -0.6771529757489971), ('team', -0.39275984613802317), ('goal', -0.2365538810942563), ('zone', -0.15157248883497101), ('file', 0.13549199029909362), ('game', -0.13360002518378492), ('allowed', -0.1296533620537427), ('csv', 0.11207272268683074), ('year', 0.10167480240330877), ('one', 0.09169777743903763), ('date', 0.0873547278113432), ('information', 0.08116850624413084), ('percentage', -0.07638303589178184), ('name', 0.07292486260371045), ('value', 0.07226472152312192), ('individual', -0.06761623121225609), ('taken', -0.06508028919279098), ('scored', -0.0646386826022946), ('relative', -0.059529463604475474), ('time', 0.05844862551826812)]\n",
            "\n",
            "Document #5:\n",
            "Dominant Topics (top 3): ['T5', 'T2', 'T3']\n",
            "Paper Summary:\n",
            "Context\n",
            "For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.\n",
            "To share some of the initial insights from the survey, we’ve worked with the folks from The Pudding to put together \n",
            "Topic model 5: [('integer', 0.8640843330985445), ('csv', -0.34323748323905173), ('movie', 0.10904985964337308), ('year', -0.10346387925314211), ('point', 0.09170125227863321), ('people', 0.09157694811226921), ('categorical', 0.08896060613805948), ('always', 0.0859741029789151), ('music', 0.07682194281997572), ('preference', 0.06298022575290338), ('interest', 0.06251741525554357), ('lot', 0.05920866237977235), ('item', 0.05612453136466026), ('enjoy', 0.0485538633435726), ('numeric', -0.04820304351081477), ('player', -0.04588385920265606), ('time', 0.04215891163502139), ('money', 0.04079181319162761), ('life', 0.03862593315219237), ('often', 0.03613278966634023)]\n",
            "\n",
            "Document #10:\n",
            "Dominant Topics (top 3): ['T4', 'T2', 'T9']\n",
            "Paper Summary:\n",
            "These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of a\n",
            "Topic model 4: [('csv', 0.6334792711458185), ('date', -0.29246379413739515), ('year', -0.22142714981716535), ('number', -0.19452716386112245), ('integer', 0.18294111118787237), ('registration', -0.16034625622593435), ('zero', -0.15748096668186842), ('time', -0.14429339858005663), ('start', -0.134933511373109), ('application', -0.12984797663563105), ('position', -0.11668209010919899), ('code', -0.11333145605202852), ('one', -0.11161897604231404), ('movie', 0.10963260087092926), ('containing', -0.10902784307858368), ('numeric', 0.0990507442043553), ('element', -0.09814475741660983), ('section', -0.09086770986409563), ('text', 0.08525738504483706), ('user', 0.08263643637282912)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykl0jA0lbCgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}