{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_NLP_upvoted_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcPqpEuTs/VHWZwqRM3yXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/Kaggle_NLP_upvoted_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE3HCPOVUhjz",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/canggih/voted-kaggle-dataset/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-vTIH-OUbRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz28thIKUhzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "fc383b41-dcee-4292-92a8-c6e325876aa8"
      },
      "source": [
        "data=pd.read_csv('https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/voted-kaggle-dataset.csv?raw=true')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Subtitle</th>\n",
              "      <th>Owner</th>\n",
              "      <th>Votes</th>\n",
              "      <th>Versions</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Data Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>License</th>\n",
              "      <th>Views</th>\n",
              "      <th>Download</th>\n",
              "      <th>Kernels</th>\n",
              "      <th>Topics</th>\n",
              "      <th>URL</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Credit Card Fraud Detection</td>\n",
              "      <td>Anonymized credit card transactions labeled as...</td>\n",
              "      <td>Machine Learning Group - ULB</td>\n",
              "      <td>1241</td>\n",
              "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
              "      <td>crime\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>442,136 views</td>\n",
              "      <td>53,128 downloads</td>\n",
              "      <td>1,782 kernels</td>\n",
              "      <td>26 topics</td>\n",
              "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
              "      <td>The datasets contains transactions made by cre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>European Soccer Database</td>\n",
              "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
              "      <td>Hugo Mathien</td>\n",
              "      <td>1046</td>\n",
              "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
              "      <td>association football\\neurope</td>\n",
              "      <td>SQLite</td>\n",
              "      <td>299 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>396,214 views</td>\n",
              "      <td>46,367 downloads</td>\n",
              "      <td>1,459 kernels</td>\n",
              "      <td>75 topics</td>\n",
              "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
              "      <td>The ultimate Soccer database for data analysis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TMDB 5000 Movie Dataset</td>\n",
              "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
              "      <td>The Movie Database (TMDb)</td>\n",
              "      <td>1024</td>\n",
              "      <td>Version 2,2017-09-28</td>\n",
              "      <td>film</td>\n",
              "      <td>CSV</td>\n",
              "      <td>44 MB</td>\n",
              "      <td>Other</td>\n",
              "      <td>446,255 views</td>\n",
              "      <td>62,002 downloads</td>\n",
              "      <td>1,394 kernels</td>\n",
              "      <td>46 topics</td>\n",
              "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
              "      <td>Background\\nWhat can we say about the success ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Global Terrorism Database</td>\n",
              "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
              "      <td>START Consortium</td>\n",
              "      <td>789</td>\n",
              "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
              "      <td>crime\\nterrorism\\ninternational relations</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>Other</td>\n",
              "      <td>187,877 views</td>\n",
              "      <td>26,309 downloads</td>\n",
              "      <td>608 kernels</td>\n",
              "      <td>11 topics</td>\n",
              "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
              "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bitcoin Historical Data</td>\n",
              "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
              "      <td>Zielak</td>\n",
              "      <td>618</td>\n",
              "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
              "      <td>history\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>119 MB</td>\n",
              "      <td>CC4</td>\n",
              "      <td>146,734 views</td>\n",
              "      <td>16,868 downloads</td>\n",
              "      <td>68 kernels</td>\n",
              "      <td>13 topics</td>\n",
              "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
              "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Title  ...                                        Description\n",
              "0  Credit Card Fraud Detection  ...  The datasets contains transactions made by cre...\n",
              "1     European Soccer Database  ...  The ultimate Soccer database for data analysis...\n",
              "2      TMDB 5000 Movie Dataset  ...  Background\\nWhat can we say about the success ...\n",
              "3    Global Terrorism Database  ...  Context\\nInformation on more than 170,000 Terr...\n",
              "4      Bitcoin Historical Data  ...  Context\\nBitcoin is the longest running and mo...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEVdPq3QU96N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "797720c0-39cf-454f-d95e-379a03955b92"
      },
      "source": [
        "data.Description[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhh0YWlxXLKG",
        "colab_type": "text"
      },
      "source": [
        "# Ch06b - Topic Modeling with gensim.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPj-D9NXXXiW",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgVxQFhcXhL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "89604aa2-14bc-48d7-9afd-25237ddda1ee"
      },
      "source": [
        "papers=data.Description\n",
        "print(len(papers))\n",
        "papers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       The datasets contains transactions made by cre...\n",
              "1       The ultimate Soccer database for data analysis...\n",
              "2       Background\\nWhat can we say about the success ...\n",
              "3       Context\\nInformation on more than 170,000 Terr...\n",
              "4       Context\\nBitcoin is the longest running and mo...\n",
              "                              ...                        \n",
              "2145    Context\\nFortnite: Battle Royale has over 20 m...\n",
              "2146    Context\\nThis dataset provides the nationaliti...\n",
              "2147    lem.json\\nThis file contains lementized englis...\n",
              "2148    Context\\nThis data set contains weather data f...\n",
              "2149    Context\\nBirths in U.S during 1994 to 2003.\\nC...\n",
              "Name: Description, Length: 2150, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyQh58zWYHTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "papers=papers.astype(str)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7frNCl40WDfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "42ec6598-b331-4987-f647-236f2f13928a"
      },
      "source": [
        "%%time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(papers):\n",
        "    norm_papers = []\n",
        "    for paper in papers:\n",
        "        paper = paper.lower()\n",
        "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "        paper_tokens = list(filter(None, paper_tokens))\n",
        "        if paper_tokens:\n",
        "            norm_papers.append(paper_tokens)\n",
        "            \n",
        "    return norm_papers\n",
        "    \n",
        "norm_papers = normalize_corpus(papers)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "CPU times: user 3.14 s, sys: 33.9 ms, total: 3.17 s\n",
            "Wall time: 3.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqy2Zt0VXnxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "896cca7c-7482-4632-d971-9afc9b0754f4"
      },
      "source": [
        "print(papers[0])\n",
        "print(norm_papers[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
            "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
            "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
            "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
            "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze66dZ44YlUq",
        "colab_type": "text"
      },
      "source": [
        "## Text Representation with Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO6346G5XpN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') \n",
        "# higher threshold fewer phrases.\n",
        "bigram_model = gensim.models.phrases.Phraser(bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hupfI97NYyY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "412accac-80aa-4f6f-9c39-53b0b40cfa1d"
      },
      "source": [
        "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
        "print(norm_corpus_bigrams[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine_learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please_cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C01Gv8vBZBrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "# Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
        "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHKgFEkuZNoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e50dcada-ffe8-4cbd-f9ca-6cbb94433965"
      },
      "source": [
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, 'ac'), (1, 'account'), (2, 'accuracy'), (3, 'amount'), (4, 'analysed'), (5, 'andrea'), (6, 'area'), (7, 'artml'), (8, 'auprc'), (9, 'available'), (10, 'background'), (11, 'big'), (12, 'bontempi'), (13, 'brufence'), (14, 'bruxelles')]\n",
            "Total Vocabulary Size: 32097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX-lAwjdZPqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "881b100c-30b0-4b6c-9c43-9922692392ca"
      },
      "source": [
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, 'ac'), (1, 'account'), (2, 'accuracy'), (3, 'amount'), (4, 'area'), (5, 'available'), (6, 'background'), (7, 'big'), (8, 'cannot'), (9, 'card'), (10, 'case'), (11, 'class'), (12, 'classification'), (13, 'collected'), (14, 'component')]\n",
            "Total Vocabulary Size: 1821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BufKRboIZVoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "98096c4e-0d0b-4149-9169-3cdfbafbe967"
      },
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
        "print(bow_corpus[0][:50])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 3), (1, 1), (2, 2), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 3), (12, 2), (13, 1), (14, 1), (15, 1), (16, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 7), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbnvw8ROZpX9",
        "colab_type": "text"
      },
      "source": [
        "doc2bow: Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method.\n",
        "\n",
        "If allow_update is set, then also update dictionary in the process: create ids for new words. At the same time, update document frequencies -- for each word appearing in this document, increase its document frequency (self.dfs) by one.\n",
        "\n",
        "If allow_update is not set, this function is const, aka read-only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBqSH8EsZe3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7fd6bd05-cf67-4f3d-e034-fbd7d33e0e3c"
      },
      "source": [
        "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('card', 1), ('case', 1), ('class', 2), ('day', 1), ('european', 1), ('feature', 1), ('first', 1), ('ha', 3), ('http', 2), ('machine_learning', 1), ('original', 2), ('probability', 2), ('project', 1), ('time', 2), ('using', 2), ('value', 1), ('able', 2), ('access', 2), ('achieved', 1), ('across', 1), ('adding', 1), ('algorithm', 1), ('also', 3), ('analysis', 1), ('api', 1), ('appears', 1), ('ask', 1), ('asset', 1), ('attribute', 5), ('away', 1), ('base', 1), ('called', 1), ('changed', 1), ('classifier', 1), ('click', 2), ('co', 1), ('collection', 1), ('column', 1), ('com', 3), ('come', 1), ('commercial', 1), ('compare', 1), ('containing', 1), ('coordinate', 1), ('could', 1), ('country', 1), ('crawling', 2), ('cross', 1), ('database', 2), ('design', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZhG9ywZt_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3417334-7cd0-460d-a962-b77cdad9c47f"
      },
      "source": [
        "print('Total number of papers:', len(bow_corpus))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of papers: 2150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksCjrdIwZxgZ",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meXt3wwNZvr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e80037d0-e56a-42fe-8496-8184b70117fc"
      },
      "source": [
        "%%time\n",
        "TOTAL_TOPICS = 20\n",
        "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
        "                                 onepass=True, chunksize=1740, power_iters=1000)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min, sys: 46.6 s, total: 2min 47s\n",
            "Wall time: 1min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-VT5R5Z2AI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d108f00-cb22-44d2-b200-6cdb6c87750a"
      },
      "source": [
        "for topic_id, topic in lsi_bow.print_topics(num_topics=20, num_words=20):\n",
        "    print('Topic #'+str(topic_id+1)+':')\n",
        "    print(topic)\n",
        "    print()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "0.879*\"university\" + 0.124*\"player\" + 0.101*\"number\" + 0.091*\"wa\" + 0.090*\"college\" + 0.088*\"year\" + 0.088*\"time\" + 0.085*\"file\" + 0.079*\"team\" + 0.075*\"csv\" + 0.069*\"one\" + 0.066*\"information\" + 0.064*\"name\" + 0.063*\"ha\" + 0.060*\"date\" + 0.049*\"value\" + 0.046*\"institute\" + 0.045*\"used\" + 0.045*\"new\" + 0.045*\"university_california\"\n",
            "\n",
            "Topic #2:\n",
            "-0.452*\"university\" + 0.294*\"player\" + 0.207*\"number\" + 0.183*\"team\" + 0.176*\"year\" + 0.176*\"time\" + 0.172*\"wa\" + 0.168*\"file\" + 0.146*\"csv\" + 0.134*\"one\" + 0.130*\"name\" + 0.124*\"date\" + 0.122*\"information\" + 0.121*\"ha\" + 0.100*\"goal\" + 0.098*\"value\" + 0.088*\"game\" + 0.086*\"code\" + 0.085*\"used\" + 0.080*\"use\"\n",
            "\n",
            "Topic #3:\n",
            "-0.678*\"player\" + -0.394*\"team\" + -0.237*\"goal\" + -0.151*\"zone\" + 0.135*\"file\" + -0.134*\"game\" + -0.129*\"allowed\" + 0.110*\"csv\" + 0.102*\"year\" + 0.091*\"one\" + 0.087*\"date\" + 0.080*\"information\" + -0.076*\"percentage\" + 0.075*\"name\" + 0.072*\"value\" + -0.068*\"individual\" + -0.065*\"taken\" + -0.065*\"scored\" + -0.059*\"relative\" + 0.058*\"time\"\n",
            "\n",
            "Topic #4:\n",
            "0.607*\"year\" + -0.252*\"date\" + -0.230*\"file\" + 0.188*\"total\" + -0.155*\"csv\" + -0.154*\"one\" + -0.150*\"registration\" + -0.145*\"zero\" + -0.142*\"start\" + -0.134*\"application\" + -0.132*\"time\" + 0.123*\"given\" + -0.119*\"position\" + -0.119*\"numeric\" + -0.115*\"containing\" + 0.107*\"energy\" + -0.104*\"text\" + 0.095*\"state\" + -0.094*\"element\" + 0.088*\"child\"\n",
            "\n",
            "Topic #5:\n",
            "0.633*\"csv\" + -0.302*\"date\" + -0.200*\"number\" + -0.191*\"year\" + 0.169*\"integer\" + -0.166*\"registration\" + -0.163*\"zero\" + -0.153*\"time\" + -0.140*\"start\" + -0.135*\"application\" + -0.121*\"position\" + -0.119*\"one\" + 0.115*\"movie\" + -0.114*\"code\" + -0.113*\"containing\" + -0.102*\"element\" + 0.097*\"user\" + -0.095*\"section\" + 0.087*\"numeric\" + -0.083*\"mark\"\n",
            "\n",
            "Topic #6:\n",
            "0.856*\"integer\" + -0.356*\"csv\" + -0.118*\"year\" + 0.107*\"movie\" + 0.092*\"point\" + 0.092*\"people\" + 0.088*\"categorical\" + 0.085*\"always\" + 0.076*\"music\" + 0.063*\"preference\" + 0.062*\"interest\" + 0.059*\"lot\" + 0.056*\"item\" + 0.048*\"enjoy\" + -0.048*\"player\" + 0.041*\"money\" + 0.039*\"team\" + 0.039*\"life\" + 0.038*\"time\" + 0.037*\"often\"\n",
            "\n",
            "Topic #7:\n",
            "0.774*\"numeric\" + 0.499*\"text\" + -0.272*\"csv\" + -0.111*\"integer\" + 0.069*\"word\" + -0.062*\"date\" + -0.054*\"file\" + 0.051*\"open\" + -0.045*\"time\" + 0.043*\"food\" + -0.042*\"movie\" + 0.039*\"language\" + 0.039*\"product\" + 0.037*\"student\" + -0.036*\"number\" + -0.035*\"one\" + -0.035*\"registration\" + 0.035*\"database\" + -0.034*\"zero\" + 0.034*\"use\"\n",
            "\n",
            "Topic #8:\n",
            "-0.499*\"csv\" + -0.354*\"year\" + -0.293*\"integer\" + -0.289*\"numeric\" + -0.164*\"date\" + 0.143*\"image\" + 0.123*\"word\" + 0.111*\"use\" + -0.104*\"total\" + -0.103*\"registration\" + -0.099*\"zero\" + 0.098*\"user\" + 0.095*\"language\" + 0.091*\"information\" + 0.090*\"wa\" + 0.084*\"using\" + -0.083*\"time\" + -0.083*\"position\" + -0.081*\"given\" + -0.080*\"application\"\n",
            "\n",
            "Topic #9:\n",
            "0.312*\"year\" + 0.262*\"file\" + -0.262*\"name\" + -0.246*\"value\" + -0.240*\"de\" + 0.213*\"image\" + 0.165*\"movie\" + -0.160*\"number\" + 0.157*\"total\" + -0.143*\"fire\" + 0.138*\"user\" + 0.128*\"word\" + -0.112*\"unit\" + -0.109*\"csv\" + -0.108*\"code\" + -0.105*\"state\" + 0.104*\"given\" + -0.101*\"event\" + -0.101*\"day\" + -0.100*\"city\"\n",
            "\n",
            "Topic #10:\n",
            "0.588*\"image\" + -0.314*\"de\" + -0.247*\"user\" + -0.206*\"movie\" + -0.165*\"en\" + 0.150*\"label\" + -0.142*\"per\" + 0.134*\"class\" + 0.127*\"value\" + -0.105*\"number\" + -0.101*\"word\" + -0.100*\"le\" + -0.100*\"rating\" + 0.099*\"sample\" + -0.096*\"language\" + 0.096*\"point\" + -0.095*\"com\" + -0.095*\"tag\" + -0.093*\"la\" + -0.089*\"ha\"\n",
            "\n",
            "Topic #11:\n",
            "-0.444*\"de\" + -0.277*\"value\" + -0.264*\"image\" + -0.226*\"en\" + -0.223*\"per\" + 0.168*\"child\" + -0.150*\"le\" + 0.142*\"police\" + 0.134*\"fire\" + 0.129*\"age\" + -0.127*\"file\" + -0.123*\"year\" + -0.122*\"la\" + -0.116*\"com\" + 0.115*\"information\" + -0.111*\"station\" + 0.109*\"woman\" + 0.106*\"death\" + 0.105*\"state\" + 0.098*\"survey\"\n",
            "\n",
            "Topic #12:\n",
            "0.320*\"de\" + 0.290*\"image\" + -0.277*\"user\" + -0.240*\"value\" + -0.190*\"file\" + -0.188*\"station\" + 0.180*\"en\" + -0.176*\"movie\" + 0.170*\"child\" + 0.168*\"number\" + 0.165*\"per\" + -0.139*\"id\" + 0.132*\"word\" + -0.131*\"name\" + 0.110*\"age\" + -0.107*\"event\" + 0.100*\"le\" + -0.097*\"tag\" + 0.097*\"la\" + 0.096*\"language\"\n",
            "\n",
            "Topic #13:\n",
            "-0.343*\"child\" + -0.280*\"number\" + -0.276*\"word\" + 0.210*\"fire\" + -0.183*\"age\" + -0.177*\"language\" + 0.150*\"de\" + -0.141*\"value\" + -0.136*\"name\" + 0.134*\"national\" + 0.118*\"department\" + 0.118*\"code\" + -0.112*\"sample\" + 0.110*\"information\" + -0.108*\"station\" + 0.103*\"en\" + 0.103*\"state\" + -0.102*\"corpus\" + 0.101*\"service\" + 0.100*\"center\"\n",
            "\n",
            "Topic #14:\n",
            "0.376*\"race\" + 0.319*\"time\" + -0.283*\"image\" + -0.236*\"file\" + -0.234*\"name\" + 0.174*\"word\" + 0.170*\"point\" + -0.159*\"number\" + -0.149*\"child\" + -0.145*\"player\" + 0.143*\"section\" + -0.136*\"age\" + 0.118*\"taken\" + -0.109*\"code\" + 0.102*\"match\" + 0.100*\"team\" + 0.092*\"language\" + 0.089*\"position\" + -0.087*\"rating\" + -0.085*\"movie\"\n",
            "\n",
            "Topic #15:\n",
            "0.365*\"word\" + -0.365*\"user\" + -0.340*\"race\" + 0.248*\"language\" + -0.196*\"time\" + -0.175*\"number\" + 0.171*\"file\" + -0.155*\"image\" + 0.131*\"corpus\" + -0.123*\"tweet\" + -0.116*\"movie\" + -0.112*\"age\" + -0.109*\"id\" + 0.105*\"code\" + -0.104*\"section\" + 0.097*\"fire\" + -0.097*\"rating\" + 0.092*\"team\" + -0.089*\"game\" + -0.087*\"police\"\n",
            "\n",
            "Topic #16:\n",
            "0.317*\"number\" + -0.285*\"file\" + -0.248*\"team\" + -0.218*\"police\" + 0.178*\"player\" + 0.165*\"game\" + 0.163*\"fire\" + -0.162*\"woman\" + -0.157*\"station\" + -0.154*\"point\" + -0.149*\"age\" + -0.146*\"total\" + -0.145*\"child\" + 0.143*\"user\" + -0.140*\"property\" + -0.126*\"goal\" + 0.122*\"word\" + -0.117*\"population\" + 0.116*\"wa\" + 0.113*\"language\"\n",
            "\n",
            "Topic #17:\n",
            "-0.229*\"race\" + -0.223*\"fire\" + -0.222*\"user\" + 0.198*\"station\" + -0.190*\"word\" + -0.178*\"point\" + -0.176*\"image\" + -0.155*\"name\" + -0.153*\"child\" + 0.153*\"feature\" + 0.152*\"game\" + -0.139*\"taken\" + -0.130*\"time\" + -0.125*\"team\" + -0.114*\"unit\" + 0.108*\"match\" + -0.107*\"language\" + -0.106*\"file\" + -0.106*\"state\" + 0.102*\"player\"\n",
            "\n",
            "Topic #18:\n",
            "-0.292*\"police\" + -0.248*\"station\" + 0.236*\"drug\" + 0.215*\"plan\" + 0.207*\"survey\" + 0.192*\"rating\" + 0.192*\"race\" + 0.190*\"health\" + -0.184*\"crime\" + -0.144*\"fire\" + -0.132*\"location\" + -0.127*\"user\" + -0.117*\"woman\" + -0.116*\"death\" + -0.113*\"wa\" + 0.112*\"sample\" + 0.111*\"disease\" + 0.105*\"taken\" + -0.100*\"feature\" + -0.099*\"incident\"\n",
            "\n",
            "Topic #19:\n",
            "0.350*\"race\" + 0.279*\"file\" + 0.278*\"station\" + -0.252*\"user\" + 0.181*\"feature\" + 0.176*\"fire\" + -0.150*\"event\" + -0.134*\"team\" + -0.131*\"date\" + -0.121*\"sample\" + -0.115*\"tweet\" + -0.112*\"monitor\" + 0.107*\"player\" + -0.106*\"value\" + 0.105*\"section\" + 0.100*\"id\" + 0.099*\"contains\" + -0.098*\"parameter\" + 0.098*\"number\" + 0.097*\"game\"\n",
            "\n",
            "Topic #20:\n",
            "0.313*\"police\" + -0.240*\"price\" + -0.236*\"name\" + -0.234*\"team\" + -0.215*\"point\" + -0.211*\"match\" + 0.168*\"player\" + 0.162*\"crime\" + 0.162*\"woman\" + -0.159*\"sale\" + 0.146*\"value\" + 0.119*\"drug\" + 0.112*\"plan\" + 0.107*\"survey\" + 0.107*\"health\" + 0.107*\"word\" + 0.102*\"death\" + 0.098*\"taken\" + 0.097*\"station\" + 0.097*\"race\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnToggzkaZAj",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a moment to understand these results. A brief recap on the LSI model— it is based on the principle that words that are used in the same contexts tend to have similar meanings. You can observe in this output that each topic is a combination of terms (which basically tend to convey an overall sense of the topic) and weights. Now the problem here is that we have both positive and negative weights. What does that mean?\n",
        "\n",
        "Based on existing research and my interpretations, considering we are reducing the dimensionality here to a 10-dimensional space based on the number of topics, the sign on each term indicates a sense of direction or orientation in the vector space for a particular topic. The higher the weight, the more important the contribution. So similar correlated terms have the same sign or direction. Hence, it is perfectly possible for a topic to have two different sub-themes based on the sign or orientation of terms. Let’s separate these terms and try to interpret the topics again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSdChuYwaLmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d75a9aaa-dfc7-47dd-e474-fb3a1b2c0606"
      },
      "source": [
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('university', 0.879), ('player', 0.124), ('number', 0.101), ('wa', 0.091), ('college', 0.09), ('year', 0.088), ('time', 0.088), ('file', 0.085), ('team', 0.079), ('csv', 0.075), ('one', 0.069), ('information', 0.066), ('name', 0.064), ('ha', 0.063), ('date', 0.06), ('value', 0.049), ('institute', 0.046), ('used', 0.045), ('new', 0.045), ('university_california', 0.045)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.294), ('number', 0.207), ('team', 0.183), ('year', 0.176), ('time', 0.176), ('wa', 0.172), ('file', 0.168), ('csv', 0.146), ('one', 0.134), ('name', 0.13), ('date', 0.124), ('information', 0.122), ('ha', 0.121), ('goal', 0.1), ('value', 0.098), ('game', 0.088), ('code', 0.086), ('used', 0.085), ('use', 0.08)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('university', -0.452)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('file', 0.135), ('csv', 0.11), ('year', 0.102), ('one', 0.091), ('date', 0.087), ('information', 0.08), ('name', 0.075), ('value', 0.072), ('time', 0.058)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('player', -0.678), ('team', -0.394), ('goal', -0.237), ('zone', -0.151), ('game', -0.134), ('allowed', -0.129), ('percentage', -0.076), ('individual', -0.068), ('taken', -0.065), ('scored', -0.065), ('relative', -0.059)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.607), ('total', 0.188), ('given', 0.123), ('energy', 0.107), ('state', 0.095), ('child', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('date', -0.252), ('file', -0.23), ('csv', -0.155), ('one', -0.154), ('registration', -0.15), ('zero', -0.145), ('start', -0.142), ('application', -0.134), ('time', -0.132), ('position', -0.119), ('numeric', -0.119), ('containing', -0.115), ('text', -0.104), ('element', -0.094)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('csv', 0.633), ('integer', 0.169), ('movie', 0.115), ('user', 0.097), ('numeric', 0.087)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('date', -0.302), ('number', -0.2), ('year', -0.191), ('registration', -0.166), ('zero', -0.163), ('time', -0.153), ('start', -0.14), ('application', -0.135), ('position', -0.121), ('one', -0.119), ('code', -0.114), ('containing', -0.113), ('element', -0.102), ('section', -0.095), ('mark', -0.083)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('integer', 0.856), ('movie', 0.107), ('point', 0.092), ('people', 0.092), ('categorical', 0.088), ('always', 0.085), ('music', 0.076), ('preference', 0.063), ('interest', 0.062), ('lot', 0.059), ('item', 0.056), ('enjoy', 0.048), ('money', 0.041), ('team', 0.039), ('life', 0.039), ('time', 0.038), ('often', 0.037)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.356), ('year', -0.118), ('player', -0.048)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.774), ('text', 0.499), ('word', 0.069), ('open', 0.051), ('food', 0.043), ('language', 0.039), ('product', 0.039), ('student', 0.037), ('database', 0.035), ('use', 0.034)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.272), ('integer', -0.111), ('date', -0.062), ('file', -0.054), ('time', -0.045), ('movie', -0.042), ('number', -0.036), ('one', -0.035), ('registration', -0.035), ('zero', -0.034)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.143), ('word', 0.123), ('use', 0.111), ('user', 0.098), ('language', 0.095), ('information', 0.091), ('wa', 0.09), ('using', 0.084)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.499), ('year', -0.354), ('integer', -0.293), ('numeric', -0.289), ('date', -0.164), ('total', -0.104), ('registration', -0.103), ('zero', -0.099), ('time', -0.083), ('position', -0.083), ('given', -0.081), ('application', -0.08)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.312), ('file', 0.262), ('image', 0.213), ('movie', 0.165), ('total', 0.157), ('user', 0.138), ('word', 0.128), ('given', 0.104)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('name', -0.262), ('value', -0.246), ('de', -0.24), ('number', -0.16), ('fire', -0.143), ('unit', -0.112), ('csv', -0.109), ('code', -0.108), ('state', -0.105), ('event', -0.101), ('day', -0.101), ('city', -0.1)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.588), ('label', 0.15), ('class', 0.134), ('value', 0.127), ('sample', 0.099), ('point', 0.096)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.314), ('user', -0.247), ('movie', -0.206), ('en', -0.165), ('per', -0.142), ('number', -0.105), ('word', -0.101), ('le', -0.1), ('rating', -0.1), ('language', -0.096), ('com', -0.095), ('tag', -0.095), ('la', -0.093), ('ha', -0.089)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #11:\n",
            "==================================================\n",
            "Direction 1: [('child', 0.168), ('police', 0.142), ('fire', 0.134), ('age', 0.129), ('information', 0.115), ('woman', 0.109), ('death', 0.106), ('state', 0.105), ('survey', 0.098)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.444), ('value', -0.277), ('image', -0.264), ('en', -0.226), ('per', -0.223), ('le', -0.15), ('file', -0.127), ('year', -0.123), ('la', -0.122), ('com', -0.116), ('station', -0.111)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #12:\n",
            "==================================================\n",
            "Direction 1: [('de', 0.32), ('image', 0.29), ('en', 0.18), ('child', 0.17), ('number', 0.168), ('per', 0.165), ('word', 0.132), ('age', 0.11), ('le', 0.1), ('la', 0.097), ('language', 0.096)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.277), ('value', -0.24), ('file', -0.19), ('station', -0.188), ('movie', -0.176), ('id', -0.139), ('name', -0.131), ('event', -0.107), ('tag', -0.097)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #13:\n",
            "==================================================\n",
            "Direction 1: [('fire', 0.21), ('de', 0.15), ('national', 0.134), ('department', 0.118), ('code', 0.118), ('information', 0.11), ('en', 0.103), ('state', 0.103), ('service', 0.101), ('center', 0.1)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('child', -0.343), ('number', -0.28), ('word', -0.276), ('age', -0.183), ('language', -0.177), ('value', -0.141), ('name', -0.136), ('sample', -0.112), ('station', -0.108), ('corpus', -0.102)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #14:\n",
            "==================================================\n",
            "Direction 1: [('race', 0.376), ('time', 0.319), ('word', 0.174), ('point', 0.17), ('section', 0.143), ('taken', 0.118), ('match', 0.102), ('team', 0.1), ('language', 0.092), ('position', 0.089)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -0.283), ('file', -0.236), ('name', -0.234), ('number', -0.159), ('child', -0.149), ('player', -0.145), ('age', -0.136), ('code', -0.109), ('rating', -0.087), ('movie', -0.085)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #15:\n",
            "==================================================\n",
            "Direction 1: [('word', 0.365), ('language', 0.248), ('file', 0.171), ('corpus', 0.131), ('code', 0.105), ('fire', 0.097), ('team', 0.092)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.365), ('race', -0.34), ('time', -0.196), ('number', -0.175), ('image', -0.155), ('tweet', -0.123), ('movie', -0.116), ('age', -0.112), ('id', -0.109), ('section', -0.104), ('rating', -0.097), ('game', -0.089), ('police', -0.087)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #16:\n",
            "==================================================\n",
            "Direction 1: [('number', 0.317), ('player', 0.178), ('game', 0.165), ('fire', 0.163), ('user', 0.143), ('word', 0.122), ('wa', 0.116), ('language', 0.113)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('file', -0.285), ('team', -0.248), ('police', -0.218), ('woman', -0.162), ('station', -0.157), ('point', -0.154), ('age', -0.149), ('total', -0.146), ('child', -0.145), ('property', -0.14), ('goal', -0.126), ('population', -0.117)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #17:\n",
            "==================================================\n",
            "Direction 1: [('station', 0.198), ('feature', 0.153), ('game', 0.152), ('match', 0.108), ('player', 0.102)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('race', -0.229), ('fire', -0.223), ('user', -0.222), ('word', -0.19), ('point', -0.178), ('image', -0.176), ('name', -0.155), ('child', -0.153), ('taken', -0.139), ('time', -0.13), ('team', -0.125), ('unit', -0.114), ('language', -0.107), ('file', -0.106), ('state', -0.106)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #18:\n",
            "==================================================\n",
            "Direction 1: [('drug', 0.236), ('plan', 0.215), ('survey', 0.207), ('rating', 0.192), ('race', 0.192), ('health', 0.19), ('sample', 0.112), ('disease', 0.111), ('taken', 0.105)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('police', -0.292), ('station', -0.248), ('crime', -0.184), ('fire', -0.144), ('location', -0.132), ('user', -0.127), ('woman', -0.117), ('death', -0.116), ('wa', -0.113), ('feature', -0.1), ('incident', -0.099)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #19:\n",
            "==================================================\n",
            "Direction 1: [('race', 0.35), ('file', 0.279), ('station', 0.278), ('feature', 0.181), ('fire', 0.176), ('player', 0.107), ('section', 0.105), ('id', 0.1), ('contains', 0.099), ('number', 0.098), ('game', 0.097)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.252), ('event', -0.15), ('team', -0.134), ('date', -0.131), ('sample', -0.121), ('tweet', -0.115), ('monitor', -0.112), ('value', -0.106), ('parameter', -0.098)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #20:\n",
            "==================================================\n",
            "Direction 1: [('police', 0.313), ('player', 0.168), ('crime', 0.162), ('woman', 0.162), ('value', 0.146), ('drug', 0.119), ('plan', 0.112), ('survey', 0.107), ('health', 0.107), ('word', 0.107), ('death', 0.102), ('taken', 0.098), ('station', 0.097), ('race', 0.097)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('price', -0.24), ('name', -0.236), ('team', -0.234), ('point', -0.215), ('match', -0.211), ('sale', -0.159)]\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFiq2ihqa8RA",
        "colab_type": "text"
      },
      "source": [
        "Does this make things better? Well, it’s definitely a lot better than the previous interpretation. Here we can see clear themes of modeling being applied in chips and electronic devices, classification and recognition models, neural models talking about the human brain components like cells, stimuli, neurons, cortical components, and even themes around reinforcement learning! We explore these in detail later in a more structured way. Let’s try to get the three major matrices (U, S, and VT) from our topic model, which uses SVD (based on the foundational concepts mentioned earlier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9E85ziEaaDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0a39c159-4a94-440a-f311-04f8c3ed7880"
      },
      "source": [
        "term_topic = lsi_bow.projection.u\n",
        "singular_values = lsi_bow.projection.s\n",
        "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
        "term_topic.shape, singular_values.shape, topic_document.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:502: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1821, 20), (20,), (20, 2150))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRNTimEma9LJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "f910bd16-9b91-419b-f023-687981cbb044"
      },
      "source": [
        "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
        "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "print(document_topics.shape)\n",
        "document_topics.head(15)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "      <th>T11</th>\n",
              "      <th>T12</th>\n",
              "      <th>T13</th>\n",
              "      <th>T14</th>\n",
              "      <th>T15</th>\n",
              "      <th>T16</th>\n",
              "      <th>T17</th>\n",
              "      <th>T18</th>\n",
              "      <th>T19</th>\n",
              "      <th>T20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.025</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.020</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.041</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.033</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.015</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.019</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>-0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.032</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.005</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>0.047</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.032</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.019</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.016</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.040</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.010</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.018</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.002</td>\n",
              "      <td>-0.021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>-0.020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.018</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>0.118</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.874</td>\n",
              "      <td>-0.289</td>\n",
              "      <td>-0.065</td>\n",
              "      <td>0.044</td>\n",
              "      <td>-0.032</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.027</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       T1     T2     T3     T4     T5  ...    T16    T17    T18    T19    T20\n",
              "0   0.006  0.012  0.009 -0.002  0.000  ... -0.016  0.025 -0.005  0.017  0.000\n",
              "1   0.020  0.043 -0.041 -0.002  0.014  ...  0.003  0.064 -0.012 -0.017 -0.031\n",
              "2   0.015  0.027  0.018 -0.007  0.007  ...  0.002  0.017 -0.000 -0.021 -0.005\n",
              "3   0.032  0.035  0.023 -0.016  0.000  ...  0.032 -0.017 -0.012 -0.067  0.006\n",
              "4   0.007  0.013  0.008 -0.004  0.019  ... -0.002  0.011 -0.008 -0.014 -0.008\n",
              "5   0.016  0.032  0.020 -0.003  0.040  ... -0.007  0.017  0.013 -0.013 -0.000\n",
              "6   0.002  0.004  0.003 -0.001  0.001  ...  0.001  0.004 -0.001 -0.001  0.000\n",
              "7   0.002  0.004  0.003 -0.000  0.001  ... -0.004  0.008  0.004 -0.002 -0.006\n",
              "8   0.009  0.017  0.012 -0.009  0.016  ... -0.002  0.020 -0.008 -0.012 -0.004\n",
              "9   0.009  0.017  0.007  0.006  0.004  ...  0.004  0.008  0.002  0.002 -0.021\n",
              "10  0.004  0.007  0.006 -0.008 -0.001  ... -0.012  0.001  0.003  0.018 -0.006\n",
              "11  0.010  0.020  0.010  0.009  0.004  ...  0.007  0.000  0.003 -0.004 -0.020\n",
              "12  0.010  0.018  0.013 -0.000  0.020  ...  0.000  0.004 -0.007 -0.012 -0.011\n",
              "13  0.018  0.035  0.043 -0.133  0.118  ... -0.024 -0.000  0.002  0.012 -0.013\n",
              "14  0.007  0.008  0.004  0.023 -0.003  ...  0.008  0.013  0.003  0.010  0.005\n",
              "\n",
              "[15 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omNHa8tcd5Uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d806e30d-da9b-4fcf-cb2e-d4c62fe14eb6"
      },
      "source": [
        "top_topics[0][1:]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'19'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwK6FPqNa-3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "51036474-e612-459a-ba4e-fe7c6764de92"
      },
      "source": [
        "document_numbers = [1, 5, 10]\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print('Topic model '+top_topics[0][1:]+':',lsi_bow.show_topic(int(top_topics[0][1:]), topn=20))\n",
        "    print('Topic model '+top_topics[1][1:]+':',lsi_bow.show_topic(int(top_topics[1][1:]), topn=20))\n",
        "    print()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document #1:\n",
            "Dominant Topics (top 3): ['T17', 'T2', 'T3']\n",
            "Paper Summary:\n",
            "The ultimate Soccer database for data analysis and machine learning\n",
            "What you get:\n",
            "+25,000 matches\n",
            "+10,000 players\n",
            "11 European Countries with their lead championship\n",
            "Seasons 2008 to 2016\n",
            "Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates\n",
            "Team line up with squad formation (X, Y coordinates)\n",
            "Betting odds from up to 10 providers\n",
            "Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
            "*16th Oct 201\n",
            "Topic model 17: [('police', -0.2924384771617504), ('station', -0.24798426375102275), ('drug', 0.23643131690674565), ('plan', 0.21513466376974605), ('survey', 0.20661591026593037), ('rating', 0.19236671004567105), ('race', 0.1919343437447153), ('health', 0.18984901662405568), ('crime', -0.18405253536345698), ('fire', -0.14441848483960898), ('location', -0.1315384180137746), ('user', -0.1273879204036706), ('woman', -0.11664968655438007), ('death', -0.11638703260677678), ('wa', -0.1128321451398375), ('sample', 0.11205724639860422), ('disease', 0.11146711894606401), ('taken', 0.1050480894353286), ('feature', -0.09967846262371209), ('incident', -0.09935598940277286)]\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "\n",
            "Document #5:\n",
            "Dominant Topics (top 3): ['T5', 'T2', 'T11']\n",
            "Paper Summary:\n",
            "Context\n",
            "For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.\n",
            "To share some of the initial insights from the survey, we’ve worked with the folks from The Pudding to put together \n",
            "Topic model 5: [('integer', 0.8558303369204103), ('csv', -0.3560007655627152), ('year', -0.11843953713157371), ('movie', 0.106975688485878), ('point', 0.09238243803741718), ('people', 0.09161095744046643), ('categorical', 0.08822485877639168), ('always', 0.08510579110355394), ('music', 0.07618614080130515), ('preference', 0.06265756436109131), ('interest', 0.062228398544929925), ('lot', 0.059128426516308646), ('item', 0.05563414372912065), ('enjoy', 0.048183264967643975), ('player', -0.047582647417016455), ('money', 0.040527058791298635), ('team', 0.039237088394806025), ('life', 0.0391879794239862), ('time', 0.03752584807592715), ('often', 0.03658451227435939)]\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "\n",
            "Document #10:\n",
            "Dominant Topics (top 3): ['T19', 'T14', 'T16']\n",
            "Paper Summary:\n",
            "These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of a\n",
            "Topic model 19: [('police', 0.3128588551437026), ('price', -0.2404099736902586), ('name', -0.2362804984051518), ('team', -0.23388998567359673), ('point', -0.21470422083397508), ('match', -0.21138040287139537), ('player', 0.16810992021928678), ('crime', 0.1621768561107652), ('woman', 0.161825010221465), ('sale', -0.15926053300798132), ('value', 0.1461893502348703), ('drug', 0.11916556822921215), ('plan', 0.11151090366441535), ('survey', 0.10717370360550377), ('health', 0.1068285362030665), ('word', 0.10662870393637465), ('death', 0.10167088503277935), ('taken', 0.09832158507176714), ('station', 0.09740267955044828), ('race', 0.09710118307388292)]\n",
            "Topic model 14: [('word', 0.3647410730005829), ('user', -0.3645749216730648), ('race', -0.3399994383347151), ('language', 0.2477049704013479), ('time', -0.1962980398941309), ('number', -0.17451247778404122), ('file', 0.17100183502430066), ('image', -0.15508001106020472), ('corpus', 0.1306953130534607), ('tweet', -0.1228217884113363), ('movie', -0.11588357615553925), ('age', -0.11211632390658081), ('id', -0.10924241236924614), ('code', 0.10451163565355795), ('section', -0.1042149691550576), ('fire', 0.09689701399383074), ('rating', -0.09682795666565036), ('team', 0.09167078537234875), ('game', -0.08924001235890998), ('police', -0.0871052119172898)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ySUzqzenp3",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykl0jA0lbCgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "720104db-ed02-4e1d-cd39-90529d683673"
      },
      "source": [
        "%%time\n",
        "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, \n",
        "                                   alpha='auto', eta='auto', random_state=42,\n",
        "                                   iterations=500, num_topics=TOTAL_TOPICS, \n",
        "                                   passes=20, eval_every=None)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 32.3 s, sys: 17.1 ms, total: 32.3 s\n",
            "Wall time: 32.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZpWk53derMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "517c30a8-e130-46a2-c9e4-acba3abf4b6d"
      },
      "source": [
        "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
        "    print('Topic #'+str(topic_id+1)+':')\n",
        "    print(topic)\n",
        "    print()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #18:\n",
            "0.061*\"numeric\" + 0.026*\"temperature\" + 0.024*\"university_california\" + 0.017*\"traffic\" + 0.016*\"ic_uci\" + 0.015*\"data_set\" + 0.015*\"text\" + 0.015*\"edu_ml\" + 0.015*\"location\" + 0.015*\"http_archive\" + 0.014*\"wind\" + 0.013*\"fatality\" + 0.013*\"school\" + 0.012*\"uci_machine\" + 0.012*\"weather\" + 0.011*\"learning_repository\" + 0.011*\"incident\" + 0.010*\"acknowledgement\" + 0.009*\"inspiration\" + 0.008*\"predict\"\n",
            "\n",
            "Topic #15:\n",
            "0.033*\"year\" + 0.025*\"energy\" + 0.021*\"total\" + 0.020*\"animal\" + 0.019*\"india\" + 0.017*\"solar\" + 0.016*\"rate\" + 0.014*\"response\" + 0.013*\"outcome\" + 0.013*\"customer\" + 0.012*\"natural\" + 0.012*\"space\" + 0.011*\"earth\" + 0.010*\"range\" + 0.009*\"wa\" + 0.008*\"acknowledgement\" + 0.008*\"type\" + 0.008*\"death\" + 0.008*\"based\" + 0.008*\"inspiration\"\n",
            "\n",
            "Topic #20:\n",
            "0.041*\"crime\" + 0.041*\"time\" + 0.038*\"score\" + 0.028*\"attack\" + 0.018*\"number\" + 0.018*\"defense\" + 0.014*\"submission\" + 0.014*\"non\" + 0.014*\"csv\" + 0.014*\"request\" + 0.014*\"statistic\" + 0.013*\"image\" + 0.013*\"matrix\" + 0.012*\"police\" + 0.012*\"file\" + 0.011*\"run\" + 0.010*\"name\" + 0.010*\"date\" + 0.009*\"id\" + 0.009*\"team\"\n",
            "\n",
            "Topic #12:\n",
            "0.033*\"column\" + 0.032*\"activity\" + 0.021*\"sensor\" + 0.020*\"feature\" + 0.018*\"signal\" + 0.017*\"right\" + 0.014*\"using\" + 0.013*\"left\" + 0.013*\"driving\" + 0.011*\"label\" + 0.010*\"subject\" + 0.010*\"participant\" + 0.009*\"open\" + 0.009*\"information\" + 0.009*\"class\" + 0.009*\"recognition\" + 0.008*\"time\" + 0.008*\"help\" + 0.008*\"database\" + 0.008*\"datasets\"\n",
            "\n",
            "Topic #1:\n",
            "0.231*\"university\" + 0.042*\"college\" + 0.028*\"map\" + 0.014*\"stop\" + 0.013*\"institute\" + 0.012*\"chicago\" + 0.010*\"technology\" + 0.009*\"file\" + 0.008*\"washington\" + 0.008*\"national\" + 0.008*\"archive\" + 0.008*\"new\" + 0.007*\"record\" + 0.007*\"population\" + 0.007*\"district\" + 0.007*\"project\" + 0.007*\"center\" + 0.007*\"research\" + 0.006*\"state\" + 0.006*\"analysis\"\n",
            "\n",
            "Topic #9:\n",
            "0.020*\"tweet\" + 0.018*\"wa\" + 0.013*\"com\" + 0.012*\"twitter\" + 0.012*\"one\" + 0.011*\"time\" + 0.011*\"like\" + 0.011*\"many\" + 0.011*\"inspiration\" + 0.010*\"song\" + 0.010*\"user\" + 0.010*\"column\" + 0.010*\"used\" + 0.009*\"would\" + 0.009*\"site\" + 0.009*\"acknowledgement\" + 0.008*\"people\" + 0.008*\"could\" + 0.007*\"find\" + 0.007*\"nltk\"\n",
            "\n",
            "Topic #17:\n",
            "0.023*\"city\" + 0.019*\"time\" + 0.018*\"information\" + 0.013*\"number\" + 0.012*\"location\" + 0.012*\"date\" + 0.010*\"new_york\" + 0.010*\"property\" + 0.010*\"acknowledgement\" + 0.009*\"election\" + 0.009*\"event\" + 0.009*\"restaurant\" + 0.008*\"name\" + 0.008*\"includes\" + 0.008*\"one\" + 0.007*\"inspiration\" + 0.007*\"station\" + 0.007*\"wa\" + 0.006*\"food\" + 0.006*\"day\"\n",
            "\n",
            "Topic #19:\n",
            "0.025*\"year\" + 0.016*\"country\" + 0.015*\"survey\" + 0.014*\"age\" + 0.013*\"number\" + 0.012*\"state\" + 0.011*\"name\" + 0.011*\"variable\" + 0.010*\"population\" + 0.008*\"month\" + 0.007*\"source\" + 0.007*\"information\" + 0.007*\"education\" + 0.007*\"student\" + 0.007*\"gender\" + 0.006*\"policy\" + 0.006*\"acknowledgement\" + 0.006*\"wa\" + 0.006*\"data_set\" + 0.006*\"rate\"\n",
            "\n",
            "Topic #2:\n",
            "0.010*\"wa\" + 0.009*\"time\" + 0.009*\"information\" + 0.008*\"source\" + 0.008*\"many\" + 0.008*\"world\" + 0.008*\"acknowledgement\" + 0.007*\"inspiration\" + 0.007*\"available\" + 0.007*\"project\" + 0.007*\"ha\" + 0.006*\"database\" + 0.006*\"one\" + 0.006*\"date\" + 0.006*\"also\" + 0.006*\"use\" + 0.006*\"country\" + 0.006*\"report\" + 0.005*\"number\" + 0.005*\"people\"\n",
            "\n",
            "Topic #13:\n",
            "0.011*\"acknowledgement\" + 0.010*\"information\" + 0.010*\"health\" + 0.009*\"wa\" + 0.009*\"state\" + 0.009*\"ha\" + 0.008*\"inspiration\" + 0.007*\"government\" + 0.007*\"company\" + 0.007*\"federal\" + 0.007*\"service\" + 0.006*\"published\" + 0.006*\"patient\" + 0.006*\"database\" + 0.006*\"year\" + 0.006*\"department\" + 0.006*\"vehicle\" + 0.006*\"risk\" + 0.005*\"doe\" + 0.005*\"gov\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inEFsCEseuXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f2a1ad7-aef3-450b-f0c9-6f6efa8df70f"
      },
      "source": [
        "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
        "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
        "print('Avg. Coherence Score:', avg_coherence_score)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Coherence Score: -2.5180356822814263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA28bQ3ieuc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02b51bfe-7795-4101-f2b1-72bb6e3f1942"
      },
      "source": [
        "topics_with_wts = [item[0] for item in topics_coherences]\n",
        "print('LDA Topics with Weights')\n",
        "print('='*50)\n",
        "for idx, topic in enumerate(topics_with_wts):\n",
        "    print('Topic #'+str(idx+1)+':')\n",
        "    print([(term, round(wt, 3)) for wt, term in topic])\n",
        "    print()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA Topics with Weights\n",
            "==================================================\n",
            "Topic #1:\n",
            "[('wa', 0.01), ('time', 0.009), ('information', 0.009), ('source', 0.008), ('many', 0.008), ('world', 0.008), ('acknowledgement', 0.008), ('inspiration', 0.007), ('available', 0.007), ('project', 0.007), ('ha', 0.007), ('database', 0.006), ('one', 0.006), ('date', 0.006), ('also', 0.006), ('use', 0.006), ('country', 0.006), ('report', 0.006), ('number', 0.005), ('people', 0.005)]\n",
            "\n",
            "Topic #2:\n",
            "[('file', 0.036), ('user', 0.026), ('movie', 0.025), ('csv', 0.024), ('tag', 0.015), ('rating', 0.013), ('id', 0.012), ('contains', 0.012), ('research', 0.011), ('use', 0.011), ('paper', 0.01), ('information', 0.01), ('ha', 0.009), ('one', 0.009), ('format', 0.009), ('value', 0.009), ('data_set', 0.008), ('used', 0.008), ('following', 0.007), ('system', 0.007)]\n",
            "\n",
            "Topic #3:\n",
            "[('image', 0.048), ('class', 0.016), ('label', 0.011), ('instance', 0.011), ('cell', 0.01), ('integer', 0.01), ('sample', 0.01), ('model', 0.01), ('number', 0.008), ('training', 0.008), ('using', 0.008), ('file', 0.007), ('classifier', 0.007), ('example', 0.007), ('different', 0.007), ('wa', 0.007), ('two', 0.007), ('machine_learning', 0.006), ('used', 0.006), ('test', 0.006)]\n",
            "\n",
            "Topic #4:\n",
            "[('attribute', 0.019), ('area', 0.012), ('variable', 0.011), ('building', 0.01), ('product', 0.01), ('per', 0.01), ('ha', 0.009), ('type', 0.009), ('day', 0.009), ('model', 0.008), ('value', 0.008), ('number', 0.008), ('feature', 0.008), ('cluster', 0.007), ('using', 0.007), ('use', 0.006), ('database', 0.006), ('one', 0.006), ('food', 0.006), ('point', 0.006)]\n",
            "\n",
            "Topic #5:\n",
            "[('year', 0.025), ('country', 0.016), ('survey', 0.015), ('age', 0.014), ('number', 0.013), ('state', 0.012), ('name', 0.011), ('variable', 0.011), ('population', 0.01), ('month', 0.008), ('source', 0.007), ('information', 0.007), ('education', 0.007), ('student', 0.007), ('gender', 0.007), ('policy', 0.006), ('acknowledgement', 0.006), ('wa', 0.006), ('data_set', 0.006), ('rate', 0.006)]\n",
            "\n",
            "Topic #6:\n",
            "[('text', 0.026), ('article', 0.019), ('title', 0.019), ('sentence', 0.016), ('speech', 0.013), ('file', 0.013), ('post', 0.012), ('line', 0.012), ('wa', 0.012), ('id', 0.01), ('using', 0.01), ('news', 0.009), ('corpus', 0.008), ('ha', 0.008), ('used', 0.007), ('inspiration', 0.007), ('word', 0.007), ('en', 0.007), ('google', 0.007), ('comment', 0.007)]\n",
            "\n",
            "Topic #7:\n",
            "[('city', 0.023), ('time', 0.019), ('information', 0.018), ('number', 0.013), ('location', 0.012), ('date', 0.012), ('new_york', 0.01), ('property', 0.01), ('acknowledgement', 0.01), ('election', 0.009), ('event', 0.009), ('restaurant', 0.009), ('name', 0.008), ('includes', 0.008), ('one', 0.008), ('inspiration', 0.007), ('station', 0.007), ('wa', 0.007), ('food', 0.006), ('day', 0.006)]\n",
            "\n",
            "Topic #8:\n",
            "[('tweet', 0.02), ('wa', 0.018), ('com', 0.013), ('twitter', 0.012), ('one', 0.012), ('time', 0.011), ('like', 0.011), ('many', 0.011), ('inspiration', 0.011), ('song', 0.01), ('user', 0.01), ('column', 0.01), ('used', 0.01), ('would', 0.009), ('site', 0.009), ('acknowledgement', 0.009), ('people', 0.008), ('could', 0.008), ('find', 0.007), ('nltk', 0.007)]\n",
            "\n",
            "Topic #9:\n",
            "[('name', 0.021), ('code', 0.019), ('value', 0.015), ('fire', 0.013), ('county', 0.012), ('state', 0.012), ('air', 0.011), ('row', 0.01), ('csv', 0.01), ('day', 0.01), ('year', 0.009), ('number', 0.009), ('specie', 0.009), ('event', 0.008), ('unit', 0.008), ('hour', 0.008), ('time', 0.008), ('mean', 0.008), ('date', 0.008), ('measured', 0.008)]\n",
            "\n",
            "Topic #10:\n",
            "[('word', 0.06), ('review', 0.029), ('language', 0.026), ('corpus', 0.025), ('name', 0.023), ('file', 0.022), ('http', 0.02), ('license', 0.017), ('text', 0.014), ('code', 0.014), ('english', 0.013), ('copyright', 0.012), ('version', 0.011), ('com', 0.011), ('list', 0.011), ('use', 0.009), ('org', 0.009), ('metadata', 0.009), ('unzip', 0.009), ('frequency', 0.008)]\n",
            "\n",
            "Topic #11:\n",
            "[('column', 0.033), ('activity', 0.032), ('sensor', 0.021), ('feature', 0.02), ('signal', 0.018), ('right', 0.017), ('using', 0.014), ('left', 0.013), ('driving', 0.013), ('label', 0.011), ('subject', 0.01), ('participant', 0.01), ('open', 0.009), ('information', 0.009), ('class', 0.009), ('recognition', 0.009), ('time', 0.008), ('help', 0.008), ('database', 0.008), ('datasets', 0.008)]\n",
            "\n",
            "Topic #12:\n",
            "[('price', 0.06), ('csv', 0.037), ('de', 0.028), ('stock', 0.023), ('com', 0.021), ('question', 0.02), ('date', 0.019), ('market', 0.015), ('acknowledgement', 0.012), ('http_www', 0.011), ('day', 0.011), ('en', 0.011), ('inspiration', 0.01), ('company', 0.01), ('close', 0.009), ('open', 0.009), ('volume', 0.008), ('la', 0.008), ('index', 0.008), ('per', 0.008)]\n",
            "\n",
            "Topic #13:\n",
            "[('university', 0.231), ('college', 0.042), ('map', 0.028), ('stop', 0.014), ('institute', 0.013), ('chicago', 0.012), ('technology', 0.01), ('file', 0.009), ('washington', 0.008), ('national', 0.008), ('archive', 0.008), ('new', 0.008), ('record', 0.007), ('population', 0.007), ('district', 0.007), ('project', 0.007), ('center', 0.007), ('research', 0.007), ('state', 0.006), ('analysis', 0.006)]\n",
            "\n",
            "Topic #14:\n",
            "[('acknowledgement', 0.011), ('information', 0.01), ('health', 0.01), ('wa', 0.009), ('state', 0.009), ('ha', 0.009), ('inspiration', 0.008), ('government', 0.007), ('company', 0.007), ('federal', 0.007), ('service', 0.007), ('published', 0.006), ('patient', 0.006), ('database', 0.006), ('year', 0.006), ('department', 0.006), ('vehicle', 0.006), ('risk', 0.006), ('doe', 0.005), ('gov', 0.005)]\n",
            "\n",
            "Topic #15:\n",
            "[('crime', 0.041), ('time', 0.041), ('score', 0.038), ('attack', 0.028), ('number', 0.018), ('defense', 0.018), ('submission', 0.014), ('non', 0.014), ('csv', 0.014), ('request', 0.014), ('statistic', 0.014), ('image', 0.013), ('matrix', 0.013), ('police', 0.012), ('file', 0.012), ('run', 0.011), ('name', 0.01), ('date', 0.01), ('id', 0.009), ('team', 0.009)]\n",
            "\n",
            "Topic #16:\n",
            "[('player', 0.039), ('team', 0.034), ('game', 0.028), ('match', 0.021), ('season', 0.014), ('goal', 0.01), ('wa', 0.01), ('com', 0.008), ('table', 0.007), ('point', 0.007), ('result', 0.007), ('play', 0.007), ('information', 0.006), ('league', 0.006), ('pre_trained', 0.006), ('sport', 0.006), ('use', 0.006), ('played', 0.006), ('model', 0.006), ('win', 0.006)]\n",
            "\n",
            "Topic #17:\n",
            "[('car', 0.033), ('inspiration', 0.027), ('largest', 0.021), ('past_research', 0.021), ('science_community', 0.021), ('acquired', 0.02), ('front_world', 0.02), ('question_want', 0.02), ('see_answered', 0.02), ('data_set', 0.02), ('help_others', 0.019), ('represents', 0.019), ('acknowledgement_without', 0.019), ('along_citation', 0.018), ('owe_attribution', 0.018), ('thanks_include', 0.018), ('time_period', 0.018), ('package_id', 0.016), ('content_inside', 0.015), ('every', 0.015)]\n",
            "\n",
            "Topic #18:\n",
            "[('year', 0.033), ('energy', 0.025), ('total', 0.021), ('animal', 0.02), ('india', 0.019), ('solar', 0.017), ('rate', 0.016), ('response', 0.014), ('outcome', 0.013), ('customer', 0.013), ('natural', 0.012), ('space', 0.012), ('earth', 0.011), ('range', 0.01), ('wa', 0.009), ('acknowledgement', 0.008), ('type', 0.008), ('death', 0.008), ('based', 0.008), ('inspiration', 0.008)]\n",
            "\n",
            "Topic #19:\n",
            "[('yet', 0.041), ('doe_description', 0.036), ('collection', 0.026), ('text', 0.026), ('txt', 0.02), ('use', 0.017), ('street', 0.017), ('south', 0.016), ('american', 0.015), ('csv', 0.014), ('library', 0.013), ('number', 0.012), ('city', 0.011), ('name', 0.009), ('digital', 0.009), ('tool', 0.009), ('http', 0.009), ('license', 0.008), ('open', 0.008), ('new_york', 0.007)]\n",
            "\n",
            "Topic #20:\n",
            "[('numeric', 0.061), ('temperature', 0.026), ('university_california', 0.024), ('traffic', 0.017), ('ic_uci', 0.016), ('data_set', 0.015), ('text', 0.015), ('edu_ml', 0.015), ('location', 0.015), ('http_archive', 0.015), ('wind', 0.014), ('fatality', 0.013), ('school', 0.013), ('uci_machine', 0.012), ('weather', 0.012), ('learning_repository', 0.011), ('incident', 0.011), ('acknowledgement', 0.01), ('inspiration', 0.009), ('predict', 0.008)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8TOC8axezZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e69e42dd-d343-4cd2-9af2-792a5e49ada2"
      },
      "source": [
        "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
        "                                                      texts=norm_corpus_bigrams,\n",
        "                                                      dictionary=dictionary, \n",
        "                                                      coherence='c_v')\n",
        "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
        "\n",
        "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
        "                                                         texts=norm_corpus_bigrams,\n",
        "                                                         dictionary=dictionary, \n",
        "                                                         coherence='u_mass')\n",
        "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
        "\n",
        "perplexity = lda_model.log_perplexity(bow_corpus)\n",
        "\n",
        "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
        "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
        "print('Model Perplexity:', perplexity)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Coherence Score (Cv): 0.42230731344440936\n",
            "Avg. Coherence Score (UMass): -2.5180356822814263\n",
            "Model Perplexity: -6.85161118426254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnWEo5vse85t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics = [[(term, round(wt, 3)) \n",
        "               for term, wt in lda_model.show_topic(n, topn=20)] \n",
        "                   for n in range(0, lda_model.num_topics)]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfUS4_F8frVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85e35cd0-c25f-4cd9-bd3b-b64135c3927a"
      },
      "source": [
        "for idx, topic in enumerate(topics):\n",
        "    print('Topic #'+str(idx+1)+':')\n",
        "    print([term for term, wt in topic])\n",
        "    print()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "['university', 'college', 'map', 'stop', 'institute', 'chicago', 'technology', 'file', 'washington', 'national', 'archive', 'new', 'record', 'population', 'district', 'project', 'center', 'research', 'state', 'analysis']\n",
            "\n",
            "Topic #2:\n",
            "['wa', 'time', 'information', 'source', 'many', 'world', 'acknowledgement', 'inspiration', 'available', 'project', 'ha', 'database', 'one', 'date', 'also', 'use', 'country', 'report', 'number', 'people']\n",
            "\n",
            "Topic #3:\n",
            "['image', 'class', 'label', 'instance', 'cell', 'integer', 'sample', 'model', 'number', 'training', 'using', 'file', 'classifier', 'example', 'different', 'wa', 'two', 'machine_learning', 'used', 'test']\n",
            "\n",
            "Topic #4:\n",
            "['word', 'review', 'language', 'corpus', 'name', 'file', 'http', 'license', 'text', 'code', 'english', 'copyright', 'version', 'com', 'list', 'use', 'org', 'metadata', 'unzip', 'frequency']\n",
            "\n",
            "Topic #5:\n",
            "['text', 'article', 'title', 'sentence', 'speech', 'file', 'post', 'line', 'wa', 'id', 'using', 'news', 'corpus', 'ha', 'used', 'inspiration', 'word', 'en', 'google', 'comment']\n",
            "\n",
            "Topic #6:\n",
            "['price', 'csv', 'de', 'stock', 'com', 'question', 'date', 'market', 'acknowledgement', 'http_www', 'day', 'en', 'inspiration', 'company', 'close', 'open', 'volume', 'la', 'index', 'per']\n",
            "\n",
            "Topic #7:\n",
            "['yet', 'doe_description', 'collection', 'text', 'txt', 'use', 'street', 'south', 'american', 'csv', 'library', 'number', 'city', 'name', 'digital', 'tool', 'http', 'license', 'open', 'new_york']\n",
            "\n",
            "Topic #8:\n",
            "['file', 'user', 'movie', 'csv', 'tag', 'rating', 'id', 'contains', 'research', 'use', 'paper', 'information', 'ha', 'one', 'format', 'value', 'data_set', 'used', 'following', 'system']\n",
            "\n",
            "Topic #9:\n",
            "['tweet', 'wa', 'com', 'twitter', 'one', 'time', 'like', 'many', 'inspiration', 'song', 'user', 'column', 'used', 'would', 'site', 'acknowledgement', 'people', 'could', 'find', 'nltk']\n",
            "\n",
            "Topic #10:\n",
            "['attribute', 'area', 'variable', 'building', 'product', 'per', 'ha', 'type', 'day', 'model', 'value', 'number', 'feature', 'cluster', 'using', 'use', 'database', 'one', 'food', 'point']\n",
            "\n",
            "Topic #11:\n",
            "['car', 'inspiration', 'largest', 'past_research', 'science_community', 'acquired', 'front_world', 'question_want', 'see_answered', 'data_set', 'help_others', 'represents', 'acknowledgement_without', 'along_citation', 'owe_attribution', 'thanks_include', 'time_period', 'package_id', 'content_inside', 'every']\n",
            "\n",
            "Topic #12:\n",
            "['column', 'activity', 'sensor', 'feature', 'signal', 'right', 'using', 'left', 'driving', 'label', 'subject', 'participant', 'open', 'information', 'class', 'recognition', 'time', 'help', 'database', 'datasets']\n",
            "\n",
            "Topic #13:\n",
            "['acknowledgement', 'information', 'health', 'wa', 'state', 'ha', 'inspiration', 'government', 'company', 'federal', 'service', 'published', 'patient', 'database', 'year', 'department', 'vehicle', 'risk', 'doe', 'gov']\n",
            "\n",
            "Topic #14:\n",
            "['name', 'code', 'value', 'fire', 'county', 'state', 'air', 'row', 'csv', 'day', 'year', 'number', 'specie', 'event', 'unit', 'hour', 'time', 'mean', 'date', 'measured']\n",
            "\n",
            "Topic #15:\n",
            "['year', 'energy', 'total', 'animal', 'india', 'solar', 'rate', 'response', 'outcome', 'customer', 'natural', 'space', 'earth', 'range', 'wa', 'acknowledgement', 'type', 'death', 'based', 'inspiration']\n",
            "\n",
            "Topic #16:\n",
            "['player', 'team', 'game', 'match', 'season', 'goal', 'wa', 'com', 'table', 'point', 'result', 'play', 'information', 'league', 'pre_trained', 'sport', 'use', 'played', 'model', 'win']\n",
            "\n",
            "Topic #17:\n",
            "['city', 'time', 'information', 'number', 'location', 'date', 'new_york', 'property', 'acknowledgement', 'election', 'event', 'restaurant', 'name', 'includes', 'one', 'inspiration', 'station', 'wa', 'food', 'day']\n",
            "\n",
            "Topic #18:\n",
            "['numeric', 'temperature', 'university_california', 'traffic', 'ic_uci', 'data_set', 'text', 'edu_ml', 'location', 'http_archive', 'wind', 'fatality', 'school', 'uci_machine', 'weather', 'learning_repository', 'incident', 'acknowledgement', 'inspiration', 'predict']\n",
            "\n",
            "Topic #19:\n",
            "['year', 'country', 'survey', 'age', 'number', 'state', 'name', 'variable', 'population', 'month', 'source', 'information', 'education', 'student', 'gender', 'policy', 'acknowledgement', 'wa', 'data_set', 'rate']\n",
            "\n",
            "Topic #20:\n",
            "['crime', 'time', 'score', 'attack', 'number', 'defense', 'submission', 'non', 'csv', 'request', 'statistic', 'image', 'matrix', 'police', 'file', 'run', 'name', 'date', 'id', 'team']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eI8KbDmfvmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "c8ef99bd-92d8-4de6-d47c-8ff51d205630"
      },
      "source": [
        "topics_df = pd.DataFrame([[term for term, wt in topic] \n",
        "                              for topic in topics], \n",
        "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
        "                         index=['Topic '+str(t) for t in range(1, lda_model.num_topics+1)]).T\n",
        "topics_df"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "      <th>Topic 4</th>\n",
              "      <th>Topic 5</th>\n",
              "      <th>Topic 6</th>\n",
              "      <th>Topic 7</th>\n",
              "      <th>Topic 8</th>\n",
              "      <th>Topic 9</th>\n",
              "      <th>Topic 10</th>\n",
              "      <th>Topic 11</th>\n",
              "      <th>Topic 12</th>\n",
              "      <th>Topic 13</th>\n",
              "      <th>Topic 14</th>\n",
              "      <th>Topic 15</th>\n",
              "      <th>Topic 16</th>\n",
              "      <th>Topic 17</th>\n",
              "      <th>Topic 18</th>\n",
              "      <th>Topic 19</th>\n",
              "      <th>Topic 20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Term1</th>\n",
              "      <td>university</td>\n",
              "      <td>wa</td>\n",
              "      <td>image</td>\n",
              "      <td>word</td>\n",
              "      <td>text</td>\n",
              "      <td>price</td>\n",
              "      <td>yet</td>\n",
              "      <td>file</td>\n",
              "      <td>tweet</td>\n",
              "      <td>attribute</td>\n",
              "      <td>car</td>\n",
              "      <td>column</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>name</td>\n",
              "      <td>year</td>\n",
              "      <td>player</td>\n",
              "      <td>city</td>\n",
              "      <td>numeric</td>\n",
              "      <td>year</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term2</th>\n",
              "      <td>college</td>\n",
              "      <td>time</td>\n",
              "      <td>class</td>\n",
              "      <td>review</td>\n",
              "      <td>article</td>\n",
              "      <td>csv</td>\n",
              "      <td>doe_description</td>\n",
              "      <td>user</td>\n",
              "      <td>wa</td>\n",
              "      <td>area</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>activity</td>\n",
              "      <td>information</td>\n",
              "      <td>code</td>\n",
              "      <td>energy</td>\n",
              "      <td>team</td>\n",
              "      <td>time</td>\n",
              "      <td>temperature</td>\n",
              "      <td>country</td>\n",
              "      <td>time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term3</th>\n",
              "      <td>map</td>\n",
              "      <td>information</td>\n",
              "      <td>label</td>\n",
              "      <td>language</td>\n",
              "      <td>title</td>\n",
              "      <td>de</td>\n",
              "      <td>collection</td>\n",
              "      <td>movie</td>\n",
              "      <td>com</td>\n",
              "      <td>variable</td>\n",
              "      <td>largest</td>\n",
              "      <td>sensor</td>\n",
              "      <td>health</td>\n",
              "      <td>value</td>\n",
              "      <td>total</td>\n",
              "      <td>game</td>\n",
              "      <td>information</td>\n",
              "      <td>university_california</td>\n",
              "      <td>survey</td>\n",
              "      <td>score</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term4</th>\n",
              "      <td>stop</td>\n",
              "      <td>source</td>\n",
              "      <td>instance</td>\n",
              "      <td>corpus</td>\n",
              "      <td>sentence</td>\n",
              "      <td>stock</td>\n",
              "      <td>text</td>\n",
              "      <td>csv</td>\n",
              "      <td>twitter</td>\n",
              "      <td>building</td>\n",
              "      <td>past_research</td>\n",
              "      <td>feature</td>\n",
              "      <td>wa</td>\n",
              "      <td>fire</td>\n",
              "      <td>animal</td>\n",
              "      <td>match</td>\n",
              "      <td>number</td>\n",
              "      <td>traffic</td>\n",
              "      <td>age</td>\n",
              "      <td>attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term5</th>\n",
              "      <td>institute</td>\n",
              "      <td>many</td>\n",
              "      <td>cell</td>\n",
              "      <td>name</td>\n",
              "      <td>speech</td>\n",
              "      <td>com</td>\n",
              "      <td>txt</td>\n",
              "      <td>tag</td>\n",
              "      <td>one</td>\n",
              "      <td>product</td>\n",
              "      <td>science_community</td>\n",
              "      <td>signal</td>\n",
              "      <td>state</td>\n",
              "      <td>county</td>\n",
              "      <td>india</td>\n",
              "      <td>season</td>\n",
              "      <td>location</td>\n",
              "      <td>ic_uci</td>\n",
              "      <td>number</td>\n",
              "      <td>number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term6</th>\n",
              "      <td>chicago</td>\n",
              "      <td>world</td>\n",
              "      <td>integer</td>\n",
              "      <td>file</td>\n",
              "      <td>file</td>\n",
              "      <td>question</td>\n",
              "      <td>use</td>\n",
              "      <td>rating</td>\n",
              "      <td>time</td>\n",
              "      <td>per</td>\n",
              "      <td>acquired</td>\n",
              "      <td>right</td>\n",
              "      <td>ha</td>\n",
              "      <td>state</td>\n",
              "      <td>solar</td>\n",
              "      <td>goal</td>\n",
              "      <td>date</td>\n",
              "      <td>data_set</td>\n",
              "      <td>state</td>\n",
              "      <td>defense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term7</th>\n",
              "      <td>technology</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>sample</td>\n",
              "      <td>http</td>\n",
              "      <td>post</td>\n",
              "      <td>date</td>\n",
              "      <td>street</td>\n",
              "      <td>id</td>\n",
              "      <td>like</td>\n",
              "      <td>ha</td>\n",
              "      <td>front_world</td>\n",
              "      <td>using</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>air</td>\n",
              "      <td>rate</td>\n",
              "      <td>wa</td>\n",
              "      <td>new_york</td>\n",
              "      <td>text</td>\n",
              "      <td>name</td>\n",
              "      <td>submission</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term8</th>\n",
              "      <td>file</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>model</td>\n",
              "      <td>license</td>\n",
              "      <td>line</td>\n",
              "      <td>market</td>\n",
              "      <td>south</td>\n",
              "      <td>contains</td>\n",
              "      <td>many</td>\n",
              "      <td>type</td>\n",
              "      <td>question_want</td>\n",
              "      <td>left</td>\n",
              "      <td>government</td>\n",
              "      <td>row</td>\n",
              "      <td>response</td>\n",
              "      <td>com</td>\n",
              "      <td>property</td>\n",
              "      <td>edu_ml</td>\n",
              "      <td>variable</td>\n",
              "      <td>non</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term9</th>\n",
              "      <td>washington</td>\n",
              "      <td>available</td>\n",
              "      <td>number</td>\n",
              "      <td>text</td>\n",
              "      <td>wa</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>american</td>\n",
              "      <td>research</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>day</td>\n",
              "      <td>see_answered</td>\n",
              "      <td>driving</td>\n",
              "      <td>company</td>\n",
              "      <td>csv</td>\n",
              "      <td>outcome</td>\n",
              "      <td>table</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>location</td>\n",
              "      <td>population</td>\n",
              "      <td>csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term10</th>\n",
              "      <td>national</td>\n",
              "      <td>project</td>\n",
              "      <td>training</td>\n",
              "      <td>code</td>\n",
              "      <td>id</td>\n",
              "      <td>http_www</td>\n",
              "      <td>csv</td>\n",
              "      <td>use</td>\n",
              "      <td>song</td>\n",
              "      <td>model</td>\n",
              "      <td>data_set</td>\n",
              "      <td>label</td>\n",
              "      <td>federal</td>\n",
              "      <td>day</td>\n",
              "      <td>customer</td>\n",
              "      <td>point</td>\n",
              "      <td>election</td>\n",
              "      <td>http_archive</td>\n",
              "      <td>month</td>\n",
              "      <td>request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term11</th>\n",
              "      <td>archive</td>\n",
              "      <td>ha</td>\n",
              "      <td>using</td>\n",
              "      <td>english</td>\n",
              "      <td>using</td>\n",
              "      <td>day</td>\n",
              "      <td>library</td>\n",
              "      <td>paper</td>\n",
              "      <td>user</td>\n",
              "      <td>value</td>\n",
              "      <td>help_others</td>\n",
              "      <td>subject</td>\n",
              "      <td>service</td>\n",
              "      <td>year</td>\n",
              "      <td>natural</td>\n",
              "      <td>result</td>\n",
              "      <td>event</td>\n",
              "      <td>wind</td>\n",
              "      <td>source</td>\n",
              "      <td>statistic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term12</th>\n",
              "      <td>new</td>\n",
              "      <td>database</td>\n",
              "      <td>file</td>\n",
              "      <td>copyright</td>\n",
              "      <td>news</td>\n",
              "      <td>en</td>\n",
              "      <td>number</td>\n",
              "      <td>information</td>\n",
              "      <td>column</td>\n",
              "      <td>number</td>\n",
              "      <td>represents</td>\n",
              "      <td>participant</td>\n",
              "      <td>published</td>\n",
              "      <td>number</td>\n",
              "      <td>space</td>\n",
              "      <td>play</td>\n",
              "      <td>restaurant</td>\n",
              "      <td>fatality</td>\n",
              "      <td>information</td>\n",
              "      <td>image</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term13</th>\n",
              "      <td>record</td>\n",
              "      <td>one</td>\n",
              "      <td>classifier</td>\n",
              "      <td>version</td>\n",
              "      <td>corpus</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>city</td>\n",
              "      <td>ha</td>\n",
              "      <td>used</td>\n",
              "      <td>feature</td>\n",
              "      <td>acknowledgement_without</td>\n",
              "      <td>open</td>\n",
              "      <td>patient</td>\n",
              "      <td>specie</td>\n",
              "      <td>earth</td>\n",
              "      <td>information</td>\n",
              "      <td>name</td>\n",
              "      <td>school</td>\n",
              "      <td>education</td>\n",
              "      <td>matrix</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term14</th>\n",
              "      <td>population</td>\n",
              "      <td>date</td>\n",
              "      <td>example</td>\n",
              "      <td>com</td>\n",
              "      <td>ha</td>\n",
              "      <td>company</td>\n",
              "      <td>name</td>\n",
              "      <td>one</td>\n",
              "      <td>would</td>\n",
              "      <td>cluster</td>\n",
              "      <td>along_citation</td>\n",
              "      <td>information</td>\n",
              "      <td>database</td>\n",
              "      <td>event</td>\n",
              "      <td>range</td>\n",
              "      <td>league</td>\n",
              "      <td>includes</td>\n",
              "      <td>uci_machine</td>\n",
              "      <td>student</td>\n",
              "      <td>police</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term15</th>\n",
              "      <td>district</td>\n",
              "      <td>also</td>\n",
              "      <td>different</td>\n",
              "      <td>list</td>\n",
              "      <td>used</td>\n",
              "      <td>close</td>\n",
              "      <td>digital</td>\n",
              "      <td>format</td>\n",
              "      <td>site</td>\n",
              "      <td>using</td>\n",
              "      <td>owe_attribution</td>\n",
              "      <td>class</td>\n",
              "      <td>year</td>\n",
              "      <td>unit</td>\n",
              "      <td>wa</td>\n",
              "      <td>pre_trained</td>\n",
              "      <td>one</td>\n",
              "      <td>weather</td>\n",
              "      <td>gender</td>\n",
              "      <td>file</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term16</th>\n",
              "      <td>project</td>\n",
              "      <td>use</td>\n",
              "      <td>wa</td>\n",
              "      <td>use</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>open</td>\n",
              "      <td>tool</td>\n",
              "      <td>value</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>use</td>\n",
              "      <td>thanks_include</td>\n",
              "      <td>recognition</td>\n",
              "      <td>department</td>\n",
              "      <td>hour</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>sport</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>learning_repository</td>\n",
              "      <td>policy</td>\n",
              "      <td>run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term17</th>\n",
              "      <td>center</td>\n",
              "      <td>country</td>\n",
              "      <td>two</td>\n",
              "      <td>org</td>\n",
              "      <td>word</td>\n",
              "      <td>volume</td>\n",
              "      <td>http</td>\n",
              "      <td>data_set</td>\n",
              "      <td>people</td>\n",
              "      <td>database</td>\n",
              "      <td>time_period</td>\n",
              "      <td>time</td>\n",
              "      <td>vehicle</td>\n",
              "      <td>time</td>\n",
              "      <td>type</td>\n",
              "      <td>use</td>\n",
              "      <td>station</td>\n",
              "      <td>incident</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term18</th>\n",
              "      <td>research</td>\n",
              "      <td>report</td>\n",
              "      <td>machine_learning</td>\n",
              "      <td>metadata</td>\n",
              "      <td>en</td>\n",
              "      <td>la</td>\n",
              "      <td>license</td>\n",
              "      <td>used</td>\n",
              "      <td>could</td>\n",
              "      <td>one</td>\n",
              "      <td>package_id</td>\n",
              "      <td>help</td>\n",
              "      <td>risk</td>\n",
              "      <td>mean</td>\n",
              "      <td>death</td>\n",
              "      <td>played</td>\n",
              "      <td>wa</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>wa</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term19</th>\n",
              "      <td>state</td>\n",
              "      <td>number</td>\n",
              "      <td>used</td>\n",
              "      <td>unzip</td>\n",
              "      <td>google</td>\n",
              "      <td>index</td>\n",
              "      <td>open</td>\n",
              "      <td>following</td>\n",
              "      <td>find</td>\n",
              "      <td>food</td>\n",
              "      <td>content_inside</td>\n",
              "      <td>database</td>\n",
              "      <td>doe</td>\n",
              "      <td>date</td>\n",
              "      <td>based</td>\n",
              "      <td>model</td>\n",
              "      <td>food</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>data_set</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term20</th>\n",
              "      <td>analysis</td>\n",
              "      <td>people</td>\n",
              "      <td>test</td>\n",
              "      <td>frequency</td>\n",
              "      <td>comment</td>\n",
              "      <td>per</td>\n",
              "      <td>new_york</td>\n",
              "      <td>system</td>\n",
              "      <td>nltk</td>\n",
              "      <td>point</td>\n",
              "      <td>every</td>\n",
              "      <td>datasets</td>\n",
              "      <td>gov</td>\n",
              "      <td>measured</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>win</td>\n",
              "      <td>day</td>\n",
              "      <td>predict</td>\n",
              "      <td>rate</td>\n",
              "      <td>team</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Topic 1          Topic 2  ...         Topic 19    Topic 20\n",
              "Term1   university               wa  ...             year       crime\n",
              "Term2      college             time  ...          country        time\n",
              "Term3          map      information  ...           survey       score\n",
              "Term4         stop           source  ...              age      attack\n",
              "Term5    institute             many  ...           number      number\n",
              "Term6      chicago            world  ...            state     defense\n",
              "Term7   technology  acknowledgement  ...             name  submission\n",
              "Term8         file      inspiration  ...         variable         non\n",
              "Term9   washington        available  ...       population         csv\n",
              "Term10    national          project  ...            month     request\n",
              "Term11     archive               ha  ...           source   statistic\n",
              "Term12         new         database  ...      information       image\n",
              "Term13      record              one  ...        education      matrix\n",
              "Term14  population             date  ...          student      police\n",
              "Term15    district             also  ...           gender        file\n",
              "Term16     project              use  ...           policy         run\n",
              "Term17      center          country  ...  acknowledgement        name\n",
              "Term18    research           report  ...               wa        date\n",
              "Term19       state           number  ...         data_set          id\n",
              "Term20    analysis           people  ...             rate        team\n",
              "\n",
              "[20 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blVMu1Npf0y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb7c51ce-5c9d-45a3-dd0e-e9e26b468e6e"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
        "                              for topic in topics],\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, lda_model.num_topics+1)]\n",
        "                         )\n",
        "topics_df"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>university, college, map, stop, institute, chicago, technology, file, washington, national, archive, new, record, population, district, project, center, research, state, analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>image, class, label, instance, cell, integer, sample, model, number, training, using, file, classifier, example, different, wa, two, machine_learning, used, test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>text, article, title, sentence, speech, file, post, line, wa, id, using, news, corpus, ha, used, inspiration, word, en, google, comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>price, csv, de, stock, com, question, date, market, acknowledgement, http_www, day, en, inspiration, company, close, open, volume, la, index, per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>yet, doe_description, collection, text, txt, use, street, south, american, csv, library, number, city, name, digital, tool, http, license, open, new_york</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>file, user, movie, csv, tag, rating, id, contains, research, use, paper, information, ha, one, format, value, data_set, used, following, system</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>car, inspiration, largest, past_research, science_community, acquired, front_world, question_want, see_answered, data_set, help_others, represents, acknowledgement_without, along_citation, owe_attribution, thanks_include, time_period, package_id, content_inside, every</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>column, activity, sensor, feature, signal, right, using, left, driving, label, subject, participant, open, information, class, recognition, time, help, database, datasets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>name, code, value, fire, county, state, air, row, csv, day, year, number, specie, event, unit, hour, time, mean, date, measured</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>year, energy, total, animal, india, solar, rate, response, outcome, customer, natural, space, earth, range, wa, acknowledgement, type, death, based, inspiration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>city, time, information, number, location, date, new_york, property, acknowledgement, election, event, restaurant, name, includes, one, inspiration, station, wa, food, day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowledgement, inspiration, predict</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>crime, time, score, attack, number, defense, submission, non, csv, request, statistic, image, matrix, police, file, run, name, date, id, team</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                      Terms per Topic\n",
              "Topic1   university, college, map, stop, institute, chicago, technology, file, washington, national, archive, new, record, population, district, project, center, research, state, analysis                                                                                          \n",
              "Topic2   wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people                                                                                                           \n",
              "Topic3   image, class, label, instance, cell, integer, sample, model, number, training, using, file, classifier, example, different, wa, two, machine_learning, used, test                                                                                                           \n",
              "Topic4   word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency                                                                                                                         \n",
              "Topic5   text, article, title, sentence, speech, file, post, line, wa, id, using, news, corpus, ha, used, inspiration, word, en, google, comment                                                                                                                                     \n",
              "Topic6   price, csv, de, stock, com, question, date, market, acknowledgement, http_www, day, en, inspiration, company, close, open, volume, la, index, per                                                                                                                           \n",
              "Topic7   yet, doe_description, collection, text, txt, use, street, south, american, csv, library, number, city, name, digital, tool, http, license, open, new_york                                                                                                                   \n",
              "Topic8   file, user, movie, csv, tag, rating, id, contains, research, use, paper, information, ha, one, format, value, data_set, used, following, system                                                                                                                             \n",
              "Topic9   tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk                                                                                                                              \n",
              "Topic10  attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point                                                                                                                            \n",
              "Topic11  car, inspiration, largest, past_research, science_community, acquired, front_world, question_want, see_answered, data_set, help_others, represents, acknowledgement_without, along_citation, owe_attribution, thanks_include, time_period, package_id, content_inside, every\n",
              "Topic12  column, activity, sensor, feature, signal, right, using, left, driving, label, subject, participant, open, information, class, recognition, time, help, database, datasets                                                                                                  \n",
              "Topic13  acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov                                                                                            \n",
              "Topic14  name, code, value, fire, county, state, air, row, csv, day, year, number, specie, event, unit, hour, time, mean, date, measured                                                                                                                                             \n",
              "Topic15  year, energy, total, animal, india, solar, rate, response, outcome, customer, natural, space, earth, range, wa, acknowledgement, type, death, based, inspiration                                                                                                            \n",
              "Topic16  player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win                                                                                                                              \n",
              "Topic17  city, time, information, number, location, date, new_york, property, acknowledgement, election, event, restaurant, name, includes, one, inspiration, station, wa, food, day                                                                                                 \n",
              "Topic18  numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowledgement, inspiration, predict                                            \n",
              "Topic19  year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate                                                                                                  \n",
              "Topic20  crime, time, score, attack, number, defense, submission, non, csv, request, statistic, image, matrix, police, file, run, name, date, id, team                                                                                                                               "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK67-x2dgULw",
        "colab_type": "text"
      },
      "source": [
        "## Interpreting Topic Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-pNi8H0gBli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tm_results = lda_model[bow_corpus]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4mfPgPKgYPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cf45e005-99d4-4aee-e04f-feae6702c053"
      },
      "source": [
        "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] \n",
        "                     for topics in tm_results]\n",
        "corpus_topics[:5]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, 0.5704999),\n",
              " (15, 0.5066352),\n",
              " (1, 0.4261907),\n",
              " (1, 0.32978037),\n",
              " (5, 0.447578)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PtIwnb3gZJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_topic_df = pd.DataFrame()\n",
        "corpus_topic_df['Document'] = range(0, len(papers))\n",
        "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
        "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
        "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
        "corpus_topic_df['Paper'] = papers"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mZl0bvDgcGX",
        "colab_type": "text"
      },
      "source": [
        "### Dominant Topics Distribution across Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srKjC-qFjy0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "1eade1db-0fea-4651-a89f-c0f29e926cde"
      },
      "source": [
        "corpus_topic_df.head(2)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>57.05</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>50.66</td>\n",
              "      <td>player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win</td>\n",
              "      <td>The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlay...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document  ...                                                                                                                                                                                                    Paper\n",
              "0         0  ...  The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...\n",
              "1         1  ...  The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlay...\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePOdQJxFkS-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1faba6f-4721-4b3b-8c03-73cd9455f91c"
      },
      "source": [
        "corpus_topic_df.columns"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Document', 'Dominant Topic', 'Contribution %', 'Topic Desc', 'Paper'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzvm7j3Og77a",
        "colab_type": "text"
      },
      "source": [
        "### Dominant Topics in Specific Research Papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodNBQD5geiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2c1cb1d-d2eb-45fc-a32e-f6ba057dc669"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 200)\n",
        "(corpus_topic_df[corpus_topic_df['Document']\n",
        "                 .isin([681, 9, 392, 1622, 17, \n",
        "                        906, 996, 503, 13, 733])])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>43.52</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>99.67</td>\n",
              "      <td>numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowled...</td>\n",
              "      <td>A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>53.98</td>\n",
              "      <td>word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency</td>\n",
              "      <td>Context\\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>392</td>\n",
              "      <td>2</td>\n",
              "      <td>34.49</td>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "      <td>Context\\nBible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>503</td>\n",
              "      <td>19</td>\n",
              "      <td>44.98</td>\n",
              "      <td>year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate</td>\n",
              "      <td>Description:\\nHappyDB is a corpus of more than 100,000 happy moments crowd-sourced via Amazon’s Mechanical Turk.\\nEach worker is given the following task: What made you happy today? Reflect on the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681</th>\n",
              "      <td>681</td>\n",
              "      <td>10</td>\n",
              "      <td>56.89</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>Build the proposed optimized pricing model: Determine the largest or key value driver from the data Build price segments using product characteristics, distribution channel, behavior and demograph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>733</td>\n",
              "      <td>2</td>\n",
              "      <td>43.54</td>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "      <td>Context\\nThis dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>906</td>\n",
              "      <td>13</td>\n",
              "      <td>33.67</td>\n",
              "      <td>acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov</td>\n",
              "      <td>Context\\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>9</td>\n",
              "      <td>64.77</td>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "      <td>Context\\nAs a huge LOTR fan, I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here: https://github.com/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>1622</td>\n",
              "      <td>9</td>\n",
              "      <td>44.45</td>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "      <td>Context\\nI started to scrape TripAdvisor reviews for a personal project on Sentiment Analysis. I thought it could be good to share my data on Kaggle, since this can help other with similar ideas.\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Document  ...                                                                                                                                                                                                    Paper\n",
              "9            9  ...  This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use ...\n",
              "13          13  ...  A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...\n",
              "17          17  ...  Context\\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and us...\n",
              "392        392  ...  Context\\nBible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between G...\n",
              "503        503  ...  Description:\\nHappyDB is a corpus of more than 100,000 happy moments crowd-sourced via Amazon’s Mechanical Turk.\\nEach worker is given the following task: What made you happy today? Reflect on the...\n",
              "681        681  ...  Build the proposed optimized pricing model: Determine the largest or key value driver from the data Build price segments using product characteristics, distribution channel, behavior and demograph...\n",
              "733        733  ...  Context\\nThis dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelo...\n",
              "906        906  ...  Context\\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includ...\n",
              "996        996  ...  Context\\nAs a huge LOTR fan, I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here: https://github.com/...\n",
              "1622      1622  ...  Context\\nI started to scrape TripAdvisor reviews for a personal project on Sentiment Analysis. I thought it could be good to share my data on Kaggle, since this can help other with similar ideas.\\...\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXXJjDFng5Jq",
        "colab_type": "text"
      },
      "source": [
        "### Relevant Research Papers per Topic based on Dominance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb56VzB4gu3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c19ca03b-d610-436b-c2c9-2b4138eb28a7"
      },
      "source": [
        "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], \n",
        "                                                                                         ascending=False)\n",
        "                                                                             .iloc[0]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>722</td>\n",
              "      <td>1</td>\n",
              "      <td>99.35</td>\n",
              "      <td>university, college, map, stop, institute, chicago, technology, file, washington, national, archive, new, record, population, district, project, center, research, state, analysis</td>\n",
              "      <td>Context\\nInformation reproduced from the National Archives:\\n\"The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58,220 U.S. mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1930</td>\n",
              "      <td>2</td>\n",
              "      <td>98.50</td>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "      <td>82.558 Human Instructions in Chinese Extracted from wikiHow\\nStep-by-step instructions in Chinese extracted from wikiHow and decomposed into a formal graph representation in RDF.\\nThis is one of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>610</td>\n",
              "      <td>3</td>\n",
              "      <td>98.90</td>\n",
              "      <td>image, class, label, instance, cell, integer, sample, model, number, training, using, file, classifier, example, different, wa, two, machine_learning, used, test</td>\n",
              "      <td>This dataset contains 16,000 images of four shapes; square, star, circle, and triangle. Each image is 200x200 pixels.\\nThe data was collected using a Garmin Virb 1080p action camera. The shapes we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2081</td>\n",
              "      <td>4</td>\n",
              "      <td>97.81</td>\n",
              "      <td>word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency</td>\n",
              "      <td>Context\\nFastText word embeddings trained on English wikipedia\\nFastText embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.\\nContent\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1860</td>\n",
              "      <td>5</td>\n",
              "      <td>99.00</td>\n",
              "      <td>text, article, title, sentence, speech, file, post, line, wa, id, using, news, corpus, ha, used, inspiration, word, en, google, comment</td>\n",
              "      <td>Context:\\nYoutube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Robert Phoenix The 11th House upload...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>812</td>\n",
              "      <td>6</td>\n",
              "      <td>99.59</td>\n",
              "      <td>price, csv, de, stock, com, question, date, market, acknowledgement, http_www, day, en, inspiration, company, close, open, volume, la, index, per</td>\n",
              "      <td>«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1197</td>\n",
              "      <td>7</td>\n",
              "      <td>99.17</td>\n",
              "      <td>yet, doe_description, collection, text, txt, use, street, south, american, csv, library, number, city, name, digital, tool, http, license, open, new_york</td>\n",
              "      <td>Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1963</td>\n",
              "      <td>8</td>\n",
              "      <td>98.54</td>\n",
              "      <td>file, user, movie, csv, tag, rating, id, contains, research, use, paper, information, ha, one, format, value, data_set, used, following, system</td>\n",
              "      <td>Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2064</td>\n",
              "      <td>9</td>\n",
              "      <td>98.78</td>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "      <td>Context\\nWalking around a Total Wine one day I wondered if there was any data I could find that would help me figure out what new rums to try, I later was able to find some information on rumratin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1805</td>\n",
              "      <td>10</td>\n",
              "      <td>97.90</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>Context\\nThere are 4933 pharmacies in Belgium, and each pharmacy (in groups) are obliged to create a network of night-guard pharmacies covering complete Belgium. Compare it with a hospital that ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2134</td>\n",
              "      <td>11</td>\n",
              "      <td>98.10</td>\n",
              "      <td>car, inspiration, largest, past_research, science_community, acquired, front_world, question_want, see_answered, data_set, help_others, represents, acknowledgement_without, along_citation, owe_att...</td>\n",
              "      <td>Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2103</td>\n",
              "      <td>12</td>\n",
              "      <td>95.41</td>\n",
              "      <td>column, activity, sensor, feature, signal, right, using, left, driving, label, subject, participant, open, information, class, recognition, time, help, database, datasets</td>\n",
              "      <td>This directory contains the cross-position activity recognition datasets used in the following paper. Please consider citing this article if you want to use the datasets.\\nJindong Wang, Yiqiang Ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2131</td>\n",
              "      <td>13</td>\n",
              "      <td>99.07</td>\n",
              "      <td>acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov</td>\n",
              "      <td>310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>416</td>\n",
              "      <td>14</td>\n",
              "      <td>99.88</td>\n",
              "      <td>name, code, value, fire, county, state, air, row, csv, day, year, number, specie, event, unit, hour, time, mean, date, measured</td>\n",
              "      <td>Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>238</td>\n",
              "      <td>15</td>\n",
              "      <td>96.84</td>\n",
              "      <td>year, energy, total, animal, india, solar, rate, response, outcome, customer, natural, space, earth, range, wa, acknowledgement, type, death, based, inspiration</td>\n",
              "      <td>The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1881</td>\n",
              "      <td>16</td>\n",
              "      <td>99.62</td>\n",
              "      <td>player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win</td>\n",
              "      <td>Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1907</td>\n",
              "      <td>17</td>\n",
              "      <td>98.22</td>\n",
              "      <td>city, time, information, number, location, date, new_york, property, acknowledgement, election, event, restaurant, name, includes, one, inspiration, station, wa, food, day</td>\n",
              "      <td>Context\\nThis dataset contains a subset of information, pertaining to the weather patterns during Januaray 2016 - June 2016 in NYC. This may be important to those who are looking to add columns to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>99.67</td>\n",
              "      <td>numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowled...</td>\n",
              "      <td>A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2067</td>\n",
              "      <td>19</td>\n",
              "      <td>99.26</td>\n",
              "      <td>year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate</td>\n",
              "      <td>Context\\nThe United States Census Bureau conducts regular surveys to assess education levels in the U.S. These surveys sample participants' highest levels of education (i.e. high school diploma, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1794</td>\n",
              "      <td>20</td>\n",
              "      <td>99.60</td>\n",
              "      <td>crime, time, score, attack, number, defense, submission, non, csv, request, statistic, image, matrix, police, file, run, name, date, id, team</td>\n",
              "      <td>This dataset contains run time statistics and details about scores for the second development round of NIPS 2017 Adversarial learning competition\\nContent\\nMatrices with intermediate results\\nFoll...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Document  ...                                                                                                                                                                                                    Paper\n",
              "Dominant Topic            ...                                                                                                                                                                                                         \n",
              "1                    722  ...  Context\\nInformation reproduced from the National Archives:\\n\"The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58,220 U.S. mi...\n",
              "2                   1930  ...  82.558 Human Instructions in Chinese Extracted from wikiHow\\nStep-by-step instructions in Chinese extracted from wikiHow and decomposed into a formal graph representation in RDF.\\nThis is one of m...\n",
              "3                    610  ...  This dataset contains 16,000 images of four shapes; square, star, circle, and triangle. Each image is 200x200 pixels.\\nThe data was collected using a Garmin Virb 1080p action camera. The shapes we...\n",
              "4                   2081  ...  Context\\nFastText word embeddings trained on English wikipedia\\nFastText embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.\\nContent\\n...\n",
              "5                   1860  ...  Context:\\nYoutube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Robert Phoenix The 11th House upload...\n",
              "6                    812  ...  «Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...\n",
              "7                   1197  ...  Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...\n",
              "8                   1963  ...  Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...\n",
              "9                   2064  ...  Context\\nWalking around a Total Wine one day I wondered if there was any data I could find that would help me figure out what new rums to try, I later was able to find some information on rumratin...\n",
              "10                  1805  ...  Context\\nThere are 4933 pharmacies in Belgium, and each pharmacy (in groups) are obliged to create a network of night-guard pharmacies covering complete Belgium. Compare it with a hospital that ha...\n",
              "11                  2134  ...  Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...\n",
              "12                  2103  ...  This directory contains the cross-position activity recognition datasets used in the following paper. Please consider citing this article if you want to use the datasets.\\nJindong Wang, Yiqiang Ch...\n",
              "13                  2131  ...  310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...\n",
              "14                   416  ...  Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...\n",
              "15                   238  ...  The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...\n",
              "16                  1881  ...  Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...\n",
              "17                  1907  ...  Context\\nThis dataset contains a subset of information, pertaining to the weather patterns during Januaray 2016 - June 2016 in NYC. This may be important to those who are looking to add columns to...\n",
              "18                    13  ...  A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...\n",
              "19                  2067  ...  Context\\nThe United States Census Bureau conducts regular surveys to assess education levels in the U.S. These surveys sample participants' highest levels of education (i.e. high school diploma, b...\n",
              "20                  1794  ...  This dataset contains run time statistics and details about scores for the second development round of NIPS 2017 Adversarial learning competition\\nContent\\nMatrices with intermediate results\\nFoll...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3OYd7GlEIk",
        "colab_type": "text"
      },
      "source": [
        "#Ch06c-Topic model with sk learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqny43nigwki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "9b70197d-ba53-4fb3-97fd-6a2a353e8f83"
      },
      "source": [
        "data=pd.read_csv('https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/voted-kaggle-dataset.csv?raw=true')\n",
        "print(data.shape)\n",
        "data.head(1)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Subtitle</th>\n",
              "      <th>Owner</th>\n",
              "      <th>Votes</th>\n",
              "      <th>Versions</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Data Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>License</th>\n",
              "      <th>Views</th>\n",
              "      <th>Download</th>\n",
              "      <th>Kernels</th>\n",
              "      <th>Topics</th>\n",
              "      <th>URL</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Credit Card Fraud Detection</td>\n",
              "      <td>Anonymized credit card transactions labeled as fraudulent or genuine</td>\n",
              "      <td>Machine Learning Group - ULB</td>\n",
              "      <td>1241</td>\n",
              "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
              "      <td>crime\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>442,136 views</td>\n",
              "      <td>53,128 downloads</td>\n",
              "      <td>1,782 kernels</td>\n",
              "      <td>26 topics</td>\n",
              "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
              "      <td>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Title  ...                                                                                                                                                                                              Description\n",
              "0  Credit Card Fraud Detection  ...  The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMcVpjAVlrz5",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54T9gLb-lXYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ac0b347d-66eb-4739-d9a5-9038a10b7b1a"
      },
      "source": [
        "%%time\n",
        "papers=papers.astype(str)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(papers):\n",
        "    norm_papers = []\n",
        "    for paper in papers:\n",
        "        paper = paper.lower()\n",
        "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "        paper_tokens = list(filter(None, paper_tokens))\n",
        "        if paper_tokens:\n",
        "            norm_papers.append(paper_tokens)\n",
        "            \n",
        "    return norm_papers\n",
        "    \n",
        "norm_papers = normalize_corpus(papers)\n",
        "print(len(norm_papers))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "2150\n",
            "CPU times: user 2.85 s, sys: 19.4 ms, total: 2.87 s\n",
            "Wall time: 2.96 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIM0mr3ellUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "23f55276-a55f-496f-a999-4d71144b1e7f"
      },
      "source": [
        "print(norm_papers[0])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ZY5s7blt0T",
        "colab_type": "text"
      },
      "source": [
        "## Text Representation with Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16LMfEZ4lpuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "748655dc-d54b-4df9-c36c-9f3c141292b2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2),\n",
        "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
        "                     preprocessor=lambda doc: doc)\n",
        "cv_features = cv.fit_transform(norm_papers)\n",
        "cv_features.shape"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2150, 2097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU9NQzDilwT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf6e9a91-3cb7-4468-a983-cf5dcf910c2f"
      },
      "source": [
        "vocabulary = np.array(cv.get_feature_names())\n",
        "print('Total Vocabulary Size:', len(vocabulary))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Vocabulary Size: 2097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yPJx29QmN1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dde559b3-a356-4a3f-bddc-6efac1bcc8fa"
      },
      "source": [
        "vocabulary"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1st', '2nd', 'ab', ..., 'zip code', 'zip file', 'zone'],\n",
              "      dtype='<U26')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXS0V8IYmb1n",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0f7apLamPgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "432f7ae8-51ea-4d33-cf52-0dde0f506456"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "TOTAL_TOPICS = 20\n",
        "\n",
        "lsi_model = TruncatedSVD(n_components=TOTAL_TOPICS, n_iter=500, random_state=42)\n",
        "document_topics = lsi_model.fit_transform(cv_features)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10 s, sys: 7.57 s, total: 17.6 s\n",
            "Wall time: 9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4-tZppBmeBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e11b340f-a2b9-4794-8603-0ed2921acb1d"
      },
      "source": [
        "document_topics.shape\n",
        "# 2150 document"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2150, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt7cQu2jmgK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74f1f370-ed0d-4844-8974-7ed6e7033859"
      },
      "source": [
        "topic_terms = lsi_model.components_\n",
        "topic_terms.shape\n",
        "# 2097 words"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 2097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3J3oHV6mgSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5a3d378c-530e-4d23-e406-a138334d4ce8"
      },
      "source": [
        "topic_terms"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00080267,  0.00052692,  0.00037655, ...,  0.00028539,\n",
              "         0.00027183,  0.00325737],\n",
              "       [ 0.00885621,  0.00568069,  0.00364484, ...,  0.00214013,\n",
              "         0.00257936,  0.0626348 ],\n",
              "       [-0.00477828, -0.00230495, -0.00306242, ..., -0.00162598,\n",
              "        -0.00236657,  0.1410079 ],\n",
              "       ...,\n",
              "       [-0.03070314, -0.02270857,  0.00195795, ...,  0.00182991,\n",
              "         0.00139959, -0.04528696],\n",
              "       [-0.01256348, -0.00855104, -0.00197959, ..., -0.00108223,\n",
              "         0.00107812,  0.01667599],\n",
              "       [-0.01868419, -0.01859952, -0.00289716, ..., -0.00260013,\n",
              "        -0.0046619 ,  0.0088497 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgbB8qzCmgd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0edd5895-1377-467a-ebb4-de26e51c0439"
      },
      "source": [
        "top_terms = 20\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterm_weights = np.array([topic_terms[row, columns] \n",
        "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    terms, weights = topic_keyterms_weights[n]\n",
        "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
        "                          key=lambda row: -abs(row[1]))\n",
        "    for term, wt in term_weights:\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('university', 0.969), ('state', 0.175), ('college', 0.076), ('california', 0.049), ('university california', 0.041), ('institute', 0.039), ('new', 0.032), ('wa', 0.029), ('technology', 0.028), ('north', 0.027), ('year', 0.02), ('number', 0.02), ('file', 0.02), ('set', 0.02), ('san', 0.019), ('time', 0.018), ('st', 0.018), ('international', 0.018), ('csv', 0.017), ('player', 0.017)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.305), ('wa', 0.276), ('number', 0.21), ('team', 0.195), ('file', 0.189), ('time', 0.186), ('year', 0.174), ('csv', 0.17), ('one', 0.129), ('ha', 0.126), ('name', 0.125), ('information', 0.119), ('date', 0.115), ('http', 0.114), ('goal', 0.111), ('contains', 0.106), ('per', 0.1), ('value', 0.098), ('taken', 0.094)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('university', -0.121)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.587), ('team', 0.345), ('goal', 0.217), ('attempt', 0.187), ('weighted', 0.152), ('taken', 0.148), ('wa', 0.144), ('zone', 0.141), ('allowed', 0.121), ('minute', 0.109), ('game', 0.107), ('average', 0.089), ('per', 0.089)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('file', -0.156), ('csv', -0.136), ('one', -0.094), ('year', -0.093), ('date', -0.09), ('information', -0.081), ('contains', -0.077)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('integer', 0.806), ('interested', 0.353), ('enjoy', 0.347), ('much', 0.164), ('movie', 0.08), ('categorical', 0.068), ('always', 0.065), ('people', 0.062), ('music', 0.06), ('preference', 0.05), ('interest', 0.048), ('lot', 0.044), ('item', 0.041), ('money', 0.032), ('point', 0.032)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.052), ('date', -0.051), ('number', -0.036), ('file', -0.034), ('code', -0.031)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('date', 0.319), ('element', 0.259), ('tag', 0.201), ('file', 0.191), ('registration', 0.187), ('zero', 0.181), ('end', 0.176), ('one', 0.175), ('start', 0.171), ('application', 0.162), ('time', 0.158), ('position', 0.145), ('containing', 0.138), ('section', 0.1), ('mark', 0.096), ('version', 0.093)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.444), ('total', -0.154), ('state', -0.098), ('child', -0.093)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.406), ('number', 0.213), ('date', 0.206), ('element', 0.151), ('total', 0.145), ('registration', 0.112), ('zero', 0.111), ('child', 0.107), ('time', 0.104), ('end', 0.102), ('code', 0.099), ('start', 0.091), ('application', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.535), ('file', -0.146), ('image', -0.1), ('http', -0.097), ('text', -0.095), ('numeric', -0.093), ('user', -0.087)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.54), ('text', 0.364), ('word', 0.12), ('model', 0.092), ('trained', 0.08), ('wa', 0.072), ('language', 0.07), ('use', 0.068), ('image', 0.066), ('http', 0.061), ('using', 0.06), ('set', 0.054), ('reading', 0.05), ('real', 0.05)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.581), ('year', -0.167), ('file', -0.081), ('total', -0.059), ('station', -0.059), ('date', -0.051)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.638), ('csv', 0.396), ('text', 0.309), ('year', 0.243), ('total', 0.08), ('reading', 0.052)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('model', -0.144), ('image', -0.133), ('trained', -0.124), ('http', -0.095), ('feature', -0.087), ('pre', -0.08), ('wa', -0.078), ('pre trained', -0.075), ('trained model', -0.072), ('set', -0.071), ('use', -0.062), ('word', -0.061), ('using', -0.059), ('ha', -0.056)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('model', 0.314), ('trained', 0.301), ('year', 0.244), ('image', 0.216), ('pre trained', 0.187), ('pre', 0.187), ('feature', 0.186), ('trained model', 0.184), ('csv', 0.176), ('network', 0.128), ('layer', 0.107), ('total', 0.104), ('time', 0.103), ('numeric', 0.084)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('name', -0.132), ('information', -0.111), ('user', -0.1), ('com', -0.08), ('fire', -0.077), ('de', -0.076)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('value', 0.345), ('station', 0.257), ('name', 0.244), ('feature', 0.15), ('air', 0.135), ('mean', 0.127), ('day', 0.12), ('monitor', 0.119), ('hour', 0.117), ('event', 0.112), ('parameter', 0.111), ('site', 0.1), ('code', 0.1)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.218), ('movie', -0.154), ('word', -0.152), ('http', -0.143), ('total', -0.138), ('com', -0.109), ('tag', -0.103)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #11:\n",
            "==================================================\n",
            "Direction 1: [('child', 0.518), ('number', 0.278), ('word', 0.23), ('age', 0.227), ('name', 0.173), ('language', 0.149), ('sold', 0.146), ('sale', 0.11), ('month', 0.105), ('woman', 0.098), ('csv', 0.087), ('price', 0.086)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.213), ('de', -0.161), ('http', -0.142), ('com', -0.126), ('value', -0.103), ('en', -0.1), ('per', -0.097), ('www', -0.092)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #12:\n",
            "==================================================\n",
            "Direction 1: [('de', 0.426), ('en', 0.23), ('com', 0.23), ('number', 0.222), ('per', 0.21), ('http', 0.207), ('csv', 0.152), ('le', 0.145), ('la', 0.13), ('www', 0.119), ('http www', 0.117)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('file', -0.27), ('station', -0.173), ('year', -0.166), ('user', -0.163), ('set', -0.115), ('movie', -0.113), ('id', -0.11), ('wa', -0.102), ('value', -0.088)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #13:\n",
            "==================================================\n",
            "Direction 1: [('state', 0.226), ('fire', 0.188), ('department', 0.138), ('csv', 0.137), ('police', 0.133), ('national', 0.114), ('code', 0.109), ('wa', 0.109), ('time', 0.106)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.299), ('file', -0.222), ('value', -0.213), ('number', -0.207), ('station', -0.176), ('per', -0.173), ('en', -0.158), ('movie', -0.135), ('com', -0.129), ('user', -0.121), ('word', -0.107)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #14:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.655), ('label', 0.17), ('class', 0.168), ('number', 0.135), ('sample', 0.08), ('point', 0.077), ('pixel', 0.069)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('movie', -0.238), ('user', -0.21), ('trained', -0.149), ('model', -0.135), ('tag', -0.111), ('rating', -0.104), ('feature', -0.099), ('pre trained', -0.096), ('ha', -0.095), ('trained model', -0.093), ('pre', -0.092), ('state', -0.071), ('station', -0.069)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #15:\n",
            "==================================================\n",
            "Direction 1: [('user', 0.27), ('number', 0.212), ('movie', 0.176), ('game', 0.166), ('player', 0.159), ('id', 0.119), ('rating', 0.105), ('numeric', 0.103), ('set', 0.102)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -0.516), ('language', -0.254), ('wa', -0.144), ('corpus', -0.141), ('csv', -0.121), ('per', -0.113), ('text', -0.112), ('vector', -0.099), ('goal', -0.089), ('taken', -0.088), ('de', -0.086)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #16:\n",
            "==================================================\n",
            "Direction 1: [('file', 0.312), ('image', 0.307), ('child', 0.226), ('name', 0.158), ('movie', 0.151), ('age', 0.135), ('state', 0.1), ('per', 0.097), ('tag', 0.094), ('attempt', 0.09), ('weighted', 0.088), ('health', 0.087)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('time', -0.275), ('word', -0.233), ('race', -0.196), ('number', -0.174), ('game', -0.144), ('match', -0.119), ('language', -0.102), ('csv', -0.1)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #17:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.254), ('number', 0.243), ('code', 0.181), ('fire', 0.148), ('game', 0.14), ('name', 0.126), ('state', 0.115)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('race', -0.259), ('time', -0.225), ('police', -0.21), ('station', -0.193), ('section', -0.182), ('point', -0.152), ('team', -0.139), ('taken', -0.136), ('woman', -0.13), ('per', -0.12), ('age', -0.108), ('child', -0.106), ('total', -0.106)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #18:\n",
            "==================================================\n",
            "Direction 1: [('station', 0.395), ('file', 0.191), ('player', 0.187), ('feature', 0.177), ('weather', 0.141), ('null', 0.136), ('http', 0.134), ('game', 0.124), ('police', 0.101)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.263), ('movie', -0.176), ('taken', -0.156), ('race', -0.154), ('time', -0.15), ('set', -0.148), ('per', -0.129), ('mean', -0.12), ('sample', -0.114), ('hour', -0.105), ('wa', -0.105)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #19:\n",
            "==================================================\n",
            "Direction 1: [('police', 0.296), ('user', 0.248), ('de', 0.23), ('number', 0.219), ('crime', 0.174), ('image', 0.168), ('total', 0.131), ('fire', 0.128), ('tweet', 0.123), ('station', 0.121), ('department', 0.118), ('location', 0.113), ('section', 0.108), ('id', 0.108)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('match', -0.189), ('http', -0.162), ('team', -0.162), ('name', -0.138), ('child', -0.132), ('price', -0.128)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #20:\n",
            "==================================================\n",
            "Direction 1: [('health', 0.272), ('drug', 0.215), ('plan', 0.209), ('survey', 0.206), ('information', 0.123), ('de', 0.121), ('per', 0.119), ('care', 0.113), ('part', 0.108)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('http', -0.197), ('fire', -0.174), ('movie', -0.157), ('point', -0.144), ('team', -0.138), ('image', -0.125), ('com', -0.123), ('race', -0.122), ('name', -0.116), ('child', -0.109), ('www', -0.107)]\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCL0obHUmgad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "fd12b875-1d5c-485a-ad09-8ad1fff3d22c"
      },
      "source": [
        "dt_df = pd.DataFrame(np.round(document_topics, 3), \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.T\n",
        "#2150 document"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2110</th>\n",
              "      <th>2111</th>\n",
              "      <th>2112</th>\n",
              "      <th>2113</th>\n",
              "      <th>2114</th>\n",
              "      <th>2115</th>\n",
              "      <th>2116</th>\n",
              "      <th>2117</th>\n",
              "      <th>2118</th>\n",
              "      <th>2119</th>\n",
              "      <th>2120</th>\n",
              "      <th>2121</th>\n",
              "      <th>2122</th>\n",
              "      <th>2123</th>\n",
              "      <th>2124</th>\n",
              "      <th>2125</th>\n",
              "      <th>2126</th>\n",
              "      <th>2127</th>\n",
              "      <th>2128</th>\n",
              "      <th>2129</th>\n",
              "      <th>2130</th>\n",
              "      <th>2131</th>\n",
              "      <th>2132</th>\n",
              "      <th>2133</th>\n",
              "      <th>2134</th>\n",
              "      <th>2135</th>\n",
              "      <th>2136</th>\n",
              "      <th>2137</th>\n",
              "      <th>2138</th>\n",
              "      <th>2139</th>\n",
              "      <th>2140</th>\n",
              "      <th>2141</th>\n",
              "      <th>2142</th>\n",
              "      <th>2143</th>\n",
              "      <th>2144</th>\n",
              "      <th>2145</th>\n",
              "      <th>2146</th>\n",
              "      <th>2147</th>\n",
              "      <th>2148</th>\n",
              "      <th>2149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1</th>\n",
              "      <td>0.439</td>\n",
              "      <td>1.150</td>\n",
              "      <td>1.030</td>\n",
              "      <td>5.407</td>\n",
              "      <td>0.529</td>\n",
              "      <td>1.605</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.400</td>\n",
              "      <td>1.012</td>\n",
              "      <td>0.928</td>\n",
              "      <td>1.346</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.616</td>\n",
              "      <td>0.399</td>\n",
              "      <td>0.279</td>\n",
              "      <td>1.509</td>\n",
              "      <td>0.987</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.466</td>\n",
              "      <td>0.423</td>\n",
              "      <td>1.048</td>\n",
              "      <td>1.742</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.381</td>\n",
              "      <td>1.913</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.638</td>\n",
              "      <td>11.345</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.264</td>\n",
              "      <td>2.582</td>\n",
              "      <td>0.249</td>\n",
              "      <td>1.898</td>\n",
              "      <td>...</td>\n",
              "      <td>1.148</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.345</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>3.561</td>\n",
              "      <td>11.376</td>\n",
              "      <td>7.611</td>\n",
              "      <td>11.192</td>\n",
              "      <td>4.449</td>\n",
              "      <td>10.668</td>\n",
              "      <td>1.292</td>\n",
              "      <td>1.104</td>\n",
              "      <td>4.714</td>\n",
              "      <td>4.545</td>\n",
              "      <td>1.973</td>\n",
              "      <td>5.762</td>\n",
              "      <td>5.910</td>\n",
              "      <td>8.704</td>\n",
              "      <td>2.449</td>\n",
              "      <td>3.017</td>\n",
              "      <td>3.270</td>\n",
              "      <td>2.161</td>\n",
              "      <td>8.132</td>\n",
              "      <td>7.626</td>\n",
              "      <td>1.769</td>\n",
              "      <td>3.959</td>\n",
              "      <td>3.672</td>\n",
              "      <td>11.006</td>\n",
              "      <td>12.280</td>\n",
              "      <td>0.496</td>\n",
              "      <td>8.143</td>\n",
              "      <td>3.122</td>\n",
              "      <td>17.566</td>\n",
              "      <td>5.982</td>\n",
              "      <td>7.000</td>\n",
              "      <td>5.311</td>\n",
              "      <td>2.433</td>\n",
              "      <td>3.395</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.674</td>\n",
              "      <td>2.162</td>\n",
              "      <td>4.401</td>\n",
              "      <td>2.170</td>\n",
              "      <td>5.936</td>\n",
              "      <td>...</td>\n",
              "      <td>2.439</td>\n",
              "      <td>0.816</td>\n",
              "      <td>1.022</td>\n",
              "      <td>1.204</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.930</td>\n",
              "      <td>2.082</td>\n",
              "      <td>3.908</td>\n",
              "      <td>4.460</td>\n",
              "      <td>2.820</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1.201</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1.524</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1.625</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>4.460</td>\n",
              "      <td>1.625</td>\n",
              "      <td>0.939</td>\n",
              "      <td>1.449</td>\n",
              "      <td>1.825</td>\n",
              "      <td>0.323</td>\n",
              "      <td>2.731</td>\n",
              "      <td>2.829</td>\n",
              "      <td>1.221</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>-2.190</td>\n",
              "      <td>5.920</td>\n",
              "      <td>-3.482</td>\n",
              "      <td>-5.392</td>\n",
              "      <td>-1.810</td>\n",
              "      <td>-4.841</td>\n",
              "      <td>-0.462</td>\n",
              "      <td>-0.678</td>\n",
              "      <td>-2.670</td>\n",
              "      <td>-1.829</td>\n",
              "      <td>-1.329</td>\n",
              "      <td>-2.146</td>\n",
              "      <td>-1.760</td>\n",
              "      <td>-8.425</td>\n",
              "      <td>-0.747</td>\n",
              "      <td>-0.887</td>\n",
              "      <td>-1.791</td>\n",
              "      <td>-1.486</td>\n",
              "      <td>-2.670</td>\n",
              "      <td>-3.975</td>\n",
              "      <td>-1.065</td>\n",
              "      <td>-1.029</td>\n",
              "      <td>-1.025</td>\n",
              "      <td>5.676</td>\n",
              "      <td>-4.683</td>\n",
              "      <td>-0.306</td>\n",
              "      <td>-5.266</td>\n",
              "      <td>-2.017</td>\n",
              "      <td>-6.781</td>\n",
              "      <td>-2.557</td>\n",
              "      <td>-4.512</td>\n",
              "      <td>-1.904</td>\n",
              "      <td>-0.969</td>\n",
              "      <td>-2.193</td>\n",
              "      <td>-0.813</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>-0.729</td>\n",
              "      <td>-2.218</td>\n",
              "      <td>-1.300</td>\n",
              "      <td>-3.851</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.676</td>\n",
              "      <td>-0.490</td>\n",
              "      <td>-0.643</td>\n",
              "      <td>-0.817</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.505</td>\n",
              "      <td>-1.589</td>\n",
              "      <td>-2.013</td>\n",
              "      <td>-1.890</td>\n",
              "      <td>-0.958</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.436</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.183</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.604</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.902</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.851</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-1.890</td>\n",
              "      <td>-0.880</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-0.740</td>\n",
              "      <td>-1.182</td>\n",
              "      <td>-0.234</td>\n",
              "      <td>-0.334</td>\n",
              "      <td>-1.312</td>\n",
              "      <td>-0.944</td>\n",
              "      <td>-0.465</td>\n",
              "      <td>-1.032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>-0.422</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.289</td>\n",
              "      <td>-1.468</td>\n",
              "      <td>-0.393</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>-0.542</td>\n",
              "      <td>-0.077</td>\n",
              "      <td>-0.352</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.724</td>\n",
              "      <td>-2.483</td>\n",
              "      <td>0.124</td>\n",
              "      <td>-0.335</td>\n",
              "      <td>-0.394</td>\n",
              "      <td>-0.361</td>\n",
              "      <td>0.380</td>\n",
              "      <td>1.019</td>\n",
              "      <td>-0.212</td>\n",
              "      <td>-0.431</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.806</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>-0.086</td>\n",
              "      <td>0.627</td>\n",
              "      <td>-0.407</td>\n",
              "      <td>-1.552</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>-0.090</td>\n",
              "      <td>-0.262</td>\n",
              "      <td>0.162</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>0.063</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>-0.117</td>\n",
              "      <td>-0.186</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.344</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>0.099</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.161</td>\n",
              "      <td>-0.096</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.130</td>\n",
              "      <td>-0.054</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.241</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>-0.174</td>\n",
              "      <td>-0.332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T5</th>\n",
              "      <td>-0.224</td>\n",
              "      <td>-0.217</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.908</td>\n",
              "      <td>0.146</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>-0.054</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>0.607</td>\n",
              "      <td>-1.114</td>\n",
              "      <td>0.960</td>\n",
              "      <td>-1.768</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>7.174</td>\n",
              "      <td>-2.690</td>\n",
              "      <td>-0.632</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.726</td>\n",
              "      <td>-3.546</td>\n",
              "      <td>-1.033</td>\n",
              "      <td>0.109</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>1.033</td>\n",
              "      <td>0.114</td>\n",
              "      <td>-5.975</td>\n",
              "      <td>-0.239</td>\n",
              "      <td>1.954</td>\n",
              "      <td>0.815</td>\n",
              "      <td>-2.189</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.186</td>\n",
              "      <td>-0.855</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>-1.846</td>\n",
              "      <td>0.195</td>\n",
              "      <td>-0.634</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-1.931</td>\n",
              "      <td>1.375</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.828</td>\n",
              "      <td>-0.644</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>0.173</td>\n",
              "      <td>-1.063</td>\n",
              "      <td>-0.959</td>\n",
              "      <td>-0.201</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.222</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>0.493</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-1.104</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.959</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>-0.253</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.132</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>-0.799</td>\n",
              "      <td>0.480</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T6</th>\n",
              "      <td>-0.853</td>\n",
              "      <td>-2.495</td>\n",
              "      <td>-1.340</td>\n",
              "      <td>-0.549</td>\n",
              "      <td>-2.863</td>\n",
              "      <td>-7.662</td>\n",
              "      <td>-0.389</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>-2.387</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>-0.269</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>-2.359</td>\n",
              "      <td>-16.858</td>\n",
              "      <td>1.752</td>\n",
              "      <td>0.096</td>\n",
              "      <td>-0.251</td>\n",
              "      <td>-1.317</td>\n",
              "      <td>0.540</td>\n",
              "      <td>-3.537</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>-2.767</td>\n",
              "      <td>0.974</td>\n",
              "      <td>-2.679</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-7.250</td>\n",
              "      <td>-1.684</td>\n",
              "      <td>6.712</td>\n",
              "      <td>-0.406</td>\n",
              "      <td>-7.606</td>\n",
              "      <td>-1.198</td>\n",
              "      <td>-0.866</td>\n",
              "      <td>-0.310</td>\n",
              "      <td>-0.399</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>-1.614</td>\n",
              "      <td>1.019</td>\n",
              "      <td>-2.381</td>\n",
              "      <td>...</td>\n",
              "      <td>0.728</td>\n",
              "      <td>0.146</td>\n",
              "      <td>-0.675</td>\n",
              "      <td>-0.852</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.344</td>\n",
              "      <td>-1.823</td>\n",
              "      <td>-2.471</td>\n",
              "      <td>-3.111</td>\n",
              "      <td>-0.057</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.655</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-3.111</td>\n",
              "      <td>-0.158</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.972</td>\n",
              "      <td>-1.186</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>-0.488</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.524</td>\n",
              "      <td>0.859</td>\n",
              "      <td>1.609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T7</th>\n",
              "      <td>1.287</td>\n",
              "      <td>1.772</td>\n",
              "      <td>1.604</td>\n",
              "      <td>3.006</td>\n",
              "      <td>-2.224</td>\n",
              "      <td>-5.621</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.262</td>\n",
              "      <td>-1.642</td>\n",
              "      <td>0.829</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>2.057</td>\n",
              "      <td>-2.406</td>\n",
              "      <td>76.337</td>\n",
              "      <td>-0.318</td>\n",
              "      <td>1.063</td>\n",
              "      <td>1.053</td>\n",
              "      <td>0.241</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.279</td>\n",
              "      <td>0.266</td>\n",
              "      <td>-0.658</td>\n",
              "      <td>0.505</td>\n",
              "      <td>1.254</td>\n",
              "      <td>1.660</td>\n",
              "      <td>0.173</td>\n",
              "      <td>-3.794</td>\n",
              "      <td>-2.289</td>\n",
              "      <td>-4.409</td>\n",
              "      <td>0.091</td>\n",
              "      <td>-7.400</td>\n",
              "      <td>2.531</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.694</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.446</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.039</td>\n",
              "      <td>9.500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.520</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.025</td>\n",
              "      <td>4.376</td>\n",
              "      <td>4.571</td>\n",
              "      <td>5.127</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.149</td>\n",
              "      <td>0.008</td>\n",
              "      <td>1.341</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>5.127</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.936</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.454</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.186</td>\n",
              "      <td>-0.330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T8</th>\n",
              "      <td>-2.009</td>\n",
              "      <td>-2.316</td>\n",
              "      <td>-2.412</td>\n",
              "      <td>-3.280</td>\n",
              "      <td>1.151</td>\n",
              "      <td>2.929</td>\n",
              "      <td>-0.605</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.741</td>\n",
              "      <td>-1.258</td>\n",
              "      <td>-0.161</td>\n",
              "      <td>-1.776</td>\n",
              "      <td>1.154</td>\n",
              "      <td>84.006</td>\n",
              "      <td>0.920</td>\n",
              "      <td>-1.174</td>\n",
              "      <td>-1.564</td>\n",
              "      <td>0.466</td>\n",
              "      <td>-1.334</td>\n",
              "      <td>-2.811</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.606</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>-2.137</td>\n",
              "      <td>-0.241</td>\n",
              "      <td>0.840</td>\n",
              "      <td>1.290</td>\n",
              "      <td>2.623</td>\n",
              "      <td>-0.731</td>\n",
              "      <td>4.561</td>\n",
              "      <td>-2.191</td>\n",
              "      <td>-1.114</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>0.236</td>\n",
              "      <td>-0.542</td>\n",
              "      <td>-0.220</td>\n",
              "      <td>-1.915</td>\n",
              "      <td>-0.037</td>\n",
              "      <td>11.925</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.341</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>-0.185</td>\n",
              "      <td>-0.777</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>1.936</td>\n",
              "      <td>-7.141</td>\n",
              "      <td>-7.947</td>\n",
              "      <td>-0.825</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.140</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.068</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.114</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-7.947</td>\n",
              "      <td>-0.273</td>\n",
              "      <td>-0.368</td>\n",
              "      <td>-0.818</td>\n",
              "      <td>-1.628</td>\n",
              "      <td>-0.075</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.823</td>\n",
              "      <td>-0.389</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T9</th>\n",
              "      <td>1.417</td>\n",
              "      <td>-1.116</td>\n",
              "      <td>-1.347</td>\n",
              "      <td>-5.409</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>-0.312</td>\n",
              "      <td>0.940</td>\n",
              "      <td>-0.559</td>\n",
              "      <td>-0.247</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.489</td>\n",
              "      <td>7.477</td>\n",
              "      <td>0.693</td>\n",
              "      <td>1.107</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.957</td>\n",
              "      <td>-1.406</td>\n",
              "      <td>1.611</td>\n",
              "      <td>-0.613</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>-0.338</td>\n",
              "      <td>0.860</td>\n",
              "      <td>-5.232</td>\n",
              "      <td>0.112</td>\n",
              "      <td>-1.259</td>\n",
              "      <td>0.869</td>\n",
              "      <td>2.224</td>\n",
              "      <td>-1.622</td>\n",
              "      <td>1.892</td>\n",
              "      <td>0.323</td>\n",
              "      <td>-0.590</td>\n",
              "      <td>-1.471</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-0.499</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>0.203</td>\n",
              "      <td>-0.398</td>\n",
              "      <td>1.109</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>-0.333</td>\n",
              "      <td>0.718</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.334</td>\n",
              "      <td>-0.356</td>\n",
              "      <td>14.440</td>\n",
              "      <td>15.671</td>\n",
              "      <td>-1.782</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.161</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.120</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.451</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>15.671</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>-0.109</td>\n",
              "      <td>0.290</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-0.085</td>\n",
              "      <td>-0.365</td>\n",
              "      <td>-1.428</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T10</th>\n",
              "      <td>0.952</td>\n",
              "      <td>-1.901</td>\n",
              "      <td>-1.649</td>\n",
              "      <td>-2.335</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.431</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.683</td>\n",
              "      <td>-0.087</td>\n",
              "      <td>1.213</td>\n",
              "      <td>1.587</td>\n",
              "      <td>6.266</td>\n",
              "      <td>-0.662</td>\n",
              "      <td>0.391</td>\n",
              "      <td>2.788</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-0.582</td>\n",
              "      <td>-0.081</td>\n",
              "      <td>-1.573</td>\n",
              "      <td>0.597</td>\n",
              "      <td>-0.349</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.164</td>\n",
              "      <td>-5.235</td>\n",
              "      <td>0.154</td>\n",
              "      <td>5.917</td>\n",
              "      <td>-0.232</td>\n",
              "      <td>-0.835</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>-0.564</td>\n",
              "      <td>-0.080</td>\n",
              "      <td>-0.073</td>\n",
              "      <td>-0.112</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.626</td>\n",
              "      <td>0.343</td>\n",
              "      <td>2.485</td>\n",
              "      <td>...</td>\n",
              "      <td>0.484</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.368</td>\n",
              "      <td>-0.232</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>-1.402</td>\n",
              "      <td>3.245</td>\n",
              "      <td>3.073</td>\n",
              "      <td>-0.696</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.507</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>3.073</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-0.171</td>\n",
              "      <td>-0.249</td>\n",
              "      <td>-0.163</td>\n",
              "      <td>-0.314</td>\n",
              "      <td>1.036</td>\n",
              "      <td>-0.293</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-0.348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T11</th>\n",
              "      <td>-0.392</td>\n",
              "      <td>-1.278</td>\n",
              "      <td>-0.776</td>\n",
              "      <td>-2.673</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.862</td>\n",
              "      <td>0.153</td>\n",
              "      <td>-0.127</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.072</td>\n",
              "      <td>1.070</td>\n",
              "      <td>-1.175</td>\n",
              "      <td>-2.936</td>\n",
              "      <td>-0.967</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>-0.360</td>\n",
              "      <td>-0.079</td>\n",
              "      <td>-1.437</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.220</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.119</td>\n",
              "      <td>-1.554</td>\n",
              "      <td>0.363</td>\n",
              "      <td>-1.373</td>\n",
              "      <td>0.467</td>\n",
              "      <td>5.140</td>\n",
              "      <td>2.375</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.168</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.227</td>\n",
              "      <td>-0.061</td>\n",
              "      <td>1.225</td>\n",
              "      <td>0.287</td>\n",
              "      <td>1.115</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-0.037</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.233</td>\n",
              "      <td>1.144</td>\n",
              "      <td>-0.123</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-1.993</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.448</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.048</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.151</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.487</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.537</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.305</td>\n",
              "      <td>0.943</td>\n",
              "      <td>0.670</td>\n",
              "      <td>-0.429</td>\n",
              "      <td>0.688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T12</th>\n",
              "      <td>0.524</td>\n",
              "      <td>1.756</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-1.287</td>\n",
              "      <td>-0.438</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>-1.194</td>\n",
              "      <td>0.556</td>\n",
              "      <td>-0.860</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>4.509</td>\n",
              "      <td>-0.357</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>-0.209</td>\n",
              "      <td>-0.941</td>\n",
              "      <td>0.066</td>\n",
              "      <td>-1.567</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>2.016</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>1.165</td>\n",
              "      <td>4.701</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-4.982</td>\n",
              "      <td>0.574</td>\n",
              "      <td>4.880</td>\n",
              "      <td>1.271</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>-0.554</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.364</td>\n",
              "      <td>1.022</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>-1.559</td>\n",
              "      <td>0.748</td>\n",
              "      <td>3.258</td>\n",
              "      <td>...</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.474</td>\n",
              "      <td>-0.547</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.173</td>\n",
              "      <td>-1.255</td>\n",
              "      <td>3.675</td>\n",
              "      <td>3.315</td>\n",
              "      <td>4.197</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.028</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.848</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.113</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.445</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>3.315</td>\n",
              "      <td>-0.082</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>-0.234</td>\n",
              "      <td>-0.771</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>0.701</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-1.264</td>\n",
              "      <td>-0.878</td>\n",
              "      <td>0.185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T13</th>\n",
              "      <td>-0.735</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.076</td>\n",
              "      <td>5.924</td>\n",
              "      <td>1.529</td>\n",
              "      <td>3.140</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.537</td>\n",
              "      <td>-0.494</td>\n",
              "      <td>-0.160</td>\n",
              "      <td>-0.580</td>\n",
              "      <td>0.753</td>\n",
              "      <td>2.453</td>\n",
              "      <td>1.021</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.898</td>\n",
              "      <td>-1.458</td>\n",
              "      <td>-0.353</td>\n",
              "      <td>3.553</td>\n",
              "      <td>-1.504</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.171</td>\n",
              "      <td>0.091</td>\n",
              "      <td>-2.841</td>\n",
              "      <td>0.328</td>\n",
              "      <td>-7.504</td>\n",
              "      <td>-1.600</td>\n",
              "      <td>1.397</td>\n",
              "      <td>1.317</td>\n",
              "      <td>1.595</td>\n",
              "      <td>4.664</td>\n",
              "      <td>0.292</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>0.555</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>0.089</td>\n",
              "      <td>2.305</td>\n",
              "      <td>...</td>\n",
              "      <td>1.930</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.168</td>\n",
              "      <td>-0.058</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.133</td>\n",
              "      <td>-0.887</td>\n",
              "      <td>0.933</td>\n",
              "      <td>1.699</td>\n",
              "      <td>-3.060</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.626</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>1.699</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.244</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>-0.295</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>-0.489</td>\n",
              "      <td>-1.339</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>-0.336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T14</th>\n",
              "      <td>0.300</td>\n",
              "      <td>2.056</td>\n",
              "      <td>-1.496</td>\n",
              "      <td>-2.846</td>\n",
              "      <td>-0.185</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.621</td>\n",
              "      <td>1.636</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-0.426</td>\n",
              "      <td>-2.121</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.353</td>\n",
              "      <td>3.084</td>\n",
              "      <td>-1.213</td>\n",
              "      <td>0.354</td>\n",
              "      <td>8.538</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.272</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-6.633</td>\n",
              "      <td>0.084</td>\n",
              "      <td>2.933</td>\n",
              "      <td>0.086</td>\n",
              "      <td>-0.703</td>\n",
              "      <td>0.258</td>\n",
              "      <td>-0.044</td>\n",
              "      <td>-1.231</td>\n",
              "      <td>0.122</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.070</td>\n",
              "      <td>1.536</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.812</td>\n",
              "      <td>0.231</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>1.163</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.307</td>\n",
              "      <td>-0.819</td>\n",
              "      <td>-6.072</td>\n",
              "      <td>-4.852</td>\n",
              "      <td>-0.765</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.064</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.503</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.139</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.305</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-4.852</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>2.154</td>\n",
              "      <td>-0.052</td>\n",
              "      <td>0.224</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>-0.045</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T15</th>\n",
              "      <td>0.576</td>\n",
              "      <td>4.261</td>\n",
              "      <td>1.795</td>\n",
              "      <td>2.794</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.102</td>\n",
              "      <td>2.342</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.153</td>\n",
              "      <td>-1.389</td>\n",
              "      <td>4.350</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-0.051</td>\n",
              "      <td>0.698</td>\n",
              "      <td>1.263</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.332</td>\n",
              "      <td>-0.537</td>\n",
              "      <td>2.270</td>\n",
              "      <td>3.017</td>\n",
              "      <td>0.139</td>\n",
              "      <td>3.792</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>2.865</td>\n",
              "      <td>0.738</td>\n",
              "      <td>-1.763</td>\n",
              "      <td>-0.117</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.364</td>\n",
              "      <td>-0.029</td>\n",
              "      <td>0.053</td>\n",
              "      <td>-0.454</td>\n",
              "      <td>-1.879</td>\n",
              "      <td>0.107</td>\n",
              "      <td>3.954</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.780</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.140</td>\n",
              "      <td>-3.321</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.625</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.072</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.182</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.893</td>\n",
              "      <td>0.196</td>\n",
              "      <td>-2.388</td>\n",
              "      <td>0.307</td>\n",
              "      <td>1.191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T16</th>\n",
              "      <td>-0.322</td>\n",
              "      <td>-4.400</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.857</td>\n",
              "      <td>-1.370</td>\n",
              "      <td>-0.713</td>\n",
              "      <td>-0.192</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.939</td>\n",
              "      <td>-1.061</td>\n",
              "      <td>1.355</td>\n",
              "      <td>-0.927</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>3.987</td>\n",
              "      <td>-0.376</td>\n",
              "      <td>-0.529</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.126</td>\n",
              "      <td>-2.667</td>\n",
              "      <td>2.834</td>\n",
              "      <td>0.278</td>\n",
              "      <td>-1.087</td>\n",
              "      <td>-0.575</td>\n",
              "      <td>-3.304</td>\n",
              "      <td>1.002</td>\n",
              "      <td>0.164</td>\n",
              "      <td>4.279</td>\n",
              "      <td>-0.535</td>\n",
              "      <td>-3.866</td>\n",
              "      <td>-1.789</td>\n",
              "      <td>2.146</td>\n",
              "      <td>-0.979</td>\n",
              "      <td>-1.316</td>\n",
              "      <td>1.467</td>\n",
              "      <td>-0.304</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.100</td>\n",
              "      <td>-1.206</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>...</td>\n",
              "      <td>1.276</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.916</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.290</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>0.766</td>\n",
              "      <td>1.576</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.142</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.272</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-1.265</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>1.576</td>\n",
              "      <td>0.118</td>\n",
              "      <td>-0.240</td>\n",
              "      <td>0.064</td>\n",
              "      <td>1.594</td>\n",
              "      <td>0.340</td>\n",
              "      <td>-0.982</td>\n",
              "      <td>1.383</td>\n",
              "      <td>-0.350</td>\n",
              "      <td>-0.856</td>\n",
              "      <td>-0.436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T17</th>\n",
              "      <td>-1.523</td>\n",
              "      <td>0.545</td>\n",
              "      <td>-0.639</td>\n",
              "      <td>0.343</td>\n",
              "      <td>-1.400</td>\n",
              "      <td>-2.430</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>-0.176</td>\n",
              "      <td>-0.586</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.248</td>\n",
              "      <td>1.206</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.861</td>\n",
              "      <td>0.533</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>0.388</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.184</td>\n",
              "      <td>-0.367</td>\n",
              "      <td>-0.111</td>\n",
              "      <td>0.670</td>\n",
              "      <td>-1.032</td>\n",
              "      <td>2.657</td>\n",
              "      <td>-3.749</td>\n",
              "      <td>-0.071</td>\n",
              "      <td>1.256</td>\n",
              "      <td>-0.665</td>\n",
              "      <td>-5.139</td>\n",
              "      <td>1.949</td>\n",
              "      <td>0.636</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>-1.082</td>\n",
              "      <td>0.692</td>\n",
              "      <td>-0.437</td>\n",
              "      <td>-0.674</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-0.737</td>\n",
              "      <td>0.480</td>\n",
              "      <td>-2.303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.231</td>\n",
              "      <td>0.911</td>\n",
              "      <td>1.353</td>\n",
              "      <td>1.789</td>\n",
              "      <td>-1.583</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.222</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.949</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>1.789</td>\n",
              "      <td>-0.158</td>\n",
              "      <td>-0.362</td>\n",
              "      <td>-0.344</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>-0.055</td>\n",
              "      <td>0.641</td>\n",
              "      <td>0.368</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T18</th>\n",
              "      <td>1.402</td>\n",
              "      <td>4.280</td>\n",
              "      <td>-1.806</td>\n",
              "      <td>-2.204</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>-1.791</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>0.339</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>1.135</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>-0.913</td>\n",
              "      <td>-0.334</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-0.092</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>-1.889</td>\n",
              "      <td>-1.834</td>\n",
              "      <td>0.504</td>\n",
              "      <td>1.331</td>\n",
              "      <td>-0.072</td>\n",
              "      <td>1.496</td>\n",
              "      <td>1.670</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-3.837</td>\n",
              "      <td>-0.560</td>\n",
              "      <td>-2.452</td>\n",
              "      <td>-0.088</td>\n",
              "      <td>1.217</td>\n",
              "      <td>-0.742</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>-0.418</td>\n",
              "      <td>-0.123</td>\n",
              "      <td>0.239</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-1.915</td>\n",
              "      <td>0.279</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.467</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>0.490</td>\n",
              "      <td>-0.052</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-1.980</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.142</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.623</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.384</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.683</td>\n",
              "      <td>-0.386</td>\n",
              "      <td>0.418</td>\n",
              "      <td>0.634</td>\n",
              "      <td>0.288</td>\n",
              "      <td>0.616</td>\n",
              "      <td>0.289</td>\n",
              "      <td>-0.387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T19</th>\n",
              "      <td>-0.638</td>\n",
              "      <td>-5.427</td>\n",
              "      <td>-0.719</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.650</td>\n",
              "      <td>-1.104</td>\n",
              "      <td>-0.195</td>\n",
              "      <td>-0.807</td>\n",
              "      <td>-0.680</td>\n",
              "      <td>-0.422</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>-0.355</td>\n",
              "      <td>1.386</td>\n",
              "      <td>0.316</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-3.481</td>\n",
              "      <td>0.988</td>\n",
              "      <td>-0.314</td>\n",
              "      <td>-1.160</td>\n",
              "      <td>-1.242</td>\n",
              "      <td>-2.326</td>\n",
              "      <td>-8.990</td>\n",
              "      <td>-0.391</td>\n",
              "      <td>0.967</td>\n",
              "      <td>-1.288</td>\n",
              "      <td>0.931</td>\n",
              "      <td>0.916</td>\n",
              "      <td>-0.496</td>\n",
              "      <td>0.299</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>-1.626</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.484</td>\n",
              "      <td>0.475</td>\n",
              "      <td>-1.616</td>\n",
              "      <td>-1.058</td>\n",
              "      <td>-1.589</td>\n",
              "      <td>...</td>\n",
              "      <td>0.947</td>\n",
              "      <td>-0.151</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>0.146</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.066</td>\n",
              "      <td>0.786</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>3.189</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.088</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.135</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.470</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.572</td>\n",
              "      <td>-0.111</td>\n",
              "      <td>-0.519</td>\n",
              "      <td>0.632</td>\n",
              "      <td>-0.170</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>-1.696</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T20</th>\n",
              "      <td>0.888</td>\n",
              "      <td>-1.644</td>\n",
              "      <td>0.085</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>3.320</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.392</td>\n",
              "      <td>0.103</td>\n",
              "      <td>-0.166</td>\n",
              "      <td>0.126</td>\n",
              "      <td>-3.480</td>\n",
              "      <td>1.518</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.523</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>4.024</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>-0.262</td>\n",
              "      <td>-0.984</td>\n",
              "      <td>0.791</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>-3.338</td>\n",
              "      <td>-0.160</td>\n",
              "      <td>-1.934</td>\n",
              "      <td>-0.141</td>\n",
              "      <td>3.136</td>\n",
              "      <td>1.128</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.400</td>\n",
              "      <td>2.003</td>\n",
              "      <td>8.028</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.604</td>\n",
              "      <td>0.313</td>\n",
              "      <td>6.076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.350</td>\n",
              "      <td>-0.660</td>\n",
              "      <td>-0.215</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>1.476</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>-0.157</td>\n",
              "      <td>0.072</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.611</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>-0.358</td>\n",
              "      <td>-0.225</td>\n",
              "      <td>-0.035</td>\n",
              "      <td>-0.722</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>0.495</td>\n",
              "      <td>1.023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 2150 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0       1      2       3      4     ...   2145   2146   2147   2148   2149\n",
              "T1   0.439   1.150  1.030   5.407  0.529  ...  0.300  0.345  0.137  0.231  0.313\n",
              "T2   3.561  11.376  7.611  11.192  4.449  ...  2.731  2.829  1.221  1.950  2.301\n",
              "T3  -2.190   5.920 -3.482  -5.392 -1.810  ... -0.334 -1.312 -0.944 -0.465 -1.032\n",
              "T4  -0.422   0.607  0.289  -1.468 -0.393  ... -0.320 -0.241 -0.178 -0.174 -0.332\n",
              "T5  -0.224  -0.217  0.305   0.908  0.146  ... -0.100 -0.799  0.480 -0.933 -1.552\n",
              "T6  -0.853  -2.495 -1.340  -0.549 -2.863  ... -0.488 -0.482 -0.524  0.859  1.609\n",
              "T7   1.287   1.772  1.604   3.006 -2.224  ...  0.405  0.454  0.418 -0.186 -0.330\n",
              "T8  -2.009  -2.316 -2.412  -3.280  1.151  ... -0.757 -0.823 -0.389  0.250  0.357\n",
              "T9   1.417  -1.116 -1.347  -5.409  0.423  ... -0.365 -1.428 -0.299  0.310  0.065\n",
              "T10  0.952  -1.901 -1.649  -2.335  0.378  ... -0.314  1.036 -0.293  0.392 -0.348\n",
              "T11 -0.392  -1.278 -0.776  -2.673  0.592  ... -0.305  0.943  0.670 -0.429  0.688\n",
              "T12  0.524   1.756 -2.287  -0.008  0.467  ...  0.701  0.467 -1.264 -0.878  0.185\n",
              "T13 -0.735  -0.162  0.076   5.924  1.529  ... -0.479 -0.489 -1.339 -0.374 -0.336\n",
              "T14  0.300   2.056 -1.496  -2.846 -0.185  ...  0.224 -0.268 -0.045  0.093  0.238\n",
              "T15  0.576   4.261  1.795   2.794 -0.280  ...  0.893  0.196 -2.388  0.307  1.191\n",
              "T16 -0.322  -4.400 -0.283  -0.857 -1.370  ... -0.982  1.383 -0.350 -0.856 -0.436\n",
              "T17 -1.523   0.545 -0.639   0.343 -1.400  ... -0.055  0.641  0.368 -0.464  0.581\n",
              "T18  1.402   4.280 -1.806  -2.204 -0.955  ...  0.634  0.288  0.616  0.289 -0.387\n",
              "T19 -0.638  -5.427 -0.719   1.938 -0.650  ... -0.134 -1.696  0.295  0.403  0.623\n",
              "T20  0.888  -1.644  0.085  -0.307 -0.074  ... -0.035 -0.722 -0.191  0.495  1.023\n",
              "\n",
              "[20 rows x 2150 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlsNkErbmgYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "d5e61d6b-228b-4037-8818-66437fb8c037"
      },
      "source": [
        "document_numbers = [1, 4, 10]\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(dt_df.columns[np.argsort(-np.absolute(dt_df.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print('Topic model '+top_topics[0][1:]+':',lsi_bow.show_topic(int(top_topics[0][1:]), topn=20))\n",
        "    print('Topic model '+top_topics[1][1:]+':',lsi_bow.show_topic(int(top_topics[1][1:]), topn=20))\n",
        "    print()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document #1:\n",
            "Dominant Topics (top 3): ['T2', 'T3', 'T19']\n",
            "Paper Summary:\n",
            "The ultimate Soccer database for data analysis and machine learning\n",
            "What you get:\n",
            "+25,000 matches\n",
            "+10,000 players\n",
            "11 European Countries with their lead championship\n",
            "Seasons 2008 to 2016\n",
            "Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates\n",
            "Team line up with squad formation (X, Y coordinates)\n",
            "Betting odds from up to 10 providers\n",
            "Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
            "*16th Oct 201\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "Topic model 3: [('year', 0.6066476425324451), ('date', -0.2521122438137626), ('file', -0.2301105179088531), ('total', 0.18790975575918087), ('csv', -0.15496077804136865), ('one', -0.15379966195785308), ('registration', -0.14983257418264562), ('zero', -0.14495064736824229), ('start', -0.1417260644472022), ('application', -0.13396177541110163), ('time', -0.13204114910014622), ('given', 0.12287379909988017), ('position', -0.11903978091156583), ('numeric', -0.11866454317357562), ('containing', -0.11546119074012208), ('energy', 0.10716218091695022), ('text', -0.10357211852273053), ('state', 0.0950605060488379), ('element', -0.09412418384076568), ('child', 0.08843306485651935)]\n",
            "\n",
            "Document #4:\n",
            "Dominant Topics (top 3): ['T2', 'T6', 'T7']\n",
            "Paper Summary:\n",
            "Context\n",
            "Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus \"chained\" together, \n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "Topic model 6: [('numeric', 0.7737103058328608), ('text', 0.4993476682927095), ('csv', -0.27244942185952215), ('integer', -0.11130023051204573), ('word', 0.06894308219253854), ('date', -0.06198646452424762), ('file', -0.05364781371846869), ('open', 0.051067712551731204), ('time', -0.04455174645642765), ('food', 0.042626416640394786), ('movie', -0.042405335415046826), ('language', 0.03900505452131897), ('product', 0.03899985454687244), ('student', 0.036781790937351205), ('number', -0.03647304030058715), ('one', -0.035450279410746126), ('registration', -0.035042266957650656), ('database', 0.03453239442243826), ('zero', -0.0341555466525183), ('use', 0.03384596864927049)]\n",
            "\n",
            "Document #10:\n",
            "Dominant Topics (top 3): ['T2', 'T16', 'T3']\n",
            "Paper Summary:\n",
            "These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of a\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "Topic model 16: [('race', -0.22872344366458364), ('fire', -0.2231388391823038), ('user', -0.2222971221159055), ('station', 0.1980886197468324), ('word', -0.19015901320077047), ('point', -0.17797854363230017), ('image', -0.17617319805694523), ('name', -0.15545399504070395), ('child', -0.1534954586092869), ('feature', 0.15259011089725077), ('game', 0.1520167765260759), ('taken', -0.13944439777972517), ('time', -0.12973383610412498), ('team', -0.125417406509159), ('unit', -0.1142553246125783), ('match', 0.10845715681357088), ('language', -0.10729404559005745), ('file', -0.10607955244760199), ('state', -0.1056088880063532), ('player', 0.10239704493571669)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jLQoGXWozo_",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nACQzCYmgXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "892eb9d4-ed14-43d9-829d-85070e995a64"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50,\n",
        "                                      learning_method='online', batch_size=1740, learning_offset=50., \n",
        "                                      random_state=42, n_jobs=16)\n",
        "document_topics = lda_model.fit_transform(cv_features)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 8.83 s, total: 1min 10s\n",
            "Wall time: 7min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsZwaPMImgQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "947eda87-715e-4ed5-8580-13e6cf0293a0"
      },
      "source": [
        "topic_terms = lda_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms] #20\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>image, file, one, time, wa, contains, acknowledgement, inspiration, day, like, using, would, feature, used, set, see, available, column, label, different</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>image, wa, time, text, survey, csv, acknowledgement, using, file, statistic, open, ha, numeric, set, value, price, http, learning, database, paper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>player, csv, team, game, file, wa, http, user, movie, time, match, com, number, ha, www, information, http www, set, id, rating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>context data, love, claim, basic, stats, inspiration, filed, al, east, acknowledgement, modification, john, lost, player, sometimes, file, contains, number, ha, year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>wa, state, year, information, acknowledgement, time, number, country, ha, available, inspiration, national, source, public, date, record, report, survey, includes, world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>text, word, language, file, corpus, id, http, use, wa, title, contains, article, license, question, english, page, name, org, author, collection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>set, data set, value, column, activity, wa, mean, energy, model, sample, feature, using, use, used, research, sensor, attribute, system, temperature, specie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>com, http, tweet, github, github com, twitter, http github, vector, http www, www, word, nltk, model, kaggle, acknowledgement, found, zip, code, tree, used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>year, age, type, month, animal, rate, sex, male, female, outcome, bank, birth, name, credit, number, payment, set, amount, usd, data set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>model, trained, de, pre, pre trained, trained model, feature, network, ha, depth, layer, deep, architecture, time, image, learned, large, en, model pre, learned feature</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>csv, name, code, map, city, open, latitude, longitude, number, street, district, india, open data, country, coordinate, crime, http, contains, license, acknowledgement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>variable, integer, customer, attribute, categorical, datasets, park, original, file, bird, credit, german, http, continuous, one, good, two, form, version, ml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>back, lower, air, sound, may, low, flight, mile, cause, distance, body, problem, go, normal, taking, smaller, month, large, might, probability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>others, data science, community, world, science, inspiration, inspiration data, acknowledgement, question, thanks, answered, research, want, want see, largest, world largest, science community, past research, see, past</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>space, solar, earth, sun, node, nasa, near, total, occur, center, flight, possible, cover, completely, experience, one, two, acknowledgement, type, location</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>health, google, search, interest, care, condition, related, top, tech, highest, increase, towards, based, worldwide, volume, predictor, pressure, ongoing, cover, similarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>new, york, new york, school, city, numeric, student, uci, york city, job, education, learning, http, edu, machine learning, machine, ml, grade, archive, http archive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>price, description, review, doe, yet, dataset doe, doe description, description yet, company, product, sale, stock, property, car, market, restaurant, date, city, total, inspiration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>university, state, college, california, north, institute, university california, san, technology, new, francisco, south, john, washington, st, andrew, international, county, science, community</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>txt, instance, cell, group, cancer, number, medical, attribute, hospital, method, application, expression, missing, size, pp, call, body, removed, set, flow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                    Terms per Topic\n",
              "Topic1   image, file, one, time, wa, contains, acknowledgement, inspiration, day, like, using, would, feature, used, set, see, available, column, label, different                                                                 \n",
              "Topic2   image, wa, time, text, survey, csv, acknowledgement, using, file, statistic, open, ha, numeric, set, value, price, http, learning, database, paper                                                                        \n",
              "Topic3   player, csv, team, game, file, wa, http, user, movie, time, match, com, number, ha, www, information, http www, set, id, rating                                                                                           \n",
              "Topic4   context data, love, claim, basic, stats, inspiration, filed, al, east, acknowledgement, modification, john, lost, player, sometimes, file, contains, number, ha, year                                                     \n",
              "Topic5   wa, state, year, information, acknowledgement, time, number, country, ha, available, inspiration, national, source, public, date, record, report, survey, includes, world                                                 \n",
              "Topic6   text, word, language, file, corpus, id, http, use, wa, title, contains, article, license, question, english, page, name, org, author, collection                                                                          \n",
              "Topic7   set, data set, value, column, activity, wa, mean, energy, model, sample, feature, using, use, used, research, sensor, attribute, system, temperature, specie                                                              \n",
              "Topic8   com, http, tweet, github, github com, twitter, http github, vector, http www, www, word, nltk, model, kaggle, acknowledgement, found, zip, code, tree, used                                                               \n",
              "Topic9   year, age, type, month, animal, rate, sex, male, female, outcome, bank, birth, name, credit, number, payment, set, amount, usd, data set                                                                                  \n",
              "Topic10  model, trained, de, pre, pre trained, trained model, feature, network, ha, depth, layer, deep, architecture, time, image, learned, large, en, model pre, learned feature                                                  \n",
              "Topic11  csv, name, code, map, city, open, latitude, longitude, number, street, district, india, open data, country, coordinate, crime, http, contains, license, acknowledgement                                                   \n",
              "Topic12  variable, integer, customer, attribute, categorical, datasets, park, original, file, bird, credit, german, http, continuous, one, good, two, form, version, ml                                                            \n",
              "Topic13  back, lower, air, sound, may, low, flight, mile, cause, distance, body, problem, go, normal, taking, smaller, month, large, might, probability                                                                            \n",
              "Topic14  others, data science, community, world, science, inspiration, inspiration data, acknowledgement, question, thanks, answered, research, want, want see, largest, world largest, science community, past research, see, past\n",
              "Topic15  space, solar, earth, sun, node, nasa, near, total, occur, center, flight, possible, cover, completely, experience, one, two, acknowledgement, type, location                                                              \n",
              "Topic16  health, google, search, interest, care, condition, related, top, tech, highest, increase, towards, based, worldwide, volume, predictor, pressure, ongoing, cover, similarity                                              \n",
              "Topic17  new, york, new york, school, city, numeric, student, uci, york city, job, education, learning, http, edu, machine learning, machine, ml, grade, archive, http archive                                                     \n",
              "Topic18  price, description, review, doe, yet, dataset doe, doe description, description yet, company, product, sale, stock, property, car, market, restaurant, date, city, total, inspiration                                     \n",
              "Topic19  university, state, college, california, north, institute, university california, san, technology, new, francisco, south, john, washington, st, andrew, international, county, science, community                          \n",
              "Topic20  txt, instance, cell, group, cancer, number, medical, attribute, hospital, method, application, expression, missing, size, pp, call, body, removed, set, flow                                                              "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8BnOkDImgOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "021e8cf1-0436-4439-88c5-66967845b246"
      },
      "source": [
        "pd.options.display.float_format = '{:,.3f}'.format\n",
        "dt_df = pd.DataFrame(document_topics, \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.T"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2110</th>\n",
              "      <th>2111</th>\n",
              "      <th>2112</th>\n",
              "      <th>2113</th>\n",
              "      <th>2114</th>\n",
              "      <th>2115</th>\n",
              "      <th>2116</th>\n",
              "      <th>2117</th>\n",
              "      <th>2118</th>\n",
              "      <th>2119</th>\n",
              "      <th>2120</th>\n",
              "      <th>2121</th>\n",
              "      <th>2122</th>\n",
              "      <th>2123</th>\n",
              "      <th>2124</th>\n",
              "      <th>2125</th>\n",
              "      <th>2126</th>\n",
              "      <th>2127</th>\n",
              "      <th>2128</th>\n",
              "      <th>2129</th>\n",
              "      <th>2130</th>\n",
              "      <th>2131</th>\n",
              "      <th>2132</th>\n",
              "      <th>2133</th>\n",
              "      <th>2134</th>\n",
              "      <th>2135</th>\n",
              "      <th>2136</th>\n",
              "      <th>2137</th>\n",
              "      <th>2138</th>\n",
              "      <th>2139</th>\n",
              "      <th>2140</th>\n",
              "      <th>2141</th>\n",
              "      <th>2142</th>\n",
              "      <th>2143</th>\n",
              "      <th>2144</th>\n",
              "      <th>2145</th>\n",
              "      <th>2146</th>\n",
              "      <th>2147</th>\n",
              "      <th>2148</th>\n",
              "      <th>2149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1</th>\n",
              "      <td>0.416</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.401</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.368</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.978</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.908</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.874</td>\n",
              "      <td>0.905</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.851</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.934</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.345</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.587</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T6</th>\n",
              "      <td>0.072</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.989</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.415</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T7</th>\n",
              "      <td>0.324</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.523</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T8</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T9</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.034</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T10</th>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T11</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T12</th>\n",
              "      <td>0.088</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.842</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T13</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.988</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T14</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.990</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T15</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T16</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.025</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T17</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.579</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T18</th>\n",
              "      <td>0.069</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.573</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.564</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T19</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T20</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 2150 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2     3     4     5  ...  2144  2145  2146  2147  2148  2149\n",
              "T1  0.416 0.214 0.413 0.067 0.226 0.109  ... 0.008 0.001 0.764 0.002 0.720 0.001\n",
              "T2  0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T3  0.000 0.775 0.291 0.070 0.267 0.361  ... 0.008 0.851 0.001 0.002 0.001 0.001\n",
              "T4  0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.030 0.001\n",
              "T5  0.000 0.000 0.268 0.764 0.210 0.321  ... 0.008 0.001 0.001 0.002 0.001 0.383\n",
              "T6  0.072 0.000 0.000 0.080 0.000 0.070  ... 0.008 0.001 0.001 0.966 0.001 0.001\n",
              "T7  0.324 0.000 0.000 0.000 0.000 0.022  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T8  0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.047 0.002 0.001 0.001\n",
              "T9  0.000 0.000 0.025 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.226 0.575\n",
              "T10 0.024 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T11 0.000 0.000 0.000 0.000 0.000 0.010  ... 0.008 0.132 0.118 0.002 0.001 0.001\n",
              "T12 0.088 0.000 0.000 0.000 0.000 0.000  ... 0.842 0.001 0.059 0.002 0.001 0.001\n",
              "T13 0.000 0.008 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T14 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T15 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T16 0.000 0.000 0.000 0.003 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.023\n",
              "T17 0.000 0.000 0.000 0.000 0.000 0.025  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T18 0.069 0.000 0.000 0.000 0.291 0.067  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T19 0.000 0.000 0.000 0.014 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T20 0.000 0.000 0.000 0.000 0.000 0.013  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "\n",
              "[20 rows x 2150 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1zeUDjgo7DH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63ab1e98-a336-42ea-a193-5be950a89f22"
      },
      "source": [
        "pd.options.display.float_format = '{:,.5f}'.format\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "max_contrib_topics = dt_df.max(axis=0)\n",
        "dominant_topics = max_contrib_topics.index\n",
        "contrib_perc = max_contrib_topics.values\n",
        "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0]\n",
        "                       for t in dominant_topics]\n",
        "documents = [papers[i] for i in document_numbers]\n",
        "\n",
        "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
        "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
        "                          'Paper Name': documents})\n",
        "results_df"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Paper Num</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Paper Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>T1</td>\n",
              "      <td>0.99435</td>\n",
              "      <td>1977</td>\n",
              "      <td>image, file, one, time, wa, contains, acknowledgement, inspiration, day, like, using, would, feature, used, set, see, available, column, label, different</td>\n",
              "      <td>Context\\nWhile studying neural networks in machine learning, I found an ingenious 2-D scatter pattern at the cn231 course by Andrej Karpathy. Decision boundaries for the three classes of points ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>T2</td>\n",
              "      <td>0.05000</td>\n",
              "      <td>589</td>\n",
              "      <td>image, wa, time, text, survey, csv, acknowledgement, using, file, statistic, open, ha, numeric, set, value, price, http, learning, database, paper</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>T3</td>\n",
              "      <td>0.99554</td>\n",
              "      <td>1881</td>\n",
              "      <td>player, csv, team, game, file, wa, http, user, movie, time, match, com, number, ha, www, information, http www, set, id, rating</td>\n",
              "      <td>Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>T4</td>\n",
              "      <td>0.91364</td>\n",
              "      <td>2037</td>\n",
              "      <td>context data, love, claim, basic, stats, inspiration, filed, al, east, acknowledgement, modification, john, lost, player, sometimes, file, contains, number, ha, year</td>\n",
              "      <td>Context\\nData of hitters in MLB's AL East\\nContent\\nBasic fundamental data on AL East hitters sorted descending by Plate Appearances\\nAcknowledgements\\nInspiration\\nI love baseball and stats.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>T5</td>\n",
              "      <td>0.99195</td>\n",
              "      <td>1811</td>\n",
              "      <td>wa, state, year, information, acknowledgement, time, number, country, ha, available, inspiration, national, source, public, date, record, report, survey, includes, world</td>\n",
              "      <td>Context:\\nMapping the Klan is a rough timeline of the rise of the second Ku Klux Klan between 1915 and 1940. Each red dot shows a local unit or \"Klavern.\" The official numbers for each Klavern ind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>T6</td>\n",
              "      <td>0.99379</td>\n",
              "      <td>1912</td>\n",
              "      <td>text, word, language, file, corpus, id, http, use, wa, title, contains, article, license, question, english, page, name, org, author, collection</td>\n",
              "      <td>Context:\\nSome words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>T7</td>\n",
              "      <td>0.99525</td>\n",
              "      <td>1986</td>\n",
              "      <td>set, data set, value, column, activity, wa, mean, energy, model, sample, feature, using, use, used, research, sensor, attribute, system, temperature, specie</td>\n",
              "      <td>This research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from France between Jan-2016 and Aug-2016 were interviewed in ord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>T8</td>\n",
              "      <td>0.98827</td>\n",
              "      <td>1870</td>\n",
              "      <td>com, http, tweet, github, github com, twitter, http github, vector, http www, www, word, nltk, model, kaggle, acknowledgement, found, zip, code, tree, used</td>\n",
              "      <td>Context\\nThe maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus (perhaps ACE ACE 2004 Multilingual Training Corpus?)\\nIt will load an nltk.chunk.name...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>T9</td>\n",
              "      <td>0.99472</td>\n",
              "      <td>1841</td>\n",
              "      <td>year, age, type, month, animal, rate, sex, male, female, outcome, bank, birth, name, credit, number, payment, set, amount, usd, data set</td>\n",
              "      <td>Context\\nThe Austin Animal Center is the largest no-kill animal shelter in the United States that provides care and shelter to over 18,000 animals each year and is involved in a range of county, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>T10</td>\n",
              "      <td>0.99625</td>\n",
              "      <td>1355</td>\n",
              "      <td>model, trained, de, pre, pre trained, trained model, feature, network, ha, depth, layer, deep, architecture, time, image, learned, large, en, model pre, learned feature</td>\n",
              "      <td>InceptionV3\\nRethinking the Inception Architecture for Computer Vision\\nConvolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>T11</td>\n",
              "      <td>0.99208</td>\n",
              "      <td>1276</td>\n",
              "      <td>csv, name, code, map, city, open, latitude, longitude, number, street, district, india, open data, country, coordinate, crime, http, contains, license, acknowledgement</td>\n",
              "      <td>Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>T12</td>\n",
              "      <td>0.99326</td>\n",
              "      <td>1838</td>\n",
              "      <td>variable, integer, customer, attribute, categorical, datasets, park, original, file, bird, credit, german, http, continuous, one, good, two, form, version, ml</td>\n",
              "      <td>First of all, this dataset is not mine!\\nI just want to use this dataset to approve my machine learning skills.\\nSo I upload this one! :-)\\nHope you like it!\\n=====================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>T13</td>\n",
              "      <td>0.98827</td>\n",
              "      <td>106</td>\n",
              "      <td>back, lower, air, sound, may, low, flight, mile, cause, distance, body, problem, go, normal, taking, smaller, month, large, might, probability</td>\n",
              "      <td>310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>T14</td>\n",
              "      <td>0.98989</td>\n",
              "      <td>2134</td>\n",
              "      <td>others, data science, community, world, science, inspiration, inspiration data, acknowledgement, question, thanks, answered, research, want, want see, largest, world largest, science community, pa...</td>\n",
              "      <td>Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>T15</td>\n",
              "      <td>0.98516</td>\n",
              "      <td>272</td>\n",
              "      <td>space, solar, earth, sun, node, nasa, near, total, occur, center, flight, possible, cover, completely, experience, one, two, acknowledgement, type, location</td>\n",
              "      <td>Context\\nEclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral, umbral, or antumbral shadows...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>T16</td>\n",
              "      <td>0.93214</td>\n",
              "      <td>2015</td>\n",
              "      <td>health, google, search, interest, care, condition, related, top, tech, highest, increase, towards, based, worldwide, volume, predictor, pressure, ongoing, cover, similarity</td>\n",
              "      <td>The top \"how to\" related searches on Google from 2004 to 2017 worldwide. Top searches are searches with the highest search interest based on volume.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>T17</td>\n",
              "      <td>0.98797</td>\n",
              "      <td>1797</td>\n",
              "      <td>new, york, new york, school, city, numeric, student, uci, york city, job, education, learning, http, edu, machine learning, machine, ml, grade, archive, http archive</td>\n",
              "      <td>Context:\\nThe City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed, e.g. “gut renovated”) buildings in New York City. These documents assert that the cit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>T18</td>\n",
              "      <td>0.98920</td>\n",
              "      <td>593</td>\n",
              "      <td>price, description, review, doe, yet, dataset doe, doe description, description yet, company, product, sale, stock, property, car, market, restaurant, date, city, total, inspiration</td>\n",
              "      <td>About This Data\\nThis is a list of over 18,000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address, city, state, busi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>T19</td>\n",
              "      <td>0.99823</td>\n",
              "      <td>2071</td>\n",
              "      <td>university, state, college, california, north, institute, university california, san, technology, new, francisco, south, john, washington, st, andrew, international, county, science, community</td>\n",
              "      <td>Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>T20</td>\n",
              "      <td>0.99749</td>\n",
              "      <td>2011</td>\n",
              "      <td>txt, instance, cell, group, cancer, number, medical, attribute, hospital, method, application, expression, missing, size, pp, call, body, removed, set, flow</td>\n",
              "      <td>Context\\nThis breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.\\nContent\\nPast Usage:\\nAttributes 2 through 10 have been used to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topic  ...                                                                                                                                                                                               Paper Name\n",
              "Topic1              T1  ...  Context\\nWhile studying neural networks in machine learning, I found an ingenious 2-D scatter pattern at the cn231 course by Andrej Karpathy. Decision boundaries for the three classes of points ca...\n",
              "Topic2              T2  ...                                                                                                                                                                                                      nan\n",
              "Topic3              T3  ...  Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...\n",
              "Topic4              T4  ...          Context\\nData of hitters in MLB's AL East\\nContent\\nBasic fundamental data on AL East hitters sorted descending by Plate Appearances\\nAcknowledgements\\nInspiration\\nI love baseball and stats.\n",
              "Topic5              T5  ...  Context:\\nMapping the Klan is a rough timeline of the rise of the second Ku Klux Klan between 1915 and 1940. Each red dot shows a local unit or \"Klavern.\" The official numbers for each Klavern ind...\n",
              "Topic6              T6  ...  Context:\\nSome words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. Th...\n",
              "Topic7              T7  ...  This research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from France between Jan-2016 and Aug-2016 were interviewed in ord...\n",
              "Topic8              T8  ...  Context\\nThe maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus (perhaps ACE ACE 2004 Multilingual Training Corpus?)\\nIt will load an nltk.chunk.name...\n",
              "Topic9              T9  ...  Context\\nThe Austin Animal Center is the largest no-kill animal shelter in the United States that provides care and shelter to over 18,000 animals each year and is involved in a range of county, c...\n",
              "Topic10            T10  ...  InceptionV3\\nRethinking the Inception Architecture for Computer Vision\\nConvolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since...\n",
              "Topic11            T11  ...  Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...\n",
              "Topic12            T12  ...  First of all, this dataset is not mine!\\nI just want to use this dataset to approve my machine learning skills.\\nSo I upload this one! :-)\\nHope you like it!\\n=====================================...\n",
              "Topic13            T13  ...  310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...\n",
              "Topic14            T14  ...  Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...\n",
              "Topic15            T15  ...  Context\\nEclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral, umbral, or antumbral shadows...\n",
              "Topic16            T16  ...                                                     The top \"how to\" related searches on Google from 2004 to 2017 worldwide. Top searches are searches with the highest search interest based on volume.\n",
              "Topic17            T17  ...  Context:\\nThe City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed, e.g. “gut renovated”) buildings in New York City. These documents assert that the cit...\n",
              "Topic18            T18  ...  About This Data\\nThis is a list of over 18,000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address, city, state, busi...\n",
              "Topic19            T19  ...  Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...\n",
              "Topic20            T20  ...  Context\\nThis breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.\\nContent\\nPast Usage:\\nAttributes 2 through 10 have been used to...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEiZSkCvo9Zf",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Non-Negative Matrix Factorization (NMF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bzFpRIAo7U0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d044439b-a6b0-4992-8b3a-74be5fa02e8c"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500,\n",
        "                random_state=42, alpha=.1, l1_ratio=.85)\n",
        "document_topics = nmf_model.fit_transform(cv_features)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.73 s, sys: 226 ms, total: 4.96 s\n",
            "Wall time: 4.72 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3df-gbo7a1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e62857dd-c59c-4427-ef42-d9b95ad08594"
      },
      "source": [
        "topic_terms = nmf_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                  Terms per Topic\n",
              "Topic1   university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago \n",
              "Topic2   http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match                         \n",
              "Topic3   player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit                             \n",
              "Topic4   integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time                              \n",
              "Topic5   date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status                             \n",
              "Topic6   year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people                        \n",
              "Topic7   numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class                                   \n",
              "Topic8   csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included                                \n",
              "Topic9   model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using\n",
              "Topic10  value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time                                        \n",
              "Topic11  child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last                                       \n",
              "Topic12  de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value                                                    \n",
              "Topic13  file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org                                                  \n",
              "Topic14  image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease                             \n",
              "Topic15  word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also                      \n",
              "Topic16  fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record                          \n",
              "Topic17  race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run                                        \n",
              "Topic18  station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date                                    \n",
              "Topic19  police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago                                   \n",
              "Topic20  health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type                     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66q9hgKeo7Y1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "55fcacd1-ec78-405a-a4b9-49e6b7fef279"
      },
      "source": [
        "pd.options.display.float_format = '{:,.3f}'.format\n",
        "dt_df = pd.DataFrame(document_topics, \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.head(10)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "      <th>T11</th>\n",
              "      <th>T12</th>\n",
              "      <th>T13</th>\n",
              "      <th>T14</th>\n",
              "      <th>T15</th>\n",
              "      <th>T16</th>\n",
              "      <th>T17</th>\n",
              "      <th>T18</th>\n",
              "      <th>T19</th>\n",
              "      <th>T20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.184</td>\n",
              "      <td>0.636</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.402</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     T1    T2    T3    T4    T5    T6  ...   T15   T16   T17   T18   T19   T20\n",
              "0 0.000 0.195 0.000 0.000 0.019 0.000  ... 0.000 0.000 0.063 0.255 0.000 0.026\n",
              "1 0.000 0.665 0.709 0.047 0.000 0.000  ... 0.000 0.000 0.032 0.025 0.000 0.000\n",
              "2 0.001 0.476 0.013 0.058 0.089 0.023  ... 0.003 0.002 0.042 0.000 0.000 0.001\n",
              "3 0.184 0.636 0.000 0.000 0.218 0.000  ... 0.000 0.548 0.210 0.000 0.047 0.178\n",
              "4 0.000 0.214 0.023 0.000 0.031 0.000  ... 0.000 0.016 0.128 0.013 0.006 0.000\n",
              "5 0.010 0.357 0.067 0.038 0.031 0.035  ... 0.000 0.070 0.143 0.187 0.000 0.375\n",
              "6 0.000 0.088 0.008 0.000 0.002 0.000  ... 0.026 0.000 0.000 0.005 0.000 0.000\n",
              "7 0.000 0.101 0.000 0.000 0.013 0.008  ... 0.007 0.000 0.000 0.011 0.000 0.033\n",
              "8 0.000 0.193 0.000 0.000 0.110 0.059  ... 0.001 0.000 0.000 0.108 0.000 0.000\n",
              "9 0.000 0.217 0.001 0.017 0.000 0.011  ... 0.000 0.004 0.128 0.000 0.025 0.167\n",
              "\n",
              "[10 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1vJRNwipBNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b017e6ae-5ed9-4dd7-c55c-492456c0420f"
      },
      "source": [
        "pd.options.display.float_format = '{:,.5f}'.format\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "max_score_topics = dt_df.max(axis=0)\n",
        "dominant_topics = max_score_topics.index\n",
        "term_score = max_score_topics.values\n",
        "document_numbers = [dt_df[dt_df[t] == max_score_topics.loc[t]].index[0]\n",
        "                       for t in dominant_topics]\n",
        "documents = [papers[i] for i in document_numbers]\n",
        "\n",
        "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Max Score': term_score,\n",
        "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
        "                          'Paper Name': documents})\n",
        "results_df"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Max Score</th>\n",
              "      <th>Paper Num</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Paper Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>T1</td>\n",
              "      <td>16.32296</td>\n",
              "      <td>2071</td>\n",
              "      <td>university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago</td>\n",
              "      <td>Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>T2</td>\n",
              "      <td>1.63533</td>\n",
              "      <td>585</td>\n",
              "      <td>http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match</td>\n",
              "      <td>About the Missing Migrants Data\\nThis data is sourced from the International Organization for Migration. The data is part of a specific project called the Missing Migrants Project which tracks dea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>T3</td>\n",
              "      <td>14.46719</td>\n",
              "      <td>627</td>\n",
              "      <td>player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit</td>\n",
              "      <td>Context &amp; Content\\nThis dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) population...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>T4</td>\n",
              "      <td>13.65851</td>\n",
              "      <td>41</td>\n",
              "      <td>integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time</td>\n",
              "      <td>Introduction\\nIn 2013, students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.\\nThe data file (responses.csv) consists of 1010 rows and 150 co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>T5</td>\n",
              "      <td>12.93043</td>\n",
              "      <td>1469</td>\n",
              "      <td>date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status</td>\n",
              "      <td>Context:\\nThis dataset contains pending and registered trademark text data (no drawings/images) to include word mark, serial number, registration number, filing date, registration date, goods and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>T6</td>\n",
              "      <td>7.93463</td>\n",
              "      <td>238</td>\n",
              "      <td>year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people</td>\n",
              "      <td>The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>T7</td>\n",
              "      <td>8.89455</td>\n",
              "      <td>13</td>\n",
              "      <td>numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class</td>\n",
              "      <td>A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>T8</td>\n",
              "      <td>6.12363</td>\n",
              "      <td>507</td>\n",
              "      <td>csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included</td>\n",
              "      <td>Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nMinistry of Human Resource Development (MHRD), Govt of India has initiated an All India Survey on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>T9</td>\n",
              "      <td>2.10993</td>\n",
              "      <td>1357</td>\n",
              "      <td>model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using</td>\n",
              "      <td>DenseNet-201\\nDensely Connected Convolutional Networks\\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>T10</td>\n",
              "      <td>7.23471</td>\n",
              "      <td>416</td>\n",
              "      <td>value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time</td>\n",
              "      <td>Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>T11</td>\n",
              "      <td>7.73633</td>\n",
              "      <td>1181</td>\n",
              "      <td>child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last</td>\n",
              "      <td>Context\\nAbraham Lincoln's election produced Southern secession, war, and abolition. This dataset was used to study connections between news and slave prices for the period 1856-1861. By August 18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>T12</td>\n",
              "      <td>4.62960</td>\n",
              "      <td>812</td>\n",
              "      <td>de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value</td>\n",
              "      <td>«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>T13</td>\n",
              "      <td>4.97141</td>\n",
              "      <td>1963</td>\n",
              "      <td>file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org</td>\n",
              "      <td>Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>T14</td>\n",
              "      <td>3.33996</td>\n",
              "      <td>183</td>\n",
              "      <td>image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease</td>\n",
              "      <td>NIH Chest X-ray Dataset\\nNational Institutes of Health Chest X-Ray Dataset\\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>T15</td>\n",
              "      <td>2.91703</td>\n",
              "      <td>1927</td>\n",
              "      <td>word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also</td>\n",
              "      <td>Context\\nHuman communication abilities have greatly evolved with time. Speech/Text/Images/Videos are the channels we often use to communicate, store/share information.\"Text\" is one of the primary ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>T16</td>\n",
              "      <td>7.54752</td>\n",
              "      <td>120</td>\n",
              "      <td>fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record</td>\n",
              "      <td>Context:\\nThis data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>T17</td>\n",
              "      <td>7.23678</td>\n",
              "      <td>601</td>\n",
              "      <td>race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run</td>\n",
              "      <td>Can you beat the market?\\nHorse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. Inspired by the pioneers of computer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>T18</td>\n",
              "      <td>9.11955</td>\n",
              "      <td>160</td>\n",
              "      <td>station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date</td>\n",
              "      <td>Version 5 Description\\nTLDR\\nThe directory 1-1-16_5-31-17_Weather contains 1663 files (one for each of the 1663 stations in Japan)\\nAs its name implies, the data is from the same date window as th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>T19</td>\n",
              "      <td>8.26405</td>\n",
              "      <td>257</td>\n",
              "      <td>police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago</td>\n",
              "      <td>Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis dataset contains complete information about various aspects of crimes happened in India from...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>T20</td>\n",
              "      <td>8.32362</td>\n",
              "      <td>571</td>\n",
              "      <td>health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type</td>\n",
              "      <td>Context\\nHealth care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. 58% of US community hospit...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topic  ...                                                                                                                                                                                               Paper Name\n",
              "Topic1              T1  ...  Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...\n",
              "Topic2              T2  ...  About the Missing Migrants Data\\nThis data is sourced from the International Organization for Migration. The data is part of a specific project called the Missing Migrants Project which tracks dea...\n",
              "Topic3              T3  ...  Context & Content\\nThis dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) population...\n",
              "Topic4              T4  ...  Introduction\\nIn 2013, students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.\\nThe data file (responses.csv) consists of 1010 rows and 150 co...\n",
              "Topic5              T5  ...  Context:\\nThis dataset contains pending and registered trademark text data (no drawings/images) to include word mark, serial number, registration number, filing date, registration date, goods and ...\n",
              "Topic6              T6  ...  The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...\n",
              "Topic7              T7  ...  A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...\n",
              "Topic8              T8  ...  Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nMinistry of Human Resource Development (MHRD), Govt of India has initiated an All India Survey on...\n",
              "Topic9              T9  ...  DenseNet-201\\nDensely Connected Convolutional Networks\\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter...\n",
              "Topic10            T10  ...  Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...\n",
              "Topic11            T11  ...  Context\\nAbraham Lincoln's election produced Southern secession, war, and abolition. This dataset was used to study connections between news and slave prices for the period 1856-1861. By August 18...\n",
              "Topic12            T12  ...  «Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...\n",
              "Topic13            T13  ...  Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...\n",
              "Topic14            T14  ...  NIH Chest X-ray Dataset\\nNational Institutes of Health Chest X-Ray Dataset\\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clini...\n",
              "Topic15            T15  ...  Context\\nHuman communication abilities have greatly evolved with time. Speech/Text/Images/Videos are the channels we often use to communicate, store/share information.\"Text\" is one of the primary ...\n",
              "Topic16            T16  ...  Context:\\nThis data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to suppo...\n",
              "Topic17            T17  ...  Can you beat the market?\\nHorse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. Inspired by the pioneers of computer...\n",
              "Topic18            T18  ...  Version 5 Description\\nTLDR\\nThe directory 1-1-16_5-31-17_Weather contains 1663 files (one for each of the 1663 stations in Japan)\\nAs its name implies, the data is from the same date window as th...\n",
              "Topic19            T19  ...  Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis dataset contains complete information about various aspects of crimes happened in India from...\n",
              "Topic20            T20  ...  Context\\nHealth care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. 58% of US community hospit...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQoLkjjpJB7",
        "colab_type": "text"
      },
      "source": [
        "## Persisting Model and Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W1M0q23pL38",
        "colab_type": "text"
      },
      "source": [
        "This is just for visualizing the topics in the other notebook (since PyLDAViz expands the notebook size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-ljpUCpBVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dill\n",
        "\n",
        "with open('nmf_model.pkl', 'wb') as f:\n",
        "    dill.dump(nmf_model, f)\n",
        "with open('cv_features.pkl', 'wb') as f:\n",
        "    dill.dump(cv_features, f)\n",
        "with open('cv.pkl', 'wb') as f:\n",
        "    dill.dump(cv, f)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZX5ZUZZqkZt",
        "colab_type": "text"
      },
      "source": [
        "# Ch06d - Visualizing Topic Models with pyLDAvis.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF2v03rErLtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "299637a6-2615-4e86-fc63-acf97df58176"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 17.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (49.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=b920c9aa31ce1fd04349b206e8fd3a5714a8245fdcca3f603a8a2ff700953c69\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=45858e232b86e35ade290994127177ddb0b1f039494b9eb416abaf7cd8bf9561\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On8UrEeOpBTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import dill\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyedglvPpBRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('nmf_model.pkl', 'rb') as f:\n",
        "    nmf_model = dill.load(f)\n",
        "with open('cv_features.pkl', 'rb') as f:\n",
        "    cv_features = dill.load(f)\n",
        "with open('cv.pkl', 'rb') as f:\n",
        "    cv = dill.load(f)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efPPNcboqqP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1923d813-6af7-4102-8e14-dff45eb7338f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "top_terms = 20\n",
        "TOTAL_TOPICS = 20\n",
        "vocabulary = np.array(cv.get_feature_names())\n",
        "topic_terms = nmf_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                  Terms per Topic\n",
              "Topic1   university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago \n",
              "Topic2   http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match                         \n",
              "Topic3   player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit                             \n",
              "Topic4   integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time                              \n",
              "Topic5   date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status                             \n",
              "Topic6   year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people                        \n",
              "Topic7   numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class                                   \n",
              "Topic8   csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included                                \n",
              "Topic9   model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using\n",
              "Topic10  value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time                                        \n",
              "Topic11  child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last                                       \n",
              "Topic12  de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value                                                    \n",
              "Topic13  file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org                                                  \n",
              "Topic14  image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease                             \n",
              "Topic15  word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also                      \n",
              "Topic16  fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record                          \n",
              "Topic17  race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run                                        \n",
              "Topic18  station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date                                    \n",
              "Topic19  police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago                                   \n",
              "Topic20  health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type                     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGN2-sTzs_BX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e195b4d0-93d5-4f2c-bbe2-f600788df874"
      },
      "source": [
        "cv_features.shape"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2150, 2097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIOQM0EqqaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "95e1098c-b43f-48b9-880d-54ee9fc064cd"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(nmf_model, cv_features, cv, mds='mmds')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-af909b6397f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mmds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA5Y0iGUqqY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "927cf40e-38af-44ed-bb73-f0c048dd27f4"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(nmf_model, cv_features, cv, mds='tsne')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-93fd4556ecc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoFQVIEgqqXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.sklearn.prepare(nmf_model, cv_features, cv, mds='pcoa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1aJ4bucuLJ8",
        "colab_type": "text"
      },
      "source": [
        "# Visualize from KAGGLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "texmjNBSuKHh",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/canggih/topic-modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmC4iz8-uSjk",
        "colab_type": "text"
      },
      "source": [
        "Topic Modeling\n",
        "Topic modeling is a statistical model to discover the abstract \"topics\" that occur in a collection of documents.\n",
        "It is commonly used in text document. But nowadays, in social media analysis, topic modeling is an emerging research area.\n",
        "One of the most popular algorithms used is Latent Dirichlet Allocation which was proposed by\n",
        "David Blei et al in 2003.\n",
        "Here, I want to perform topic modeling for the upvoted kaggle dataset.\n",
        "\n",
        "Some notes on topic modeling:\n",
        "\n",
        "To determine the number topics, it is common to use elbow method with perplexity score as its cost function.\n",
        "To evaluate the models, we can calculate topic coherence.\n",
        "Finally, to interpret the topics, as studied in social science research, there is triangulation metho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiPfFQk8ugOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "fe7ee313-3256-42c2-d189-b699e8965a71"
      },
      "source": [
        "!pip install stop-words"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=dc2c48581b2504d3a565ad5b592e854c141b8f366154c1655b1f8b433c506bb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTAs-TqbqqTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import pyLDAvis.gensim"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VRZkvW9uUOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = r'\\b[^\\d\\W]+\\b'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "en_stop = get_stop_words('en')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BVjK6fguroZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "590c97a0-b490-4c56-f10f-fcf30877beec"
      },
      "source": [
        "df=pd.read_csv('https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/voted-kaggle-dataset.csv?raw=true')\n",
        "print(data.shape)\n",
        "# sample data\n",
        "print(df['Description'].head(2))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 15)\n",
            "0    The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "1    The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlayers and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates\\nTeam line up with squad formation (X, Y coordinates)\\nBetting odds from up to 10 providers\\nDetailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\\n*16th Oct 2016: New table containing teams' attributes from FIFA !\\nOriginal Data Source:\\nYou can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from:\\nhttp://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events\\nhttp://www.football-data.co.uk/ : betting odds. Click here to understand the column naming system for betting odds:\\nhttp://sofifa.com/ : players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.\\nWhen you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. I have called those foreign keys \"api_id\".\\nImproving the dataset:\\nYou will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games, national cups, Champion's League and Europa League. Please ask me if you're after a specific tournament.\\nPlease get in touch with me if you want to help improve this dataset.\\nCLICK HERE TO ACCESS THE PROJECT GITHUB\\nImportant note for people interested in using the crawlers: since I first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players ('Player Spider') will not work until i've updated it.\\nExploring the data:\\nNow that's the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:\\nThe Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I've achieved so far using my own SVM. Though it may sound high for such a random sport game, you've got to know that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.\\nProbabilities vs Odds\\nWhen running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.\\nExplore and visualize features\\nWith access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows, Guardiola himself may hire one of you some day!\n",
            "Name: Description, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1YkZpz_ux_W",
        "colab_type": "text"
      },
      "source": [
        "## Perform Tokenization, Words removal, and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq46oD_CuxKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b2174bea-2ab9-427d-e4c9-101af5ab34c0"
      },
      "source": [
        "# list for tokenized documents in loop\n",
        "texts = []\n",
        "\n",
        "# loop through document list\n",
        "for i in df['Description'].iteritems():\n",
        "    # clean and tokenize document string\n",
        "    raw = str(i[1]).lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
        "    \n",
        "    # lemmatize tokens\n",
        "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
        "    \n",
        "    # remove word containing only single char\n",
        "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
        "    \n",
        "    # add tokens to list\n",
        "    texts.append(new_lemma_tokens)\n",
        "\n",
        "# sample data\n",
        "print(texts[0])"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'can', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPTCaxW0u1f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq1DJbm-vsf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "73ba5408-d1b2-48a3-d988-c24d8f180b7e"
      },
      "source": [
        "print(corpus[0][:20])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 3), (1, 1), (2, 2), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4As7N0Cv49r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85ced793-e321-45be-813a-329209560996"
      },
      "source": [
        "len(corpus)\n",
        "# 2150 description được biểu diễn bằng word_index and frequency"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHEvVSrqv31b",
        "colab_type": "text"
      },
      "source": [
        "## Generate LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgHtKC9Cvtxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89189dea-b8b7-4bfa-8c52-c18dabcc7d60"
      },
      "source": [
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=20)\n",
        "import pprint\n",
        "pprint.pprint(ldamodel.top_topics(corpus,topn=5))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[([(0.0329417, 'data'),\n",
            "   (0.0123863565, 'content'),\n",
            "   (0.0108074015, 'context'),\n",
            "   (0.010650745, 'dataset'),\n",
            "   (0.009167448, 'acknowledgement')],\n",
            "  -0.32887626989415525),\n",
            " ([(0.024178976, 'price'),\n",
            "   (0.022297142, 'csv'),\n",
            "   (0.011118084, 'data'),\n",
            "   (0.0094378255, 'dataset'),\n",
            "   (0.0083645955, 'context')],\n",
            "  -0.4169131867653458),\n",
            " ([(0.020295177, 'suny'),\n",
            "   (0.008973113, 'southeastern'),\n",
            "   (0.008972908, 'denver'),\n",
            "   (0.006755966, 'xavier'),\n",
            "   (0.0059344852, 'binghamton')],\n",
            "  -0.7284820897876937),\n",
            " ([(0.020129709, 'dataset'),\n",
            "   (0.013130812, 'data'),\n",
            "   (0.011543181, 'can'),\n",
            "   (0.011151869, 'file'),\n",
            "   (0.008492353, 'contains')],\n",
            "  -0.7868496335888502),\n",
            " ([(0.033303306, 'data'),\n",
            "   (0.011286829, 'time'),\n",
            "   (0.009875901, 'dataset'),\n",
            "   (0.00782922, 'name'),\n",
            "   (0.0076212115, 'city')],\n",
            "  -1.3336312210573988),\n",
            " ([(0.016599022, 'de'),\n",
            "   (0.015345448, 'song'),\n",
            "   (0.012352671, 'data'),\n",
            "   (0.009723297, 'row'),\n",
            "   (0.008581463, 'csv')],\n",
            "  -1.6178652324756506),\n",
            " ([(0.028868373, 'cell'),\n",
            "   (0.027811162, 'instance'),\n",
            "   (0.012931919, 'group'),\n",
            "   (0.0110666165, 'number'),\n",
            "   (0.010631614, 'attribute')],\n",
            "  -1.623405682146976),\n",
            " ([(0.02627896, 'data'),\n",
            "   (0.010659817, 'dataset'),\n",
            "   (0.010638159, 'player'),\n",
            "   (0.0102934735, 'year'),\n",
            "   (0.008218511, 'state')],\n",
            "  -1.864988245570594),\n",
            " ([(0.014424611, 'column'),\n",
            "   (0.013530629, 'dataset'),\n",
            "   (0.0129872495, 'activity'),\n",
            "   (0.008110101, 'class'),\n",
            "   (0.0078061116, 'datasets')],\n",
            "  -1.867334523048574),\n",
            " ([(0.0124892, 'health'),\n",
            "   (0.012193404, 'data'),\n",
            "   (0.01131069, 'product'),\n",
            "   (0.009860387, 'numeric'),\n",
            "   (0.009486651, 'number')],\n",
            "  -1.9519382684081048),\n",
            " ([(0.031297255, 'model'),\n",
            "   (0.03018826, 'trained'),\n",
            "   (0.019031756, 'pre'),\n",
            "   (0.017415253, 'dataset'),\n",
            "   (0.017129473, 'restaurant')],\n",
            "  -2.1283814769745826),\n",
            " ([(0.22321583, 'university'),\n",
            "   (0.042242963, 'state'),\n",
            "   (0.027429331, 'college'),\n",
            "   (0.013267666, 'california'),\n",
            "   (0.011463129, 'texas')],\n",
            "  -2.51880602100133),\n",
            " ([(0.009756962, 'name'),\n",
            "   (0.009043854, 'nationality'),\n",
            "   (0.009013263, 'district'),\n",
            "   (0.006538612, 'center'),\n",
            "   (0.006515746, 'data')],\n",
            "  -3.9747938725810696),\n",
            " ([(0.059478242, 'dataset'),\n",
            "   (0.043607693, 'description'),\n",
            "   (0.04078815, 'yet'),\n",
            "   (0.024347667, 'tweet'),\n",
            "   (0.010149399, 'time')],\n",
            "  -4.237350780330964),\n",
            " ([(0.012063888, 'game'),\n",
            "   (0.011968463, 'data'),\n",
            "   (0.011677587, 'dataset'),\n",
            "   (0.010380441, 'arabic'),\n",
            "   (0.010039434, 'text')],\n",
            "  -4.327349661902804)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyApqtvywItX",
        "colab_type": "text"
      },
      "source": [
        "## Visualize the topic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9lrxsN4wFJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "outputId": "1463d640-7d2b-485f-a651-b78e46d91b1f"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1071406375988602963970707658\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1071406375988602963970707658_data = {\"mdsDat\": {\"x\": [-0.1832139589071907, -0.1605233501431215, -0.15770879857706196, -0.16985251223151182, -0.02671714303851343, -0.024833938496956618, 0.17340664773744013, -0.10172295610789067, 0.021689417730886326, 0.00819272029050208, 0.02571897470896414, 0.03481560310317291, 0.25267519735244476, 0.09622476956089314, 0.21184932701794237], \"y\": [0.02015890219935955, 0.04208340771707749, 0.10910767965623869, 0.03963984772462597, -0.01713873140820412, 0.006287464714505001, -0.09435874756984391, -0.005039129867136437, -0.09052437738859001, -0.08201536706330721, -0.06413563456830867, -0.043110873656247994, 0.3206442556619618, -0.040662087643170426, -0.10093660850896008], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [26.052385330200195, 16.507793426513672, 15.528766632080078, 12.973894119262695, 4.0515923500061035, 3.8724255561828613, 3.3751492500305176, 3.2844643592834473, 3.1805951595306396, 2.859600782394409, 2.258455276489258, 2.114459991455078, 2.085764169692993, 1.3683427572250366, 0.48631009459495544]}, \"tinfo\": {\"Term\": [\"university\", \"dataset\", \"state\", \"description\", \"csv\", \"yet\", \"model\", \"data\", \"text\", \"number\", \"time\", \"trained\", \"price\", \"game\", \"image\", \"word\", \"column\", \"player\", \"content\", \"feature\", \"context\", \"tweet\", \"college\", \"group\", \"year\", \"will\", \"pre\", \"name\", \"language\", \"contains\", \"startup\", \"vector\", \"youtube\", \"ted\", \"fasttext\", \"talk\", \"transcript\", \"joke\", \"upload\", \"expression\", \"overview\", \"registration\", \"debate\", \"component\", \"natural\", \"gram\", \"donation\", \"hot\", \"duplicate\", \"extension\", \"newspaper\", \"array\", \"lang\", \"narrative\", \"broad\", \"possibility\", \"trademark\", \"chinese\", \"literature\", \"optional\", \"sentence\", \"processing\", \"language\", \"article\", \"sentiment\", \"sheet\", \"combine\", \"rating\", \"tag\", \"comment\", \"corpus\", \"word\", \"movie\", \"wikipedia\", \"id\", \"title\", \"patient\", \"text\", \"user\", \"english\", \"post\", \"video\", \"version\", \"file\", \"original\", \"contains\", \"can\", \"dataset\", \"image\", \"found\", \"paper\", \"also\", \"format\", \"one\", \"test\", \"csv\", \"information\", \"data\", \"used\", \"use\", \"different\", \"content\", \"available\", \"code\", \"acknowledgement\", \"database\", \"time\", \"context\", \"using\", \"http\", \"number\", \"weapon\", \"solar\", \"flare\", \"instrument\", \"formula\", \"mile\", \"sic\", \"route\", \"uk\", \"trip\", \"driving\", \"critical\", \"observed\", \"wonderful\", \"ta\", \"taxi\", \"throwing\", \"fatality\", \"uber\", \"shapefiles\", \"fr\", \"transport\", \"earthquake\", \"catalog\", \"employer\", \"snowfall\", \"rarity\", \"meter\", \"precipitation\", \"greatest\", \"damage\", \"sun\", \"temperature\", \"impact\", \"fire\", \"water\", \"car\", \"land\", \"energy\", \"accident\", \"company\", \"active\", \"city\", \"measured\", \"specie\", \"air\", \"road\", \"longitude\", \"latitude\", \"traffic\", \"data\", \"location\", \"type\", \"time\", \"hour\", \"name\", \"value\", \"gov\", \"event\", \"based\", \"available\", \"code\", \"new\", \"http\", \"date\", \"area\", \"dataset\", \"york\", \"information\", \"year\", \"number\", \"can\", \"find\", \"content\", \"acknowledgement\", \"inspiration\", \"file\", \"description\", \"income\", \"played\", \"penalty\", \"attainment\", \"appointee\", \"political\", \"caput\", \"kick\", \"depodesta\", \"league\", \"beane\", \"pakistan\", \"enforcement\", \"officer\", \"president\", \"wise\", \"kicker\", \"keeper\", \"employment\", \"court\", \"scored\", \"payment\", \"opponent\", \"playoff\", \"premier\", \"geocoder\", \"disclosure\", \"asset\", \"slugging\", \"debt\", \"crime\", \"violent\", \"campaign\", \"player\", \"federal\", \"bureau\", \"trump\", \"survey\", \"law\", \"election\", \"policy\", \"united\", \"census\", \"team\", \"statistic\", \"sex\", \"population\", \"government\", \"game\", \"education\", \"percentage\", \"rate\", \"year\", \"state\", \"data\", \"variable\", \"per\", \"age\", \"season\", \"information\", \"dataset\", \"country\", \"context\", \"table\", \"acknowledgement\", \"content\", \"number\", \"can\", \"csv\", \"inspiration\", \"http\", \"file\", \"time\", \"easy\", \"wouldn\", \"owe\", \"dog\", \"hand\", \"gesture\", \"disk\", \"behave\", \"sleep\", \"rain\", \"cp\", \"felt\", \"grabbed\", \"sense\", \"plate\", \"seeing\", \"command\", \"recall\", \"doesn\", \"nice\", \"nothing\", \"promise\", \"becoming\", \"plot\", \"broadcast\", \"classical\", \"painting\", \"path\", \"amazonaws\", \"appearance\", \"ll\", \"pipeline\", \"describing\", \"started\", \"attribution\", \"inside\", \"behind\", \"want\", \"others\", \"thank\", \"just\", \"past\", \"without\", \"help\", \"need\", \"get\", \"kernel\", \"speech\", \"neighborhood\", \"music\", \"answered\", \"property\", \"citation\", \"thanks\", \"along\", \"data\", \"see\", \"content\", \"will\", \"world\", \"kaggle\", \"context\", \"acknowledgement\", \"inspiration\", \"make\", \"include\", \"science\", \"one\", \"http\", \"dataset\", \"set\", \"like\", \"column\", \"com\", \"can\", \"time\", \"research\", \"name\", \"new\", \"used\", \"file\", \"asx\", \"ticker\", \"equity\", \"bitcoin\", \"lub\", \"lstm\", \"openaddresses\", \"dub\", \"halloween\", \"bid\", \"tuition\", \"knowing\", \"telecom\", \"remix\", \"auction\", \"ra\", \"cryptocurrencies\", \"ba\", \"giant\", \"stratified\", \"edx\", \"telstra\", \"definite\", \"costume\", \"succeeded\", \"diploma\", \"undervalued\", \"manipulate\", \"candy\", \"machinelearningmastery\", \"gann\", \"price\", \"switching\", \"stock\", \"student\", \"repeat\", \"exchange\", \"teacher\", \"loan\", \"walmart\", \"close\", \"customer\", \"market\", \"csv\", \"school\", \"service\", \"australian\", \"choose\", \"low\", \"weather\", \"forecasting\", \"high\", \"volume\", \"communication\", \"gold\", \"sold\", \"date\", \"context\", \"experience\", \"day\", \"inspiration\", \"time\", \"content\", \"data\", \"dataset\", \"acknowledgement\", \"set\", \"sale\", \"contains\", \"com\", \"can\", \"mean\", \"http\", \"information\", \"year\", \"number\", \"user\", \"angina\", \"strongly\", \"smoker\", \"muscle\", \"disagree\", \"diabetes\", \"nerve\", \"disc\", \"smoking\", \"symptom\", \"deposit\", \"bone\", \"joint\", \"investment\", \"rep\", \"mild\", \"spine\", \"poetry\", \"irritated\", \"degenerating\", \"vacant\", \"effected\", \"pft\", \"xyz\", \"severe\", \"ultrasound\", \"exposure\", \"disability\", \"cervical\", \"strain\", \"treatment\", \"violation\", \"numeric\", \"pain\", \"woman\", \"segmentation\", \"agree\", \"yes\", \"back\", \"health\", \"birth\", \"integer\", \"product\", \"risk\", \"lower\", \"revenue\", \"food\", \"enjoy\", \"cause\", \"month\", \"region\", \"may\", \"number\", \"variable\", \"family\", \"total\", \"data\", \"can\", \"year\", \"day\", \"attribute\", \"set\", \"individual\", \"status\", \"different\", \"centroid\", \"titanic\", \"postcode\", \"calculating\", \"nameprism\", \"polygon\", \"daca\", \"galaxy\", \"migrant\", \"nationality\", \"velocity\", \"geocode\", \"sumit\", \"renewable\", \"arora\", \"originates\", \"ye\", \"wheeler\", \"yifan\", \"daylight\", \"bari\", \"springfield\", \"shark\", \"meizhu\", \"coskun\", \"passengerid\", \"shuchu\", \"cikm\", \"prism\", \"qin\", \"junting\", \"survival\", \"passenger\", \"kumar\", \"district\", \"center\", \"singh\", \"star\", \"population\", \"map\", \"name\", \"mapping\", \"mean\", \"migration\", \"indian\", \"state\", \"point\", \"weighted\", \"calculated\", \"project\", \"chance\", \"complete\", \"data\", \"dataset\", \"research\", \"using\", \"com\", \"http\", \"open\", \"code\", \"analysis\", \"anime\", \"mat\", \"kera\", \"numerical\", \"lake\", \"arm\", \"dsads\", \"trillion\", \"fashion\", \"tissue\", \"lifting\", \"dumbbell\", \"cache_dir\", \"winter\", \"shoe\", \"bing\", \"har\", \"opp_hl\", \"percom\", \"datasets_dir\", \"makedirs\", \"warranty\", \"shall\", \"modify\", \"opened\", \"exercise\", \"parallel\", \"simulate\", \"grouping\", \"pervasive\", \"simulation\", \"software\", \"elementary\", \"computing\", \"denotes\", \"activity\", \"copy\", \"closed\", \"left\", \"sensor\", \"label\", \"imdb\", \"column\", \"pixel\", \"class\", \"recognition\", \"right\", \"input\", \"datasets\", \"learning\", \"feature\", \"model\", \"body\", \"dataset\", \"file\", \"person\", \"use\", \"context\", \"data\", \"following\", \"contains\", \"content\", \"can\", \"research\", \"csv\", \"acknowledgement\", \"new\", \"image\", \"restaurant\", \"transferable\", \"convolutional\", \"layer\", \"imagenet\", \"submission\", \"compute\", \"vgg\", \"beneficial\", \"whichever\", \"residual\", \"trained\", \"convolution\", \"thorough\", \"cuisine\", \"nan\", \"localisation\", \"simonyan\", \"generalise\", \"convnet\", \"zisserman\", \"faceoffs\", \"wondered\", \"coco\", \"pushing\", \"secured\", \"tripadvisor\", \"architecture\", \"horizontal\", \"krakow\", \"pre\", \"shot\", \"learned\", \"achieved\", \"model\", \"null\", \"review\", \"depth\", \"bias\", \"saving\", \"benefit\", \"deep\", \"network\", \"accuracy\", \"feature\", \"else\", \"large\", \"art\", \"weight\", \"image\", \"scale\", \"will\", \"dataset\", \"non\", \"representation\", \"object\", \"time\", \"recognition\", \"use\", \"contains\", \"using\", \"yet\", \"engagement\", \"bottle\", \"inep\", \"clicked\", \"hotel\", \"churn\", \"tweet\", \"hashtags\", \"hashtag\", \"commuter\", \"abandon\", \"yale\", \"nazi\", \"northeastern\", \"enade\", \"refining\", \"trafiklab\", \"enem\", \"eaxmple\", \"lmv\", \"rickshaw\", \"noun\", \"retweeted\", \"zurich\", \"documento\", \"guest\", \"boulder\", \"requester\", \"retweets\", \"pill\", \"description\", \"exam\", \"conference\", \"chest\", \"twitter\", \"frame\", \"someone\", \"dataset\", \"child\", \"sent\", \"vehicle\", \"click\", \"train\", \"many\", \"medium\", \"frequent\", \"json\", \"account\", \"time\", \"number\", \"word\", \"class\", \"second\", \"content\", \"context\", \"game\", \"contains\", \"data\", \"http\", \"information\", \"arabic\", \"pokemon\", \"squeezenet\", \"tournament\", \"alexnet\", \"dnns\", \"dnn\", \"vocalized\", \"nba\", \"ncaa\", \"athlete\", \"museum\", \"datos\", \"halfway\", \"micrometer\", \"imo\", \"shamela\", \"badminton\", \"automl\", \"alphapy\", \"contestant\", \"dutch\", \"matthew\", \"regard\", \"compress\", \"bandwidth\", \"recruiting\", \"omitted\", \"circumstance\", \"marathon\", \"betting\", \"sport\", \"smaller\", \"round\", \"fewer\", \"game\", \"competition\", \"season\", \"score\", \"float\", \"text\", \"result\", \"final\", \"learn\", \"odds\", \"collected\", \"machine\", \"dataset\", \"system\", \"data\", \"feature\", \"level\", \"tool\", \"year\", \"using\", \"model\", \"can\", \"series\", \"context\", \"inspiration\", \"contains\", \"learning\", \"str\", \"lon\", \"lyric\", \"artist\", \"dado\", \"arb\", \"newrow\", \"pd\", \"bike\", \"organic\", \"noticias\", \"meneame\", \"fcity\", \"fzip\", \"fstreet\", \"fieldnames\", \"curr_facility\", \"song\", \"echo\", \"read_csv\", \"louisville\", \"lat\", \"nest\", \"penetrate\", \"buffalo\", \"oakland\", \"pyaudioanalysis\", \"votos\", \"diciembre\", \"eleitorais\", \"repositorio\", \"emotion\", \"facility\", \"la\", \"el\", \"de\", \"particle\", \"que\", \"genre\", \"gas\", \"en\", \"row\", \"vote\", \"txt\", \"news\", \"audio\", \"csv\", \"data\", \"california\", \"feature\", \"python\", \"content\", \"http\", \"context\", \"com\", \"music\", \"index\", \"dataset\", \"column\", \"university\", \"texas\", \"florida\", \"carolina\", \"michigan\", \"colorado\", \"tennessee\", \"illinois\", \"massachusetts\", \"georgia\", \"oklahoma\", \"alabama\", \"virginia\", \"louis\", \"missouri\", \"louisiana\", \"san\", \"mississippi\", \"pennsylvania\", \"maryland\", \"fbi\", \"angeles\", \"dakota\", \"nebraska\", \"msa\", \"indiana\", \"polytechnic\", \"southern\", \"kansa\", \"iowa\", \"college\", \"st\", \"california\", \"institute\", \"state\", \"north\", \"boston\", \"francisco\", \"shooting\", \"washington\", \"chicago\", \"technology\", \"south\", \"new\", \"stanford\", \"international\", \"school\", \"data\", \"community\", \"cell\", \"mutation\", \"wolberg\", \"dry\", \"mangasarian\", \"geocoded\", \"somatic\", \"benign\", \"dioxide\", \"malignant\", \"particulate\", \"dna\", \"sulfur\", \"nucleus\", \"pharmacy\", \"wind_speed\", \"uniformity\", \"multisurface\", \"centimetre\", \"air_area_name\", \"jma\", \"cytology\", \"epithelial\", \"rogue\", \"siam\", \"hyperplanes\", \"revised\", \"oxide\", \"nitrogen\", \"metastasis\", \"breast\", \"carbon\", \"instance\", \"cancer\", \"protein\", \"diagnosis\", \"genome\", \"divide\", \"group\", \"abnormal\", \"attribute\", \"within\", \"linear\", \"academy\", \"body\", \"number\", \"size\", \"pp\", \"sample\", \"removed\", \"method\", \"point\", \"accuracy\", \"data\", \"time\", \"set\", \"single\", \"human\", \"suny\", \"southeastern\", \"denver\", \"xavier\", \"binghamton\", \"hitter\", \"ru\", \"columbus\", \"shore\", \"dayton\", \"sgm\", \"tsca\", \"liberty\", \"enru\", \"ltd\", \"newstest\", \"summoner\", \"ucdp\", \"upc\", \"cheltenham\", \"wetland\", \"membrane\", \"tokenizer\", \"passport\", \"perl\", \"nakai\", \"inversion\", \"plc\", \"dairy\", \"comic\", \"ref\", \"marvel\", \"philadelphia\", \"chocolate\", \"scripps\", \"ride\", \"message\", \"facebook\", \"en\", \"flavor\", \"champion\", \"google\"], \"Freq\": [1459.0, 4267.0, 846.0, 662.0, 1178.0, 365.0, 573.0, 6113.0, 647.0, 1200.0, 1628.0, 309.0, 362.0, 463.0, 674.0, 682.0, 806.0, 545.0, 1735.0, 477.0, 1512.0, 231.0, 206.0, 291.0, 1108.0, 680.0, 215.0, 1042.0, 581.0, 1323.0, 153.99124145507812, 155.02137756347656, 135.56675720214844, 124.24011993408203, 116.91438293457031, 114.6977767944336, 106.92391967773438, 102.48291015625, 61.75761413574219, 53.41851806640625, 54.49985885620117, 57.74996566772461, 53.410972595214844, 56.77430725097656, 125.47061157226562, 48.273494720458984, 45.86530685424805, 44.74583435058594, 43.86882781982422, 55.26169204711914, 42.09210205078125, 40.981781005859375, 40.53582000732422, 40.9517707824707, 39.424251556396484, 38.536155700683594, 38.319496154785156, 38.48047637939453, 36.51422119140625, 33.878448486328125, 119.80138397216797, 137.11378479003906, 549.0362548828125, 224.33795166015625, 114.41565704345703, 60.4371337890625, 57.706478118896484, 240.80946350097656, 185.81126403808594, 137.07345581054688, 238.2506561279297, 573.6337280273438, 231.25257873535156, 148.14300537109375, 441.3119201660156, 259.42657470703125, 140.34136962890625, 475.10009765625, 434.27349853515625, 186.14166259765625, 214.2759246826172, 145.2142791748047, 224.349365234375, 907.5171508789062, 310.9204406738281, 691.0910034179688, 939.361328125, 1638.1160888671875, 409.7734375, 281.4844970703125, 205.07643127441406, 380.3780212402344, 231.9803009033203, 452.9082946777344, 191.57936096191406, 476.6981506347656, 510.1188659667969, 1068.5595703125, 384.35711669921875, 336.6439208984375, 302.4089660644531, 413.0093688964844, 298.83331298828125, 276.8565673828125, 337.05609130859375, 256.33673095703125, 323.6031188964844, 307.2595520019531, 264.3218688964844, 274.503173828125, 270.48199462890625, 237.8481903076172, 163.44406127929688, 154.0711212158203, 110.3416976928711, 103.50092315673828, 71.15231323242188, 67.16526794433594, 66.61880493164062, 216.60536193847656, 72.24114227294922, 51.91884231567383, 51.366024017333984, 46.77389144897461, 42.545997619628906, 41.075679779052734, 40.52959060668945, 39.60595703125, 39.059654235839844, 37.50718307495117, 35.01386260986328, 33.54301452636719, 33.36088180541992, 32.626773834228516, 32.625465393066406, 31.708118438720703, 29.868806838989258, 39.41747283935547, 36.678688049316406, 30.975276947021484, 26.74672508239746, 110.01789093017578, 71.8425064086914, 85.89578247070312, 92.74482727050781, 103.95288848876953, 59.82048416137695, 225.18572998046875, 81.1455078125, 102.45188903808594, 95.0748291015625, 304.89068603515625, 86.77364349365234, 392.9820556640625, 63.44671630859375, 74.51123809814453, 97.67699432373047, 88.28160095214844, 99.69398498535156, 102.39566040039062, 88.28417205810547, 1717.260009765625, 235.70950317382812, 280.9215393066406, 581.996826171875, 124.75879669189453, 403.7078552246094, 262.2803955078125, 163.6864776611328, 157.26234436035156, 238.24464416503906, 280.3870544433594, 248.10313415527344, 215.26669311523438, 321.7711486816406, 223.25672912597656, 141.70701599121094, 509.2434387207031, 125.34740447998047, 267.41021728515625, 232.87025451660156, 235.14048767089844, 275.3896789550781, 164.22802734375, 224.51730346679688, 185.77285766601562, 176.13333129882812, 179.58026123046875, 157.53543090820312, 98.04242706298828, 101.71483612060547, 93.99712371826172, 89.45170593261719, 81.37334442138672, 81.21187591552734, 78.29525756835938, 72.94605255126953, 69.76222229003906, 100.17276000976562, 58.15114212036133, 63.44298553466797, 66.07754516601562, 53.594974517822266, 49.236534118652344, 48.899253845214844, 47.88650131225586, 47.886470794677734, 47.38581466674805, 41.832706451416016, 41.32674026489258, 41.160377502441406, 40.82096481323242, 36.94860076904297, 36.1086311340332, 35.602088928222656, 37.414642333984375, 35.266197204589844, 34.928985595703125, 34.762229919433594, 291.180419921875, 90.78902435302734, 71.5721664428711, 516.0159301757812, 151.10621643066406, 133.41546630859375, 84.8547134399414, 300.6949462890625, 108.86810302734375, 95.99909973144531, 71.18110656738281, 216.4176788330078, 208.55577087402344, 379.48577880859375, 223.30812072753906, 86.67147827148438, 221.54383850097656, 166.70274353027344, 310.689208984375, 140.90457153320312, 123.43364715576172, 203.7359161376953, 499.2966003417969, 398.648193359375, 1274.690673828125, 214.36492919921875, 225.43479919433594, 190.73924255371094, 134.7056427001953, 334.10528564453125, 517.0664672851562, 183.52078247070312, 304.12835693359375, 158.3538360595703, 273.6469421386719, 290.0157470703125, 252.10562133789062, 257.14794921875, 213.07774353027344, 196.9048614501953, 195.91749572753906, 198.3325653076172, 185.40667724609375, 123.10856628417969, 91.57445526123047, 82.09891510009766, 135.94692993164062, 97.68170166015625, 54.70999526977539, 54.11678695678711, 42.126556396484375, 75.11905670166016, 34.13430404663086, 33.68878936767578, 32.50432586669922, 32.502525329589844, 32.21089172363281, 31.61171531677246, 29.98872184753418, 27.45252799987793, 28.178255081176758, 25.696426391601562, 25.696237564086914, 24.658763885498047, 24.658750534057617, 23.474197387695312, 23.327404022216797, 22.289875030517578, 22.28986358642578, 22.143146514892578, 31.595630645751953, 21.697656631469727, 23.210371017456055, 42.4622917175293, 28.54146385192871, 76.21150970458984, 122.49668884277344, 108.43264770507812, 90.27613067626953, 46.006797790527344, 167.74383544921875, 204.0020294189453, 72.71582794189453, 190.60374450683594, 148.44493103027344, 145.41665649414062, 248.3394775390625, 161.33522033691406, 200.697509765625, 133.42398071289062, 148.74142456054688, 56.310733795166016, 90.11168670654297, 86.0739974975586, 134.7604522705078, 116.02941131591797, 155.1047821044922, 103.87168884277344, 1334.98388671875, 256.0650329589844, 501.9651794433594, 272.67864990234375, 211.3185577392578, 192.70004272460938, 437.97698974609375, 371.51678466796875, 337.444580078125, 170.72189331054688, 157.7887725830078, 140.9683380126953, 253.17283630371094, 272.571044921875, 431.6283874511719, 230.8533935546875, 193.5967559814453, 212.62936401367188, 206.39552307128906, 231.64720153808594, 215.4329376220703, 167.3411407470703, 183.65029907226562, 165.99380493164062, 166.7871551513672, 164.34515380859375, 38.789302825927734, 35.844886779785156, 29.534011840820312, 38.223934173583984, 27.011629104614258, 23.92320442199707, 23.083406448364258, 27.011629104614258, 15.929068565368652, 15.789966583251953, 14.806659698486328, 13.123661994934082, 13.119153022766113, 12.984641075134277, 12.984638214111328, 12.562580108642578, 12.423576354980469, 12.001411437988281, 12.001032829284668, 11.440414428710938, 11.440386772155762, 11.4403076171875, 10.879343032836914, 10.879343032836914, 10.879338264465332, 10.879332542419434, 10.879322052001953, 10.875874519348145, 10.740297317504883, 10.31826400756836, 10.31826400756836, 306.0013427734375, 18.842071533203125, 82.21833038330078, 104.59517669677734, 15.840094566345215, 36.76687240600586, 17.533620834350586, 14.474308013916016, 29.821388244628906, 48.28156280517578, 76.81421661376953, 68.47940826416016, 282.1854553222656, 72.64388275146484, 95.7447280883789, 27.278154373168945, 25.82781410217285, 52.69590759277344, 72.70732116699219, 22.182353973388672, 63.85084533691406, 43.42030715942383, 31.32319450378418, 26.793764114379883, 27.754844665527344, 82.551025390625, 105.85963439941406, 33.55744171142578, 67.6501235961914, 91.43224334716797, 97.8176040649414, 94.19131469726562, 140.70689392089844, 119.44208526611328, 80.62577056884766, 70.041259765625, 44.813438415527344, 72.58090209960938, 61.526206970214844, 65.117919921875, 46.23979949951172, 56.45585250854492, 54.16407775878906, 53.15752029418945, 51.77975845336914, 48.386192321777344, 104.85574340820312, 65.66899871826172, 44.3785400390625, 34.957740783691406, 32.170284271240234, 27.748043060302734, 27.398046493530273, 25.536935806274414, 24.956361770629883, 23.44524383544922, 20.303621292114258, 20.745912551879883, 19.722990036010742, 76.35000610351562, 19.3730525970459, 17.511962890625, 17.511962890625, 17.511959075927734, 17.04668617248535, 17.04668617248535, 16.58139991760254, 16.58139991760254, 16.116132736206055, 15.884559631347656, 15.070293426513672, 14.955032348632812, 14.139725685119629, 13.67447566986084, 12.628654479980469, 12.278647422790527, 26.57124900817871, 35.61488342285156, 119.27149963378906, 65.17821502685547, 93.2728271484375, 33.02497863769531, 31.880701065063477, 63.197021484375, 110.25267791748047, 151.06968688964844, 51.802085876464844, 98.57413482666016, 136.8144073486328, 73.6916732788086, 75.67891693115234, 46.13336944580078, 65.37918853759766, 34.4476318359375, 45.18836212158203, 90.59754943847656, 54.39582443237305, 82.00628662109375, 114.75077819824219, 73.39102935791016, 43.508827209472656, 73.1946792602539, 147.4917449951172, 102.12100219726562, 82.89762115478516, 61.346595764160156, 52.69129943847656, 61.838623046875, 48.364749908447266, 46.132049560546875, 47.04312515258789, 68.16117858886719, 63.44538116455078, 35.452728271484375, 35.30152130126953, 30.58561897277832, 23.36571502685547, 22.48424530029297, 17.768369674682617, 24.004018783569336, 95.34680938720703, 14.231439590454102, 13.787994384765625, 13.198538780212402, 12.609024047851562, 12.019570350646973, 12.019556999206543, 12.01955509185791, 12.019543647766113, 11.43008041381836, 11.430068016052246, 10.840599060058594, 10.840444564819336, 10.694561004638672, 10.251111030578613, 10.251111030578613, 10.251111030578613, 10.251111030578613, 10.251111030578613, 10.251111030578613, 10.251111030578613, 10.251111030578613, 25.011796951293945, 54.72142791748047, 22.43981170654297, 95.0242919921875, 68.93474578857422, 11.282838821411133, 34.49040603637695, 68.25672912597656, 42.640716552734375, 102.86490631103516, 23.936595916748047, 46.84210968017578, 16.646709442138672, 23.341569900512695, 63.97698974609375, 41.20086669921875, 20.613718032836914, 25.568899154663086, 45.54829406738281, 21.4567813873291, 25.870237350463867, 68.69367980957031, 60.34962463378906, 29.517871856689453, 28.757463455200195, 28.651996612548828, 29.051755905151367, 26.33681297302246, 26.645122528076172, 24.255659103393555, 74.95649719238281, 64.44597625732422, 53.793113708496094, 40.884124755859375, 36.65242004394531, 31.220510482788086, 29.24503517150879, 26.0703067779541, 23.46101188659668, 21.485198974609375, 20.356706619262695, 19.51007080078125, 19.50940704345703, 18.311185836791992, 16.61695098876953, 16.053632736206055, 15.207014083862305, 14.642586708068848, 14.642586708068848, 14.642206192016602, 14.642206192016602, 14.008235931396484, 14.008228302001953, 14.008219718933105, 12.597208023071289, 26.81561851501465, 11.468361854553223, 11.46835994720459, 11.467923164367676, 10.62173080444336, 30.04523277282715, 68.90507507324219, 16.000028610229492, 20.565563201904297, 24.558937072753906, 133.24209594726562, 45.26801681518555, 21.200735092163086, 39.39386749267578, 39.14958572387695, 73.69828033447266, 39.0264778137207, 147.98863220214844, 25.71381378173828, 83.20521545410156, 56.248905181884766, 51.146751403808594, 37.8648681640625, 80.08644104003906, 74.14863586425781, 77.64387512207031, 72.99764251708984, 37.808902740478516, 138.81686401367188, 70.3797378540039, 38.97758865356445, 50.8121337890625, 53.39618682861328, 63.263587951660156, 44.04985046386719, 49.33645248413086, 50.57107925415039, 50.22203063964844, 43.47650146484375, 44.020145416259766, 42.583587646484375, 41.44660568237305, 40.974544525146484, 170.181396484375, 64.58274841308594, 51.097801208496094, 50.892173767089844, 43.110374450683594, 43.00112533569336, 39.37553405761719, 37.81844711303711, 34.81075668334961, 33.56583023071289, 32.8423957824707, 299.9205017089844, 21.843599319458008, 19.768695831298828, 18.52373504638672, 19.729665756225586, 17.693798065185547, 17.693798065185547, 17.693798065185547, 17.693798065185547, 17.693798065185547, 17.07319450378418, 15.20055866241455, 14.58301067352295, 18.108091354370117, 18.93873405456543, 13.41785717010498, 80.2302017211914, 33.44032287597656, 9.706365585327148, 189.08058166503906, 49.80247497558594, 69.32889556884766, 24.92642593383789, 310.9383850097656, 67.0806884765625, 122.4246826171875, 71.1371078491211, 36.195594787597656, 34.36393737792969, 37.87797164916992, 66.1442642211914, 107.4546127319336, 73.22169494628906, 150.62844848632812, 35.071231842041016, 76.86591339111328, 47.73611831665039, 54.6246223449707, 98.95751953125, 51.136993408203125, 94.32988739013672, 173.0206298828125, 70.95030212402344, 48.62477493286133, 49.65046691894531, 81.85145568847656, 49.516788482666016, 57.61899185180664, 55.28036117553711, 53.51862335205078, 364.33349609375, 47.197784423828125, 31.872764587402344, 28.177125930786133, 26.373973846435547, 26.082365036010742, 19.703960418701172, 217.4815673828125, 14.294528007507324, 14.294528007507324, 13.933895111083984, 12.852009773254395, 12.851987838745117, 12.8519868850708, 12.851882934570312, 12.491381645202637, 12.491379737854004, 12.491381645202637, 12.491381645202637, 12.491372108459473, 12.491232872009277, 12.491232872009277, 10.870087623596191, 10.238259315490723, 10.238241195678711, 11.940338134765625, 8.79574203491211, 12.461893081665039, 8.345759391784668, 8.074483871459961, 63.239891052246094, 389.51861572265625, 18.055185317993164, 52.3431396484375, 31.717267990112305, 53.974876403808594, 32.97709655761719, 48.4310417175293, 531.2797241210938, 43.55105209350586, 23.638111114501953, 44.44062042236328, 24.314451217651367, 46.048885345458984, 81.30085754394531, 47.579036712646484, 18.13700294494629, 37.0267448425293, 33.15314483642578, 90.65785217285156, 80.0508041381836, 48.933082580566406, 38.14372634887695, 34.711605072021484, 53.163719177246094, 47.60818862915039, 35.89657211303711, 40.80343246459961, 46.255985260009766, 38.21338653564453, 36.66168212890625, 73.22965240478516, 51.25392150878906, 41.41689682006836, 29.639183044433594, 20.734834671020508, 20.363811492919922, 20.363811492919922, 19.25072479248047, 16.83690643310547, 16.56162452697754, 16.00678825378418, 25.616437911987305, 13.593464851379395, 13.22243881225586, 13.222405433654785, 12.851420402526855, 12.851409912109375, 12.851407051086426, 12.851405143737793, 12.851405143737793, 12.017451286315918, 10.904374122619629, 10.9043607711792, 8.307169914245605, 7.936189651489258, 7.9361891746521, 7.936187267303467, 7.936184883117676, 7.935242176055908, 13.702224731445312, 17.906362533569336, 57.96103286743164, 36.85788345336914, 26.393451690673828, 15.56857681274414, 85.10565948486328, 33.994693756103516, 47.092620849609375, 56.151023864746094, 22.74015998840332, 70.82398223876953, 49.335426330566406, 27.03633689880371, 27.136295318603516, 17.190889358520508, 41.059967041015625, 34.712867736816406, 82.3804702758789, 37.068603515625, 84.4324722290039, 38.7470703125, 33.30709457397461, 26.117008209228516, 34.22981643676758, 32.29880142211914, 30.248783111572266, 34.46152877807617, 25.198034286499023, 30.259599685668945, 29.414649963378906, 29.155942916870117, 26.879974365234375, 50.26862716674805, 30.13310432434082, 22.77376365661621, 22.630687713623047, 19.52659797668457, 19.926918029785156, 19.926918029785156, 18.73189926147461, 18.516944885253906, 20.247028350830078, 14.979023933410645, 14.979023933410645, 14.958202362060547, 14.958202362060547, 14.958202362060547, 14.958202362060547, 14.958202362060547, 101.3534927368164, 11.73187255859375, 10.865781784057617, 10.86574649810791, 36.68037033081055, 10.288403511047363, 10.288403511047363, 10.288381576538086, 10.288393020629883, 9.99970817565918, 9.99970531463623, 9.99970531463623, 9.999703407287598, 9.999703407287598, 26.677623748779297, 53.74358367919922, 47.2074089050293, 30.376428604125977, 109.63308715820312, 22.350616455078125, 18.85586929321289, 37.08588790893555, 25.88086700439453, 34.12471008300781, 64.2203598022461, 33.58482360839844, 33.940162658691406, 42.66565704345703, 28.007736206054688, 56.67877960205078, 81.58682250976562, 25.912160873413086, 35.85636520385742, 28.500486373901367, 45.19415283203125, 40.27861785888672, 40.930274963378906, 34.703182220458984, 25.469467163085938, 26.170011520385742, 33.20024108886719, 25.708263397216797, 1454.2861328125, 74.68408203125, 38.536746978759766, 32.03063201904297, 31.8856258392334, 31.11439323425293, 23.884801864624023, 22.101749420166016, 22.101743698120117, 21.71612548828125, 21.523296356201172, 21.330488204956055, 19.740243911743164, 18.968952178955078, 17.426502227783203, 17.426490783691406, 32.472572326660156, 17.04088020324707, 15.257784843444824, 14.486557960510254, 14.245911598205566, 14.053163528442383, 13.908123970031738, 13.908122062683105, 13.860346794128418, 13.715312957763672, 13.52234935760498, 30.178144454956055, 11.160992622375488, 11.160987854003906, 178.70639038085938, 28.118009567260742, 86.44092559814453, 59.09769821166992, 275.2195129394531, 42.54793167114258, 17.53474235534668, 16.90513801574707, 21.48122215270996, 30.208974838256836, 22.261497497558594, 37.280147552490234, 24.720848083496094, 41.401065826416016, 21.24131202697754, 23.085147857666016, 23.202072143554688, 25.8385009765625, 21.703262329101562, 123.38912200927734, 30.733055114746094, 26.66983985900879, 24.547222137451172, 22.355836868286133, 20.54220199584961, 17.291458129882812, 16.979053497314453, 16.417144775390625, 15.478614807128906, 14.228128433227539, 13.410158157348633, 11.915059089660645, 10.414383888244629, 9.0398588180542, 8.913671493530273, 8.913671493530273, 8.913666725158691, 8.912922859191895, 8.66354751586914, 8.66354751586914, 8.66352653503418, 8.66352653503418, 8.66352653503418, 8.66352653503418, 8.66352653503418, 11.83061408996582, 6.3505096435546875, 6.1003923416137695, 5.850163459777832, 18.808815002441406, 13.917157173156738, 118.87039184570312, 35.8702507019043, 19.621685028076172, 19.704471588134766, 23.714649200439453, 14.147941589355469, 55.2735710144043, 14.473235130310059, 45.44161605834961, 32.37929153442383, 16.338279724121094, 10.734089851379395, 21.933687210083008, 47.3009033203125, 25.11117172241211, 16.710100173950195, 24.61568832397461, 19.385108947753906, 19.108007431030273, 19.84402847290039, 18.075037002563477, 26.66971206665039, 20.4807071685791, 18.765243530273438, 17.053136825561523, 16.85023307800293, 30.829450607299805, 13.630635261535645, 13.630324363708496, 10.26267147064209, 9.014798164367676, 9.012321472167969, 8.38353157043457, 5.1573967933654785, 4.904819011688232, 4.904609680175781, 4.083286762237549, 4.0832839012146, 4.854011058807373, 3.3244166374206543, 7.114745616912842, 2.8185033798217773, 2.5655276775360107, 2.312589645385742, 2.059633493423462, 2.0596330165863037, 2.0596330165863037, 2.0596330165863037, 2.0596327781677246, 2.0596327781677246, 3.0714595317840576, 1.806675910949707, 1.806675910949707, 1.8066762685775757, 1.8066755533218384, 4.463228225708008, 2.818286895751953, 3.869659423828125, 6.637660503387451, 4.053242206573486, 4.331485271453857, 5.734888076782227, 6.583508014678955, 4.103690147399902, 4.872434616088867, 2.690660238265991, 2.4491684436798096, 2.6819934844970703], \"Total\": [1459.0, 4267.0, 846.0, 662.0, 1178.0, 365.0, 573.0, 6113.0, 647.0, 1200.0, 1628.0, 309.0, 362.0, 463.0, 674.0, 682.0, 806.0, 545.0, 1735.0, 477.0, 1512.0, 231.0, 206.0, 291.0, 1108.0, 680.0, 215.0, 1042.0, 581.0, 1323.0, 154.84890747070312, 155.9417266845703, 136.42449951171875, 125.09777069091797, 117.77202606201172, 115.55546569824219, 107.7815933227539, 103.3405532836914, 62.678977966308594, 54.276607513427734, 55.37587356567383, 58.68220901489258, 54.27479553222656, 57.76335906982422, 127.72299194335938, 49.1483039855957, 46.722965240478516, 45.61080551147461, 44.72648239135742, 56.36869812011719, 42.9499397277832, 41.84099197387695, 41.393558502197266, 41.83322525024414, 40.284488677978516, 39.39609146118164, 39.177162170410156, 39.36286544799805, 37.39118957519531, 34.73609924316406, 123.43016052246094, 141.68321228027344, 581.0328369140625, 234.7464599609375, 120.3836898803711, 62.38060760498047, 59.55247116088867, 261.433349609375, 200.72518920898438, 147.1993865966797, 266.6758117675781, 682.3300170898438, 263.2197265625, 167.7272491455078, 563.8082275390625, 321.72265625, 161.43365478515625, 647.8678588867188, 585.6103515625, 226.09161376953125, 265.8894958496094, 170.56298828125, 289.6276550292969, 1618.9808349609375, 436.3997802734375, 1323.744873046875, 2000.45849609375, 4267.4521484375, 674.0712280273438, 436.6680603027344, 272.2532653808594, 708.1542358398438, 334.8309631347656, 1000.898681640625, 252.58226013183594, 1178.51220703125, 1340.6446533203125, 6113.09375, 935.2544555664062, 891.425537109375, 701.1490478515625, 1735.5450439453125, 782.2594604492188, 635.0862426757812, 1371.074462890625, 534.57763671875, 1628.1893310546875, 1512.58544921875, 776.1353149414062, 1302.9066162109375, 1200.6051025390625, 238.72764587402344, 164.32351684570312, 154.95057678222656, 111.2211685180664, 104.3933334350586, 72.07642364501953, 68.04471588134766, 67.49846649169922, 219.95050048828125, 73.36890411376953, 52.79832077026367, 52.24546813964844, 47.65337371826172, 43.42544174194336, 41.95527648925781, 41.40913772583008, 40.48541259765625, 39.939144134521484, 38.41911697387695, 35.893306732177734, 34.42290496826172, 34.24089813232422, 33.5062141418457, 33.50595474243164, 32.5887451171875, 30.74828338623047, 40.65966033935547, 37.873146057128906, 31.993267059326172, 27.626184463500977, 117.44307708740234, 76.20310974121094, 92.6778564453125, 100.83526611328125, 113.94176483154297, 63.772308349609375, 269.8227844238281, 89.29828643798828, 116.4943618774414, 107.3697738647461, 392.6343994140625, 97.49543762207031, 561.6754150390625, 69.87411499023438, 84.37457275390625, 117.99998474121094, 105.16445922851562, 124.9361801147461, 129.319580078125, 107.01923370361328, 6113.09375, 417.72674560546875, 537.6934204101562, 1628.1893310546875, 175.874755859375, 1042.1571044921875, 565.680419921875, 272.7481689453125, 259.0557861328125, 533.352294921875, 782.2594604492188, 635.0862426757812, 572.0523071289062, 1302.9066162109375, 674.6853637695312, 258.6190185546875, 4267.4521484375, 203.18218994140625, 1340.6446533203125, 1108.2618408203125, 1200.6051025390625, 2000.45849609375, 500.5242919921875, 1735.5450439453125, 1371.074462890625, 1174.0919189453125, 1618.9808349609375, 662.7713012695312, 98.93079376220703, 102.63896179199219, 94.88555145263672, 90.33995819091797, 82.26160430908203, 82.10086059570312, 79.19832611083984, 73.85113525390625, 70.65052032470703, 101.4632797241211, 59.03944396972656, 64.41323852539062, 67.10658264160156, 54.487998962402344, 50.126888275146484, 49.78759002685547, 48.77473831176758, 48.77473831176758, 48.27440643310547, 42.72210693359375, 42.215572357177734, 42.04882049560547, 41.70922088623047, 37.83684158325195, 36.99687194824219, 36.49037551879883, 38.35615158081055, 36.15655517578125, 35.81725311279297, 35.650474548339844, 301.1741027832031, 93.22233581542969, 73.54155731201172, 545.0906372070312, 157.84181213378906, 139.85662841796875, 88.20884704589844, 322.1492614746094, 115.18043518066406, 101.1343994140625, 74.54911804199219, 245.81011962890625, 240.94740295410156, 497.9180908203125, 283.59930419921875, 95.65888214111328, 298.6643371582031, 213.4910888671875, 463.3663635253906, 182.3734893798828, 153.43954467773438, 307.93927001953125, 1108.2618408203125, 846.0040893554688, 6113.09375, 392.134033203125, 446.0038146972656, 339.6357727050781, 193.80079650878906, 1340.6446533203125, 4267.4521484375, 444.3396911621094, 1512.58544921875, 326.5456237792969, 1371.074462890625, 1735.5450439453125, 1200.6051025390625, 2000.45849609375, 1178.51220703125, 1174.0919189453125, 1302.9066162109375, 1618.9808349609375, 1628.1893310546875, 124.0088119506836, 92.47419738769531, 82.9986572265625, 137.92298889160156, 99.192138671875, 55.609745025634766, 55.01696014404297, 43.026344299316406, 76.93687438964844, 35.034053802490234, 34.58854293823242, 33.404090881347656, 33.403968811035156, 33.11067581176758, 32.514793395996094, 30.88850212097168, 28.362119674682617, 29.116466522216797, 26.596233367919922, 26.59634780883789, 25.558513641357422, 25.558507919311523, 24.374082565307617, 24.227439880371094, 23.18962287902832, 23.189619064331055, 23.042903900146484, 32.88136291503906, 22.5974063873291, 24.201976776123047, 44.5378532409668, 29.835430145263672, 81.82231903076172, 133.71417236328125, 121.35343933105469, 106.13481140136719, 50.75272750854492, 215.26499938964844, 273.80126953125, 86.15614318847656, 261.5752258300781, 198.8507537841797, 197.28396606445312, 375.135986328125, 232.23631286621094, 310.8985290527344, 189.2860565185547, 227.2798309326172, 66.61954498291016, 121.0234146118164, 115.58920288085938, 208.21205139160156, 171.67791748046875, 252.6422882080078, 149.11294555664062, 6113.09375, 593.3598022460938, 1735.5450439453125, 680.2706909179688, 457.9884338378906, 404.4003601074219, 1512.58544921875, 1371.074462890625, 1174.0919189453125, 373.0814208984375, 330.6687316894531, 263.6903076171875, 1000.898681640625, 1302.9066162109375, 4267.4521484375, 946.5140380859375, 595.0638427734375, 806.0997924804688, 796.21044921875, 2000.45849609375, 1628.1893310546875, 535.6504516601562, 1042.1571044921875, 572.0523071289062, 935.2544555664062, 1618.9808349609375, 39.69348907470703, 36.74908447265625, 30.438194274902344, 39.425689697265625, 27.915836334228516, 24.827383041381836, 23.988311767578125, 28.276466369628906, 16.8332462310791, 16.694252014160156, 15.710986137390137, 14.027848243713379, 14.030475616455078, 13.888840675354004, 13.888845443725586, 13.466765403747559, 13.327762603759766, 12.905702590942383, 12.905708312988281, 12.344600677490234, 12.344615936279297, 12.344670295715332, 11.783522605895996, 11.783522605895996, 11.783527374267578, 11.783525466918945, 11.783531188964844, 11.781248092651367, 11.644536972045898, 11.222443580627441, 11.222443580627441, 362.5064697265625, 21.569581985473633, 120.23733520507812, 164.5251922607422, 18.330825805664062, 51.0908088684082, 21.226905822753906, 16.767276763916016, 44.178226470947266, 91.197021484375, 174.94100952148438, 154.24954223632812, 1178.51220703125, 177.2466583251953, 285.00823974609375, 43.14680862426758, 39.95882797241211, 121.08657836914062, 204.29539489746094, 31.974693298339844, 220.71456909179688, 113.53337097167969, 69.3421401977539, 50.72006607055664, 54.78533935546875, 674.6853637695312, 1512.58544921875, 88.9171371459961, 525.7612915039062, 1174.0919189453125, 1628.1893310546875, 1735.5450439453125, 6113.09375, 4267.4521484375, 1371.074462890625, 946.5140380859375, 197.80177307128906, 1323.744873046875, 796.21044921875, 2000.45849609375, 260.55523681640625, 1302.9066162109375, 1340.6446533203125, 1108.2618408203125, 1200.6051025390625, 585.6103515625, 105.77413177490234, 66.58905792236328, 45.296356201171875, 35.875545501708984, 33.08808898925781, 28.665868759155273, 28.31585121154785, 26.454740524291992, 25.87421226501465, 24.363136291503906, 21.2214298248291, 21.70760726928711, 20.640869140625, 79.95760345458984, 20.290882110595703, 18.429767608642578, 18.429767608642578, 18.429765701293945, 17.96449089050293, 17.96449089050293, 17.499216079711914, 17.49921417236328, 17.033939361572266, 16.802955627441406, 15.988119125366211, 15.872845649719238, 15.057575225830078, 14.59228515625, 13.546462059020996, 13.196454048156738, 28.627307891845703, 39.12062454223633, 140.9671173095703, 77.96378326416016, 119.220947265625, 38.05378341674805, 36.720428466796875, 79.98995208740234, 162.07675170898438, 236.91217041015625, 67.39368438720703, 146.37677001953125, 223.1356201171875, 116.50249481201172, 122.31458282470703, 69.1811752319336, 116.02665710449219, 48.94483947753906, 83.55480194091797, 292.98809814453125, 169.66610717773438, 497.29266357421875, 1200.6051025390625, 392.134033203125, 101.5908203125, 517.3340454101562, 6113.09375, 2000.45849609375, 1108.2618408203125, 525.7612915039062, 257.1114196777344, 946.5140380859375, 273.86480712890625, 171.3355255126953, 701.1490478515625, 69.06143188476562, 64.34556579589844, 36.35298156738281, 36.20166778564453, 31.485790252685547, 24.26595115661621, 23.384387969970703, 18.66851043701172, 25.278858184814453, 100.63868713378906, 15.13160228729248, 14.68814754486084, 14.098679542541504, 13.509180068969727, 12.919710159301758, 12.919707298278809, 12.919709205627441, 12.919700622558594, 12.330224990844727, 12.330220222473145, 11.740739822387695, 11.740636825561523, 11.594700813293457, 11.15125560760498, 11.15125560760498, 11.15125560760498, 11.15125560760498, 11.15125560760498, 11.15125560760498, 11.15125560760498, 11.15125560760498, 27.811918258666992, 72.272216796875, 27.257404327392578, 181.3583984375, 187.54806518554688, 12.919485092163086, 80.36156463623047, 298.6643371582031, 173.35183715820312, 1042.1571044921875, 61.36866760253906, 260.55523681640625, 29.831798553466797, 67.92866516113281, 846.0040893554688, 284.2540588378906, 51.7604866027832, 92.24991607666016, 433.0292663574219, 62.08263397216797, 138.06776428222656, 6113.09375, 4267.4521484375, 535.6504516601562, 776.1353149414062, 796.21044921875, 1302.9066162109375, 421.8255920410156, 635.0862426757812, 528.694091796875, 75.9041519165039, 65.39041137695312, 54.74042892456055, 41.83173370361328, 37.59626388549805, 32.16435241699219, 30.18891143798828, 27.014923095703125, 24.404870986938477, 22.430009841918945, 21.300554275512695, 20.45392608642578, 20.454660415649414, 19.25526237487793, 17.563011169433594, 16.997488021850586, 16.150859832763672, 15.586446762084961, 15.586446762084961, 15.586869239807129, 15.586869239807129, 14.952128410339355, 14.952119827270508, 14.9521484375, 13.541055679321289, 28.914886474609375, 12.41221809387207, 12.41222095489502, 12.41313648223877, 11.565594673156738, 32.722999572753906, 75.9979476928711, 17.74493408203125, 23.293180465698242, 29.152917861938477, 211.2467803955078, 65.60975646972656, 25.290546417236328, 72.44510650634766, 72.32566833496094, 206.22938537597656, 74.88660430908203, 806.0997924804688, 40.5859375, 322.79949951171875, 174.68112182617188, 147.7112274169922, 85.45106506347656, 445.06072998046875, 391.2845153808594, 477.3929748535156, 573.8732299804688, 110.30200958251953, 4267.4521484375, 1618.9808349609375, 173.1859588623047, 891.425537109375, 1512.58544921875, 6113.09375, 489.9603271484375, 1323.744873046875, 1735.5450439453125, 2000.45849609375, 535.6504516601562, 1178.51220703125, 1371.074462890625, 572.0523071289062, 674.0712280273438, 171.10745239257812, 65.50773620605469, 52.022762298583984, 51.81715393066406, 44.035343170166016, 43.93228530883789, 40.30051803588867, 38.743404388427734, 35.735755920410156, 34.49080276489258, 33.767433166503906, 309.2442321777344, 22.7685604095459, 20.69365882873535, 19.44872283935547, 20.723703384399414, 18.618759155273438, 18.618759155273438, 18.618759155273438, 18.618759155273438, 18.618759155273438, 17.998167037963867, 16.132707595825195, 15.50839614868164, 19.315732955932617, 20.23472023010254, 14.363496780395508, 86.24560546875, 36.057579040527344, 10.631331443786621, 215.05320739746094, 56.6083984375, 82.71821594238281, 28.184572219848633, 573.8732299804688, 92.95958709716797, 207.02374267578125, 105.49586486816406, 45.453155517578125, 42.65657424926758, 51.69394302368164, 113.38794708251953, 248.4521942138672, 155.46795654296875, 477.3929748535156, 52.397247314453125, 223.70809936523438, 94.73310852050781, 133.44610595703125, 674.0712280273438, 114.49622344970703, 680.2706909179688, 4267.4521484375, 349.7036437988281, 112.89445495605469, 122.26248168945312, 1628.1893310546875, 174.68112182617188, 891.425537109375, 1323.744873046875, 776.1353149414062, 365.26702880859375, 48.13058090209961, 32.805450439453125, 29.109817504882812, 27.306676864624023, 27.17346954345703, 20.63666534423828, 231.39808654785156, 15.227221488952637, 15.227218627929688, 14.86658763885498, 13.784699440002441, 13.784692764282227, 13.784711837768555, 13.78480339050293, 13.424072265625, 13.424072265625, 13.424073219299316, 13.424072265625, 13.42408275604248, 13.424223899841309, 13.424223899841309, 11.80298137664795, 11.170950889587402, 11.170964241027832, 13.171113014221191, 9.728434562683105, 13.795933723449707, 9.278464317321777, 9.007174491882324, 70.807373046875, 662.7713012695312, 22.675277709960938, 86.9612808227539, 49.417869567871094, 109.99616241455078, 60.17700958251953, 107.76634216308594, 4267.4521484375, 112.31514739990234, 39.701847076416016, 129.08535766601562, 46.61172103881836, 159.51560974121094, 551.4049072265625, 198.56802368164062, 30.034616470336914, 154.46385192871094, 124.54756164550781, 1628.1893310546875, 1200.6051025390625, 682.3300170898438, 322.79949951171875, 289.6731262207031, 1735.5450439453125, 1512.58544921875, 463.3663635253906, 1323.744873046875, 6113.09375, 1302.9066162109375, 1340.6446533203125, 74.16091918945312, 52.18538284301758, 42.34811019897461, 30.57074737548828, 21.666048049926758, 21.295024871826172, 21.295024871826172, 20.181962966918945, 17.775121688842773, 17.49289894104004, 16.93801498413086, 27.28099822998047, 14.524679183959961, 14.153654098510742, 14.153646469116211, 13.782631874084473, 13.782637596130371, 13.782641410827637, 13.78264331817627, 13.78264331817627, 12.948664665222168, 11.835599899291992, 11.835602760314941, 9.238422393798828, 8.867403030395508, 8.867403030395508, 8.867405891418457, 8.867403984069824, 8.868330955505371, 15.350846290588379, 21.785470962524414, 132.8059844970703, 73.68870544433594, 50.4119873046875, 24.521583557128906, 463.3663635253906, 104.46946716308594, 193.80079650878906, 281.4841613769531, 50.76959991455078, 647.8678588867188, 317.35528564453125, 86.2035903930664, 126.60537719726562, 32.43552780151367, 474.2106628417969, 283.662841796875, 4267.4521484375, 382.18756103515625, 6113.09375, 477.3929748535156, 329.1810302734375, 157.85751342773438, 1108.2618408203125, 776.1353149414062, 573.8732299804688, 2000.45849609375, 170.47630310058594, 1512.58544921875, 1174.0919189453125, 1323.744873046875, 391.2845153808594, 51.21156311035156, 31.078144073486328, 23.71670150756836, 23.573699951171875, 20.469547271728516, 20.897083282470703, 20.897083282470703, 19.67514419555664, 19.460386276245117, 21.442935943603516, 15.921954154968262, 15.921954154968262, 15.919175148010254, 15.919175148010254, 15.919175148010254, 15.919175148010254, 15.919175148010254, 109.26527404785156, 12.674798011779785, 11.808722496032715, 11.808723449707031, 40.02008819580078, 11.231325149536133, 11.231328010559082, 11.231318473815918, 11.231331825256348, 10.942630767822266, 10.942632675170898, 10.942632675170898, 10.942636489868164, 10.942636489868164, 30.041170120239258, 63.21991729736328, 58.50035858154297, 38.42245101928711, 165.22193908691406, 29.209125518798828, 28.120546340942383, 78.82170867919922, 45.96638870239258, 88.3923110961914, 400.50482177734375, 114.3785400390625, 120.58557891845703, 244.28369140625, 99.42578125, 1178.51220703125, 6113.09375, 113.79290771484375, 477.3929748535156, 176.91958618164062, 1735.5450439453125, 1302.9066162109375, 1512.58544921875, 796.21044921875, 121.0234146118164, 183.8238525390625, 4267.4521484375, 806.0997924804688, 1459.7286376953125, 75.64249420166016, 39.49338912963867, 32.98737335205078, 32.84221649169922, 32.07095718383789, 24.84139633178711, 23.058319091796875, 23.058324813842773, 22.672693252563477, 22.479930877685547, 22.2871150970459, 20.69681167602539, 19.925580978393555, 18.383066177368164, 18.38308334350586, 34.29405975341797, 17.99744415283203, 16.214391708374023, 15.443122863769531, 15.20272159576416, 15.009736061096191, 14.864689826965332, 14.864694595336914, 14.816926956176758, 14.67187786102295, 14.479315757751465, 32.7236442565918, 12.117559432983398, 12.117579460144043, 206.81137084960938, 31.38581657409668, 113.79290771484375, 88.0694351196289, 846.0040893554688, 73.90918731689453, 20.788311004638672, 20.2180118560791, 30.200542449951172, 56.54208755493164, 33.41188430786133, 97.91766357421875, 70.93423461914062, 572.0523071289062, 59.06689453125, 164.3592987060547, 177.2466583251953, 6113.09375, 305.0225830078125, 124.34371185302734, 31.682247161865234, 27.618276596069336, 25.495651245117188, 23.304290771484375, 21.49266242980957, 18.239952087402344, 17.927961349487305, 17.365785598754883, 16.427175521850586, 15.176546096801758, 14.378552436828613, 12.863551139831543, 11.362794876098633, 9.98827075958252, 9.862081527709961, 9.862081527709961, 9.86208438873291, 9.863534927368164, 9.611974716186523, 9.611974716186523, 9.611967086791992, 9.611967086791992, 9.611967086791992, 9.611967086791992, 9.611967086791992, 13.4390287399292, 7.2989325523376465, 7.048810005187988, 6.798882961273193, 21.937522888183594, 16.332275390625, 159.35906982421875, 45.229469299316406, 29.356632232666016, 33.27986145019531, 48.04030227661133, 23.404163360595703, 291.419677734375, 26.24773406982422, 257.1114196777344, 217.6610107421875, 43.76154708862305, 15.918707847595215, 110.30200958251953, 1200.6051025390625, 198.2703857421875, 56.39164733886719, 308.31048583984375, 119.0909652709961, 171.2551727294922, 284.2540588378906, 155.46795654296875, 6113.09375, 1628.1893310546875, 946.5140380859375, 223.06619262695312, 193.36671447753906, 31.777463912963867, 14.578703880310059, 14.57896900177002, 11.269962310791016, 9.962807655334473, 9.966131210327148, 9.331579208374023, 6.1062517166137695, 5.852835178375244, 5.853236675262451, 5.03129243850708, 5.0312957763671875, 6.1487040519714355, 4.2724223136901855, 9.222070693969727, 3.7665090560913086, 3.5135886669158936, 3.26059627532959, 3.007638931274414, 3.007638692855835, 3.007638692855835, 3.0076396465301514, 3.007639169692993, 3.0076398849487305, 4.5805463790893555, 2.754683017730713, 2.7546825408935547, 2.7546825408935547, 2.75468373298645, 6.890603542327881, 4.470322608947754, 6.502251625061035, 13.144782066345215, 7.319726943969727, 9.150419235229492, 15.849807739257812, 91.11090850830078, 30.407470703125, 88.3923110961914, 12.9481782913208, 10.249341011047363, 136.59718322753906], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.269999980926514, -6.263299942016602, -6.39739990234375, -6.484600067138672, -6.545400142669678, -6.564599990844727, -6.634699821472168, -6.677199840545654, -7.183599948883057, -7.328700065612793, -7.308700084686279, -7.250699996948242, -7.328800201416016, -7.2677998542785645, -6.474800109863281, -7.429999828338623, -7.481200218200684, -7.505899906158447, -7.525700092315674, -7.2947998046875, -7.566999912261963, -7.593699932098389, -7.604700088500977, -7.5945000648498535, -7.632500171661377, -7.655300140380859, -7.660900115966797, -7.656700134277344, -7.709199905395508, -7.78410005569458, -6.520999908447266, -6.386000156402588, -4.998700141906738, -5.893700122833252, -6.566999912261963, -7.2052998542785645, -7.251500129699707, -5.82289981842041, -6.082099914550781, -6.386300086975098, -5.833499908447266, -4.954899787902832, -5.863399982452393, -6.308700084686279, -5.217100143432617, -5.7484002113342285, -6.362800121307373, -5.1433000564575195, -5.2332000732421875, -6.080399990081787, -5.939599990844727, -6.328700065612793, -5.893700122833252, -4.496099948883057, -5.567299842834473, -4.768599987030029, -4.461699962615967, -3.905600070953369, -5.291299819946289, -5.666800022125244, -5.983500003814697, -5.365699768066406, -5.860199928283691, -5.191199779510498, -6.051599979400635, -5.139999866485596, -5.072199821472168, -4.332799911499023, -5.355299949645996, -5.487800121307373, -5.595099925994873, -5.283400058746338, -5.60699987411499, -5.6834001541137695, -5.486599922180176, -5.76039981842041, -5.527299880981445, -5.57919979095459, -5.729700088500977, -5.69189977645874, -5.706699848175049, -5.379000186920166, -5.7540998458862305, -5.813199996948242, -6.146999835968018, -6.210999965667725, -6.5858001708984375, -6.643400192260742, -6.651599884033203, -5.472499847412109, -6.5706000328063965, -6.900899887084961, -6.911600112915039, -7.005300045013428, -7.099999904632568, -7.135200023651123, -7.148600101470947, -7.171599864959717, -7.185500144958496, -7.226099967956543, -7.2947998046875, -7.337800025939941, -7.343200206756592, -7.365499973297119, -7.365499973297119, -7.394000053405762, -7.453800201416016, -7.176400184631348, -7.2484002113342285, -7.417399883270264, -7.564199924468994, -6.149899959564209, -6.576099872589111, -6.39739990234375, -6.320700168609619, -6.206600189208984, -6.759200096130371, -5.433700084686279, -6.4542999267578125, -6.221199989318848, -6.295899868011475, -5.1305999755859375, -6.38730001449585, -4.876800060272217, -6.700399875640869, -6.539599895477295, -6.268899917602539, -6.370100021362305, -6.248499870300293, -6.221700191497803, -6.369999885559082, -3.402100086212158, -5.388000011444092, -5.212500095367432, -4.484099864959717, -6.024199962615967, -4.849899768829346, -5.281199932098389, -5.752600193023682, -5.792699813842773, -5.377299785614014, -5.214399814605713, -5.336699962615967, -5.478700160980225, -5.076700210571289, -5.442299842834473, -5.8968000411987305, -4.617700099945068, -6.019499778747559, -5.2617998123168945, -5.400100231170654, -5.390399932861328, -5.232399940490723, -5.749300003051758, -5.436600208282471, -5.626100063323975, -5.679299831390381, -5.659999847412109, -5.790900230407715, -6.203999996185303, -6.167300224304199, -6.246200084686279, -6.2957000732421875, -6.390399932861328, -6.392399787902832, -6.428999900817871, -6.49970006942749, -6.544400215148926, -6.182499885559082, -6.726399898529053, -6.6392998695373535, -6.598599910736084, -6.808000087738037, -6.8927998542785645, -6.899700164794922, -6.920599937438965, -6.920599937438965, -6.931099891662598, -7.055799961090088, -7.06790018081665, -7.072000026702881, -7.0802001953125, -7.179900169372559, -7.202899932861328, -7.2170000076293945, -7.167399883270264, -7.226500034332275, -7.236100196838379, -7.240900039672852, -5.115499973297119, -6.280900001525879, -6.518700122833252, -4.543300151824951, -5.771500110626221, -5.895999908447266, -6.348499774932861, -5.0833001136779785, -6.099299907684326, -6.225100040435791, -6.524199962615967, -5.412199974060059, -5.44920015335083, -4.850599765777588, -5.380899906158447, -6.327300071716309, -5.388800144195557, -5.6732001304626465, -5.0507001876831055, -5.841400146484375, -5.973700046539307, -5.472599983215332, -4.576200008392334, -4.801400184631348, -3.6389999389648438, -5.421800136566162, -5.371399879455566, -5.53849983215332, -5.88640022277832, -4.978000164031982, -4.541299819946289, -5.577099800109863, -5.072000026702881, -5.724599838256836, -5.177599906921387, -5.119500160217285, -5.2596001625061035, -5.239799976348877, -5.427800178527832, -5.506700038909912, -5.5117998123168945, -5.499499797821045, -5.56689977645874, -5.796599864959717, -6.09250020980835, -6.2017998695373535, -5.697400093078613, -6.0279998779296875, -6.607600212097168, -6.618500232696533, -6.86899995803833, -6.290599822998047, -7.079400062561035, -7.09250020980835, -7.128300189971924, -7.128399848937988, -7.137400150299072, -7.156199932098389, -7.208899974822998, -7.2972002029418945, -7.271100044250488, -7.36329984664917, -7.36329984664917, -7.404600143432617, -7.404600143432617, -7.453800201416016, -7.460100173950195, -7.5055999755859375, -7.5055999755859375, -7.512199878692627, -7.156700134277344, -7.53249979019165, -7.465099811553955, -6.861100196838379, -7.258299827575684, -6.276199817657471, -5.801599979400635, -5.923600196838379, -6.106800079345703, -6.780900001525879, -5.487299919128418, -5.291600227355957, -6.3231000900268555, -5.359499931335449, -5.609499931335449, -5.630099773406982, -5.094900131225586, -5.526199817657471, -5.3078999519348145, -5.71619987487793, -5.607500076293945, -6.578800201416016, -6.10860013961792, -6.1545000076293945, -5.706200122833252, -5.855800151824951, -5.5655999183654785, -5.9664998054504395, -3.4130001068115234, -5.064300060272217, -4.391200065612793, -5.001399993896484, -5.25629997253418, -5.348599910736084, -4.527500152587891, -4.6921000480651855, -4.788300037384033, -5.469699859619141, -5.548399925231934, -5.661200046539307, -5.0756001472473145, -5.001800060272217, -4.542099952697754, -5.167900085449219, -5.343900203704834, -5.250100135803223, -5.279900074005127, -5.1645002365112305, -5.236999988555908, -5.489699840545654, -5.396699905395508, -5.497700214385986, -5.493000030517578, -5.507699966430664, -5.787700176239014, -5.866700172424316, -6.060299873352051, -5.8024001121521, -6.149600028991699, -6.270999908447266, -6.306700229644775, -6.149600028991699, -6.677700042724609, -6.686500072479248, -6.750800132751465, -6.871399879455566, -6.871799945831299, -6.8821001052856445, -6.8821001052856445, -6.91510009765625, -6.926300048828125, -6.9608001708984375, -6.960899829864502, -7.008699893951416, -7.008699893951416, -7.008699893951416, -7.059000015258789, -7.059000015258789, -7.059000015258789, -7.059000015258789, -7.059000015258789, -7.059299945831299, -7.071899890899658, -7.1118998527526855, -7.1118998527526855, -3.7223000526428223, -6.509799957275391, -5.036499977111816, -4.79580020904541, -6.683300018310547, -5.841300010681152, -6.581699848175049, -6.773499965667725, -6.050600051879883, -5.56879997253418, -5.104499816894531, -5.219299793243408, -3.803299903869629, -5.160299777984619, -4.884200096130371, -6.139800071716309, -6.194399833679199, -5.481299877166748, -5.15939998626709, -6.34660005569458, -5.289299964904785, -5.674900054931641, -6.001500129699707, -6.157700061798096, -6.122499942779541, -5.032400131225586, -4.783699989318848, -5.932600021362305, -5.231500148773193, -4.930300235748291, -4.862800121307373, -4.9004998207092285, -4.499199867248535, -4.663000106811523, -5.056000232696533, -5.196800231933594, -5.643400192260742, -5.161200046539307, -5.326399803161621, -5.269700050354004, -5.611999988555908, -5.412399768829346, -5.453800201416016, -5.472599983215332, -5.498899936676025, -5.5665998458862305, -4.748000144958496, -5.216000080108643, -5.607900142669678, -5.846499919891357, -5.929599761962891, -6.077499866485596, -6.090199947357178, -6.1605000495910645, -6.183499813079834, -6.245999813079834, -6.389800071716309, -6.368299961090088, -6.418799877166748, -5.065299987792969, -6.436699867248535, -6.537700176239014, -6.537700176239014, -6.537700176239014, -6.564700126647949, -6.564700126647949, -6.592400074005127, -6.592400074005127, -6.620800018310547, -6.635300159454346, -6.687900066375732, -6.6956000328063965, -6.7515997886657715, -6.785099983215332, -6.864699840545654, -6.8927998542785645, -6.120800018310547, -5.827899932861328, -4.619200229644775, -5.223499774932861, -4.865099906921387, -5.90339994430542, -5.938600063323975, -5.25439977645874, -4.69789981842041, -4.382900238037109, -5.453199863433838, -4.809800148010254, -4.48199987411499, -5.1006999015808105, -5.074100017547607, -5.5690999031066895, -5.220399856567383, -5.861199855804443, -5.589799880981445, -4.894199848175049, -5.404300212860107, -4.993800163269043, -4.657899856567383, -5.104800224304199, -5.627699851989746, -5.107500076293945, -4.406899929046631, -4.774499893188477, -4.982999801635742, -5.28410005569458, -5.436200141906738, -5.276100158691406, -5.521900177001953, -5.5690999031066895, -5.549600124359131, -5.041299819946289, -5.11299991607666, -5.695000171661377, -5.6992998123168945, -5.842700004577637, -6.1118998527526855, -6.150400161743164, -6.385799884796143, -6.085000038146973, -4.705699920654297, -6.607699871063232, -6.639400005340576, -6.68310022354126, -6.728799819946289, -6.776700019836426, -6.776700019836426, -6.776700019836426, -6.776700019836426, -6.826900005340576, -6.826900005340576, -6.879899978637695, -6.879899978637695, -6.893499851226807, -6.935800075531006, -6.935800075531006, -6.935800075531006, -6.935800075531006, -6.935800075531006, -6.935800075531006, -6.935800075531006, -6.935800075531006, -6.043799877166748, -5.260900020599365, -6.152400016784668, -4.709099769592285, -5.03000020980835, -6.839900016784668, -5.722499847412109, -5.039899826049805, -5.51039981842041, -4.629799842834473, -6.087800025939941, -5.416399955749512, -6.451000213623047, -6.11299991607666, -5.104700088500977, -5.5447001457214355, -6.237199783325195, -6.0218000411987305, -5.444399833679199, -6.197199821472168, -6.0100998878479, -5.0335001945495605, -5.163000106811523, -5.878200054168701, -5.904300212860107, -5.9079999923706055, -5.894100189208984, -5.992199897766113, -5.980599880218506, -6.07450008392334, -4.919000148773193, -5.070099830627441, -5.250800132751465, -5.525199890136719, -5.634500026702881, -5.794899940490723, -5.860199928283691, -5.975200176239014, -6.080599784851074, -6.168600082397461, -6.222499847412109, -6.264999866485596, -6.265100002288818, -6.328400135040283, -6.42549991607666, -6.460000038146973, -6.514200210571289, -6.552000045776367, -6.552000045776367, -6.552000045776367, -6.552000045776367, -6.59630012512207, -6.59630012512207, -6.59630012512207, -6.702499866485596, -5.947000026702881, -6.79640007019043, -6.79640007019043, -6.79640007019043, -6.8730998039245605, -5.8333001136779785, -5.003200054168701, -6.463399887084961, -6.212299823760986, -6.034900188446045, -4.343800067901611, -5.423399925231934, -6.1819000244140625, -5.562300205230713, -5.568600177764893, -4.935999870300293, -5.571700096130371, -4.238800048828125, -5.988900184631348, -4.814599990844727, -5.206200122833252, -5.301300048828125, -5.601900100708008, -4.852799892425537, -4.929900169372559, -4.883800029754639, -4.945499897003174, -5.603400230407715, -4.302800178527832, -4.98199987411499, -5.572999954223633, -5.307799816131592, -5.258200168609619, -5.088600158691406, -5.4506001472473145, -5.337299823760986, -5.312600135803223, -5.319499969482422, -5.463699817657471, -5.451300144195557, -5.484499931335449, -5.511499881744385, -5.5229997634887695, -4.066999912261963, -5.035900115966797, -5.270100116729736, -5.274099826812744, -5.440100193023682, -5.442599773406982, -5.530700206756592, -5.571000099182129, -5.653900146484375, -5.690299987792969, -5.712100028991699, -3.5002999305725098, -6.119900226593018, -6.219699859619141, -6.284800052642822, -6.221700191497803, -6.330599784851074, -6.330599784851074, -6.330599784851074, -6.330599784851074, -6.330599784851074, -6.366300106048584, -6.482500076293945, -6.52400016784668, -6.307499885559082, -6.262599945068359, -6.6072001457214355, -4.818900108337402, -5.6940999031066895, -6.931000232696533, -3.9616000652313232, -5.29580020904541, -4.965000152587891, -5.9878997802734375, -3.464200019836426, -4.997900009155273, -4.396299839019775, -4.939199924468994, -5.6149001121521, -5.666800022125244, -5.569399833679199, -5.01200008392334, -4.526800155639648, -4.910299777984619, -4.189000129699707, -5.646399974822998, -4.861800193786621, -5.338099956512451, -5.2032999992370605, -4.609099864959717, -5.2692999839782715, -4.6570000648498535, -4.0503997802734375, -4.941800117492676, -5.319699764251709, -5.298799991607666, -4.798900127410889, -5.301499843597412, -5.150000095367432, -5.191400051116943, -5.223800182342529, -3.199399948120117, -5.243100166320801, -5.635700225830078, -5.758900165557861, -5.825099945068359, -5.83620023727417, -6.116600036621094, -3.7153000831604004, -6.437600135803223, -6.437600135803223, -6.463099956512451, -6.543900012969971, -6.543900012969971, -6.543900012969971, -6.543900012969971, -6.572400093078613, -6.572400093078613, -6.572400093078613, -6.572400093078613, -6.572400093078613, -6.572400093078613, -6.572400093078613, -6.711400032043457, -6.771299839019775, -6.771299839019775, -6.617499828338623, -6.9232001304626465, -6.57480001449585, -6.9756999015808105, -7.008699893951416, -4.950500011444092, -3.132499933242798, -6.203999996185303, -5.139599800109863, -5.640600204467773, -5.10890007019043, -5.601600170135498, -5.217299938201904, -2.8220999240875244, -5.323500156402588, -5.934599876403809, -5.303299903869629, -5.906400203704834, -5.2677001953125, -4.6992998123168945, -5.235000133514404, -6.19950008392334, -5.485799789428711, -5.59630012512207, -4.5903000831604, -4.714799880981445, -5.206999778747559, -5.456099987030029, -5.5503997802734375, -5.124100208282471, -5.234399795532227, -5.5167999267578125, -5.388700008392334, -5.263199806213379, -5.45419979095459, -5.495699882507324, -4.567800045013428, -4.924600124359131, -5.137700080871582, -5.472300052642822, -5.829599857330322, -5.847700119018555, -5.847700119018555, -5.903900146484375, -6.037899971008301, -6.054299831390381, -6.088399887084961, -5.618199825286865, -6.251800060272217, -6.2795000076293945, -6.2795000076293945, -6.308000087738037, -6.308000087738037, -6.308000087738037, -6.308000087738037, -6.308000087738037, -6.375100135803223, -6.472300052642822, -6.472300052642822, -6.74429988861084, -6.789999961853027, -6.789999961853027, -6.789999961853027, -6.789999961853027, -6.79010009765625, -6.243899822235107, -5.97629976272583, -4.801700115203857, -5.25439977645874, -5.5883002281188965, -6.116199970245361, -4.417500019073486, -5.33519983291626, -5.009300231933594, -4.833399772644043, -5.737299919128418, -4.601200103759766, -4.962800025939941, -5.564300060272217, -5.5605998039245605, -6.017099857330322, -5.146399974822998, -5.314300060272217, -4.450099945068359, -5.248700141906738, -4.42549991607666, -5.204400062561035, -5.3557000160217285, -5.598800182342529, -5.3282999992370605, -5.38640022277832, -5.452000141143799, -5.321599960327148, -5.634699821472168, -5.451600074768066, -5.479899883270264, -5.488800048828125, -5.570099830627441, -4.878200054168701, -5.389900207519531, -5.669899940490723, -5.676199913024902, -5.823800086975098, -5.803500175476074, -5.803500175476074, -5.865300178527832, -5.8769001960754395, -5.787499904632568, -6.088900089263916, -6.088900089263916, -6.0903000831604, -6.0903000831604, -6.0903000831604, -6.0903000831604, -6.0903000831604, -4.1768999099731445, -6.333199977874756, -6.409900188446045, -6.409900188446045, -5.193299770355225, -6.4644999504089355, -6.4644999504089355, -6.4644999504089355, -6.4644999504089355, -6.493000030517578, -6.493000030517578, -6.493000030517578, -6.493000030517578, -6.493000030517578, -5.51170015335083, -4.811299800872803, -4.940999984741211, -5.381899833679199, -4.098400115966797, -5.688700199127197, -5.858699798583984, -5.182300090789795, -5.541999816894531, -5.265500068664551, -4.633200168609619, -5.281499862670898, -5.270999908447266, -5.042200088500977, -5.463099956512451, -4.758200168609619, -4.393899917602539, -5.540800094604492, -5.216000080108643, -5.4456000328063965, -4.984600067138672, -5.099699974060059, -5.083700180053711, -5.248700141906738, -5.55810022354126, -5.530900001525879, -5.293000221252441, -5.548699855804443, -1.4996000528335571, -4.468599796295166, -5.130300045013428, -5.315199851989746, -5.319699764251709, -5.344200134277344, -5.60860013961792, -5.686200141906738, -5.686200141906738, -5.703800201416016, -5.712800025939941, -5.721700191497803, -5.799200057983398, -5.839099884033203, -5.923900127410889, -5.923900127410889, -5.301499843597412, -5.946300029754639, -6.05679988861084, -6.108699798583984, -6.125400066375732, -6.138999938964844, -6.149400234222412, -6.149400234222412, -6.152900218963623, -6.163400173187256, -6.177499771118164, -5.374800205230713, -6.369500160217285, -6.369500160217285, -3.596100091934204, -5.445499897003174, -4.322400093078613, -4.702700138092041, -3.164299964904785, -5.031300067901611, -5.917699813842773, -5.9542999267578125, -5.714700222015381, -5.373700141906738, -5.678999900817871, -5.163400173187256, -5.57420015335083, -5.058599948883057, -5.725900173187256, -5.6427001953125, -5.637599945068359, -5.53000020980835, -5.704400062561035, -3.5450000762939453, -4.934999942779541, -5.0767998695373535, -5.159800052642822, -5.253300189971924, -5.337900161743164, -5.5100998878479, -5.52839994430542, -5.561999797821045, -5.6209001541137695, -5.705100059509277, -5.7642998695373535, -5.882500171661377, -6.017199993133545, -6.158699989318848, -6.172800064086914, -6.172800064086914, -6.172800064086914, -6.172800064086914, -6.201200008392334, -6.201200008392334, -6.201200008392334, -6.201200008392334, -6.201200008392334, -6.201200008392334, -6.201200008392334, -5.889699935913086, -6.5117998123168945, -6.552000045776367, -6.593900203704834, -5.426000118255615, -5.727200031280518, -3.5822999477386475, -4.780399799346924, -5.383699893951416, -5.379499912261963, -5.194300174713135, -5.7108001708984375, -4.348100185394287, -5.6880998611450195, -4.543900012969971, -4.882800102233887, -5.566800117492676, -5.9868998527526855, -5.272299766540527, -4.503799915313721, -5.13700008392334, -5.544300079345703, -5.1570000648498535, -5.3958001136779785, -5.410200119018555, -5.372399806976318, -5.465799808502197, -5.0767998695373535, -5.34089994430542, -5.428299903869629, -5.52400016784668, -5.535999774932861, -3.89739990234375, -4.713500022888184, -4.713500022888184, -4.997300148010254, -5.126999855041504, -5.127200126647949, -5.1996002197265625, -5.685400009155273, -5.735599994659424, -5.7357001304626465, -5.918900012969971, -5.918900012969971, -5.745999813079834, -6.124499797821045, -5.363699913024902, -6.289599895477295, -6.383699893951416, -6.487500190734863, -6.603300094604492, -6.603300094604492, -6.603300094604492, -6.603300094604492, -6.603300094604492, -6.603300094604492, -6.203700065612793, -6.734399795532227, -6.734399795532227, -6.734399795532227, -6.734399795532227, -5.829999923706055, -6.289700031280518, -5.972700119018555, -5.43310022354126, -5.926300048828125, -5.859899997711182, -5.5792999267578125, -5.441299915313721, -5.914000034332275, -5.742199897766113, -6.336100101470947, -6.430099964141846, -6.339300155639648], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3394999504089355, 1.3391000032424927, 1.3387999534606934, 1.3381999731063843, 1.3378000259399414, 1.3375999927520752, 1.3371000289916992, 1.3366999626159668, 1.330299973487854, 1.3291000127792358, 1.3291000127792358, 1.3289999961853027, 1.3289999961853027, 1.3278000354766846, 1.327299952507019, 1.3271000385284424, 1.3265000581741333, 1.3258999586105347, 1.325700044631958, 1.3251999616622925, 1.3249000310897827, 1.3243000507354736, 1.3241000175476074, 1.323799967765808, 1.3235000371932983, 1.3229999542236328, 1.3229000568389893, 1.3223999738693237, 1.3213000297546387, 1.320099949836731, 1.3151999711990356, 1.3122999668121338, 1.2884000539779663, 1.2997000217437744, 1.2941999435424805, 1.3134000301361084, 1.313599944114685, 1.2628999948501587, 1.267899990081787, 1.273800015449524, 1.2323999404907227, 1.1714999675750732, 1.2156000137329102, 1.220900058746338, 1.100100040435791, 1.1297999620437622, 1.2050000429153442, 1.0348999500274658, 1.0461000204086304, 1.1505999565124512, 1.1291999816894531, 1.1842000484466553, 1.0896999835968018, 0.7662000060081482, 1.00600004196167, 0.6951000094413757, 0.5891000032424927, 0.38760000467300415, 0.8472999930381775, 0.906000018119812, 1.0616999864578247, 0.7235999703407288, 0.9781000018119812, 0.5521000027656555, 1.068600058555603, 0.4399000108242035, 0.37880000472068787, -0.39910000562667847, 0.45579999685287476, 0.37130001187324524, 0.5041000247001648, -0.09049999713897705, 0.38280001282691956, 0.5148000121116638, -0.057999998331069946, 0.6100999712944031, -0.27059999108314514, -0.24879999458789825, 0.2678999900817871, -0.21230000257492065, -0.1453000009059906, 1.7976000308990479, 1.7960000038146973, 1.7956000566482544, 1.7934000492095947, 1.792799949645996, 1.7884000539779663, 1.7883000373840332, 1.7882000207901, 1.7860000133514404, 1.7857999801635742, 1.784500002861023, 1.7843999862670898, 1.7826999425888062, 1.780900001525879, 1.7800999879837036, 1.7798999547958374, 1.7793999910354614, 1.779099941253662, 1.7773000001907349, 1.7764999866485596, 1.7754000425338745, 1.7753000259399414, 1.7747000455856323, 1.7747000455856323, 1.773900032043457, 1.7723000049591064, 1.770300030708313, 1.7692999839782715, 1.7690000534057617, 1.7690000534057617, 1.7359999418258667, 1.742400050163269, 1.7252999544143677, 1.7177000045776367, 1.7095999717712402, 1.7374000549316406, 1.6204999685287476, 1.7056000232696533, 1.6728999614715576, 1.6797000169754028, 1.5484000444412231, 1.6848000288009644, 1.444200038909912, 1.704800009727478, 1.6770000457763672, 1.6123000383377075, 1.6262999773025513, 1.575600028038025, 1.5678999423980713, 1.6088999509811401, 0.5315999984741211, 1.229099988937378, 1.1520999670028687, 0.772599995136261, 1.457900047302246, 0.8529999852180481, 1.0326999425888062, 1.2906999588012695, 1.3021999597549438, 0.9955000281333923, 0.7753000259399414, 0.8614000082015991, 0.8240000009536743, 0.4027999937534332, 0.6953999996185303, 1.1996999979019165, -0.3244999945163727, 1.3183000087738037, 0.1891999989748001, 0.24130000174045563, 0.17090000212192535, -0.18160000443458557, 0.6869000196456909, -0.24379999935626984, -0.19750000536441803, -0.09570000320672989, -0.397599995136261, 0.3646000027656555, 1.8535000085830688, 1.8533999919891357, 1.8530999422073364, 1.8525999784469604, 1.8516000509262085, 1.8516000509262085, 1.8509999513626099, 1.850100040435791, 1.8497999906539917, 1.8496999740600586, 1.8473000526428223, 1.8473000526428223, 1.847000002861023, 1.8459999561309814, 1.844599962234497, 1.844499945640564, 1.844099998474121, 1.844099998474121, 1.8438999652862549, 1.8414000272750854, 1.8411999940872192, 1.8410999774932861, 1.84089994430542, 1.8387000560760498, 1.8381999731063843, 1.8378000259399414, 1.8375999927520752, 1.837499976158142, 1.837399959564209, 1.8372000455856323, 1.8286999464035034, 1.8359999656677246, 1.8352999687194824, 1.8077000379562378, 1.8188999891281128, 1.8152999877929688, 1.823699951171875, 1.7935999631881714, 1.8061000108718872, 1.8104000091552734, 1.8162000179290771, 1.7351000308990479, 1.7180999517440796, 1.59089994430542, 1.6234999895095825, 1.763800024986267, 1.5637999773025513, 1.6151000261306763, 1.4628000259399414, 1.6045000553131104, 1.6448999643325806, 1.4493999481201172, 1.0650999546051025, 1.1100000143051147, 0.2946999967098236, 1.2585999965667725, 1.1801999807357788, 1.2855000495910645, 1.4987000226974487, 0.4729999899864197, -0.24809999763965607, 0.9782000184059143, 0.25839999318122864, 1.138700008392334, 0.25099998712539673, 0.07329999655485153, 0.30169999599456787, -0.1889999955892563, 0.15209999680519104, 0.07689999788999557, -0.03220000118017197, -0.2371000051498413, -0.3102000057697296, 2.034899950027466, 2.0325000286102295, 2.0313000679016113, 2.0278000831604004, 2.026900053024292, 2.025899887084961, 2.025700092315674, 2.0211000442504883, 2.0183000564575195, 2.016200065612793, 2.015899896621704, 2.014899969100952, 2.014899969100952, 2.014699935913086, 2.0141000747680664, 2.012700080871582, 2.0095999240875244, 2.009500026702881, 2.0078001022338867, 2.0078001022338867, 2.0064001083374023, 2.0064001083374023, 2.0046000480651855, 2.0044000148773193, 2.002700090408325, 2.002700090408325, 2.0023999214172363, 2.0023000240325928, 2.0016000270843506, 2.0004000663757324, 1.9945000410079956, 1.9979000091552734, 1.9711999893188477, 1.9545999765396118, 1.9297000169754028, 1.8803999423980713, 1.944100022315979, 1.792799949645996, 1.7480000257492065, 1.8725999593734741, 1.7257000207901, 1.749899983406067, 1.7372000217437744, 1.629699945449829, 1.6779999732971191, 1.604599952697754, 1.6924999952316284, 1.618299961090088, 1.8740999698638916, 1.7473000288009644, 1.7474000453948975, 1.607200026512146, 1.6505000591278076, 1.5543999671936035, 1.6806999444961548, 0.5206999778747559, 1.2019000053405762, 0.8016999959945679, 1.128000020980835, 1.2688000202178955, 1.3009999990463257, 0.8027999997138977, 0.7365000247955322, 0.7954000234603882, 1.2604999542236328, 1.30239999294281, 1.4160000085830688, 0.6675999760627747, 0.47780001163482666, -0.24899999797344208, 0.6312000155448914, 0.9193000197410583, 0.7095999717712402, 0.6922000050544739, -0.1137000024318695, 0.019700000062584877, 0.8787999749183655, 0.3061999976634979, 0.8050000071525574, 0.3181000053882599, -0.24539999663829803, 3.183000087738037, 3.1810998916625977, 3.1758999824523926, 3.175100088119507, 3.173099994659424, 3.1689999103546143, 3.16759991645813, 3.1603000164031982, 3.1507999897003174, 3.150399923324585, 3.1468000411987305, 3.139400005340576, 3.1389000415802, 3.138700008392334, 3.138700008392334, 3.1366000175476074, 3.1357998847961426, 3.1333999633789062, 3.1333999633789062, 3.130000114440918, 3.130000114440918, 3.130000114440918, 3.126199960708618, 3.126199960708618, 3.126199960708618, 3.126199960708618, 3.126199960708618, 3.1261000633239746, 3.125200033187866, 3.1221001148223877, 3.1221001148223877, 3.036600112915039, 3.0708999633789062, 2.8259999752044678, 2.7530999183654785, 3.059999942779541, 2.8770999908447266, 3.014899969100952, 3.059000015258789, 2.8131000995635986, 2.5701000690460205, 2.382999897003174, 2.3940000534057617, 1.7766000032424927, 2.3141000270843506, 2.1152000427246094, 2.747499942779541, 2.769700050354004, 2.3740999698638916, 2.1728999614715576, 2.840399980545044, 1.9657000303268433, 2.2448999881744385, 2.411400079727173, 2.5678999423980713, 2.5260000228881836, 1.1052000522613525, 0.5465999841690063, 2.231600046157837, 1.1555999517440796, 0.6534000039100647, 0.3939000070095062, 0.2922999858856201, -0.5654000043869019, -0.3698999881744385, 0.3725000023841858, 0.602400004863739, 1.7213000059127808, 0.30250000953674316, 0.6456999778747559, -0.21889999508857727, 1.4771000146865845, 0.06719999760389328, -0.00279999990016222, 0.1687999963760376, 0.0625, 0.7125999927520752, 3.2425999641418457, 3.2374000549316406, 3.230799913406372, 3.225399971008301, 3.2232000827789307, 3.2186999320983887, 3.2183001041412354, 3.2160000801086426, 3.2151999473571777, 3.212899923324585, 3.2070999145507812, 3.2060000896453857, 3.2058000564575195, 3.2051000595092773, 3.2049999237060547, 3.200200080871582, 3.200200080871582, 3.200200080871582, 3.1988000869750977, 3.1988000869750977, 3.1974000930786133, 3.1974000930786133, 3.1958999633789062, 3.1951000690460205, 3.192199945449829, 3.191699981689453, 3.1884000301361084, 3.186300039291382, 3.1810998916625977, 3.1791999340057373, 3.176800012588501, 3.157399892807007, 3.084199905395508, 3.072200059890747, 3.0058000087738037, 3.109600067138672, 3.109999895095825, 3.0155999660491943, 2.865999937057495, 2.801300048828125, 2.9881999492645264, 2.8559000492095947, 2.7620999813079834, 2.793299913406372, 2.771199941635132, 2.846100091934204, 2.6777000427246094, 2.9000000953674316, 2.6366000175476074, 2.0776000022888184, 2.1136999130249023, 1.4488999843597412, 0.9035000205039978, 1.5755000114440918, 2.4033000469207764, 1.295699954032898, -0.4731000065803528, 0.27630001306533813, 0.65829998254776, 1.1030000448226929, 1.666200041770935, 0.5230000019073486, 1.5174000263214111, 1.9392000436782837, 0.5496000051498413, 3.3756000995635986, 3.3745999336242676, 3.3636999130249023, 3.363600015640259, 3.3596999645233154, 3.3508999347686768, 3.3494999408721924, 3.3392999172210693, 3.3369998931884766, 3.334700107574463, 3.327399969100952, 3.325500011444092, 3.3227999210357666, 3.3197999000549316, 3.316499948501587, 3.316499948501587, 3.316499948501587, 3.316499948501587, 3.3129000663757324, 3.3129000663757324, 3.309000015258789, 3.309000015258789, 3.3078999519348145, 3.3046000003814697, 3.3046000003814697, 3.3046000003814697, 3.3046000003814697, 3.3046000003814697, 3.3046000003814697, 3.3046000003814697, 3.3046000003814697, 3.282599925994873, 3.1105000972747803, 3.194200038909912, 2.7423999309539795, 2.3879001140594482, 3.2532999515533447, 2.5429000854492188, 1.9127000570297241, 1.986199975013733, 1.073099970817566, 2.447200059890747, 1.672700047492981, 2.8053998947143555, 2.320499897003174, 0.8066999912261963, 1.4572999477386475, 2.468100070953369, 2.105600118637085, 1.1367000341415405, 2.3262999057769775, 1.7141000032424927, -1.0997999906539917, -0.8698999881744385, 0.490200012922287, 0.093299999833107, 0.0640999972820282, -0.41449999809265137, 0.6151000261306763, 0.2176000028848648, 0.3070000112056732, 3.40339994430542, 3.401400089263916, 3.3984999656677246, 3.3931000232696533, 3.390500068664551, 3.386199951171875, 3.384200096130371, 3.3803999423980713, 3.376499891281128, 3.3729000091552734, 3.3705999851226807, 3.3687000274658203, 3.3687000274658203, 3.3657000064849854, 3.360599994659424, 3.358799934387207, 3.355799913406372, 3.3534998893737793, 3.3534998893737793, 3.3533999919891357, 3.3533999919891357, 3.350800037384033, 3.350800037384033, 3.350800037384033, 3.3436999320983887, 3.34060001373291, 3.336899995803833, 3.336899995803833, 3.3368000984191895, 3.3308000564575195, 3.3306000232696533, 3.318000078201294, 3.3125, 3.2913999557495117, 3.244499921798706, 2.9551000595092773, 3.044800043106079, 3.2395999431610107, 2.8066999912261963, 2.8022000789642334, 2.38700008392334, 2.76419997215271, 1.720900058746338, 2.9595999717712402, 2.0601999759674072, 2.2827999591827393, 2.3554000854492188, 2.6019999980926514, 1.7008999586105347, 1.7525999546051025, 1.5997999906539917, 1.3539999723434448, 2.3452999591827393, -0.009700000286102295, 0.28029999136924744, 1.9246000051498413, 0.5512999892234802, 0.07209999859333038, -1.1548999547958374, 1.0069999694824219, 0.12639999389648438, -0.11969999969005585, -0.2687000036239624, 0.904699981212616, 0.12860000133514404, -0.05590000003576279, 0.791100025177002, 0.6155999898910522, 3.442699909210205, 3.4339001178741455, 3.4302000999450684, 3.4300999641418457, 3.4268999099731445, 3.4267001152038574, 3.4249000549316406, 3.4238998889923096, 3.4219000339508057, 3.4209001064300537, 3.420300006866455, 3.4175000190734863, 3.406599998474121, 3.402400016784668, 3.399399995803833, 3.398900032043457, 3.3970999717712402, 3.3970999717712402, 3.3970999717712402, 3.3970999717712402, 3.3970999717712402, 3.3952999114990234, 3.3886001110076904, 3.3866000175476074, 3.383500099182129, 3.3819000720977783, 3.380000114440918, 3.3757998943328857, 3.3726999759674072, 3.357100009918213, 3.3194000720977783, 3.319999933242798, 3.2715001106262207, 3.3252999782562256, 2.8352999687194824, 3.121799945831299, 2.922800064086914, 3.053999900817871, 3.220400094985962, 3.2318999767303467, 3.1370999813079834, 2.90910005569458, 2.6098999977111816, 2.695199966430664, 2.294600009918213, 3.046600103378296, 2.3798000812530518, 2.762700080871582, 2.5548999309539795, 1.5295000076293945, 2.6421000957489014, 1.4723999500274658, 0.24269999563694, 1.8530000448226929, 2.605799913406372, 2.5469000339508057, 0.4578000009059906, 2.1875, 0.7091000080108643, 0.27230000495910645, 0.7738000154495239, 3.5518999099731445, 3.534899950027466, 3.525599956512451, 3.521899938583374, 3.519700050354004, 3.5134999752044678, 3.50819993019104, 3.492500066757202, 3.491300106048584, 3.491300106048584, 3.4897000789642334, 3.4844000339508057, 3.4844000339508057, 3.4844000339508057, 3.4844000339508057, 3.4825000762939453, 3.4825000762939453, 3.4825000762939453, 3.4825000762939453, 3.4825000762939453, 3.4825000762939453, 3.4825000762939453, 3.4721999168395996, 3.4672999382019043, 3.4672999382019043, 3.456399917602539, 3.453700065612793, 3.4528000354766846, 3.448499917984009, 3.445199966430664, 3.441499948501587, 3.0230000019073486, 3.3266000747680664, 3.046799898147583, 3.1110000610351562, 2.842600107192993, 2.953000068664551, 2.754699945449829, 1.4709999561309814, 2.607100009918213, 3.0360000133514404, 2.4881999492645264, 2.903700113296509, 2.312000036239624, 1.6402000188827515, 2.125699996948242, 3.050100088119507, 2.126199960708618, 2.2309000492095947, 0.6664000153541565, 0.8465999960899353, 0.9193999767303467, 1.4187999963760376, 1.432800054550171, 0.06880000233650208, 0.09589999914169312, 0.9965999722480774, 0.07500000298023224, -1.3294999599456787, 0.025299999862909317, -0.04470000043511391, 3.777899980545044, 3.7725000381469727, 3.7683000564575195, 3.759500026702881, 3.7465999126434326, 3.745800018310547, 3.745800018310547, 3.7432000637054443, 3.736299991607666, 3.73580002784729, 3.7339000701904297, 3.7274999618530273, 3.7242000102996826, 3.722399950027466, 3.722399950027466, 3.7204999923706055, 3.7204999923706055, 3.7204999923706055, 3.7204999923706055, 3.7204999923706055, 3.71589994430542, 3.7084999084472656, 3.7084999084472656, 3.6842000484466553, 3.679500102996826, 3.679500102996826, 3.679500102996826, 3.679500102996826, 3.67930006980896, 3.6768999099731445, 3.594399929046631, 2.961400032043457, 3.0977001190185547, 3.143399953842163, 3.336199998855591, 2.095900058746338, 2.667799949645996, 2.3757998943328857, 2.178499937057495, 2.987299919128418, 1.5770000219345093, 1.9291000366210938, 2.63100004196167, 2.2502999305725098, 3.155600070953369, 1.3438999652862549, 1.6898000240325928, -0.15690000355243683, 1.4572999477386475, -0.4916999936103821, 1.2791999578475952, 1.4996999502182007, 1.9914000034332275, 0.31299999356269836, 0.6111999750137329, 0.8475000262260437, -0.27079999446868896, 1.8787000179290771, -0.12129999697208405, 0.10369999706745148, -0.025100000202655792, 1.1124000549316406, 3.8378000259399414, 3.825500011444092, 3.8157999515533447, 3.815500020980835, 3.8092000484466553, 3.808799982070923, 3.808799982070923, 3.8071999549865723, 3.8066999912261963, 3.7990000247955322, 3.795300006866455, 3.795300006866455, 3.794100046157837, 3.794100046157837, 3.794100046157837, 3.794100046157837, 3.794100046157837, 3.7811999320983887, 3.779099941253662, 3.773200035095215, 3.773099899291992, 3.769200086593628, 3.768699884414673, 3.768699884414673, 3.768699884414673, 3.768699884414673, 3.7662999629974365, 3.7662999629974365, 3.7662999629974365, 3.7662999629974365, 3.7662999629974365, 3.737600088119507, 3.694000005722046, 3.641900062561035, 3.6214001178741455, 3.446199893951416, 3.58870005607605, 3.456700086593628, 3.102400064468384, 3.2820000648498535, 2.904599905014038, 2.0260000228881836, 2.6308999061584473, 2.588599920272827, 2.1113998889923096, 2.589400053024292, 0.8217999935150146, -0.460099995136261, 2.376699924468994, 1.2676000595092773, 2.030600070953369, 0.20829999446868896, 0.3797999918460846, 0.2467000037431717, 0.7232999801635742, 2.2978999614715576, 1.906999945640564, -0.9998000264167786, 0.41100001335144043, 3.866300106048584, 3.857300043106079, 3.8454999923706055, 3.84060001373291, 3.8405001163482666, 3.8397998809814453, 3.8308000564575195, 3.827699899673462, 3.827699899673462, 3.826900005340576, 3.8264999389648438, 3.826200008392334, 3.822700023651123, 3.8208000659942627, 3.8166000843048096, 3.8166000843048096, 3.815500020980835, 3.8153998851776123, 3.8092000484466553, 3.8060998916625977, 3.805000066757202, 3.8041999340057373, 3.803499937057495, 3.803499937057495, 3.803299903869629, 3.8025999069213867, 3.8017001152038574, 3.789099931716919, 3.7878000736236572, 3.7878000736236572, 3.7239999771118164, 3.7600998878479004, 3.595099925994873, 3.471100091934204, 2.7471001148223877, 3.3178000450134277, 3.6998000144958496, 3.6910998821258545, 3.529400110244751, 3.2432000637054443, 3.4639999866485596, 2.904400110244751, 2.8159000873565674, 1.2440999746322632, 2.8473000526428223, 1.9071999788284302, 1.8366999626159668, -1.5963000059127808, 1.2271000146865845, 4.283899784088135, 4.261199951171875, 4.2565999031066895, 4.253699779510498, 4.25, 4.246300220489502, 4.2382001876831055, 4.237199783325195, 4.235400199890137, 4.232100009918213, 4.2270002365112305, 4.221799850463867, 4.215000152587891, 4.204400062561035, 4.191800117492676, 4.190499782562256, 4.190499782562256, 4.190499782562256, 4.190199851989746, 4.187699794769287, 4.187699794769287, 4.187699794769287, 4.187699794769287, 4.187699794769287, 4.187699794769287, 4.187699794769287, 4.164100170135498, 4.152400016784668, 4.14709997177124, 4.141300201416016, 4.137700080871582, 4.131499767303467, 3.9983999729156494, 4.059700012207031, 3.888700008392334, 3.7674999237060547, 3.585599899291992, 3.7881999015808105, 2.6291000843048096, 3.6963000297546387, 2.558500051498413, 2.386199951171875, 3.306299924850464, 3.8975000381469727, 2.6763999462127686, 1.0575000047683716, 2.2253000736236572, 3.0752999782562256, 1.763800024986267, 2.4762001037597656, 2.0985000133514404, 1.6296000480651855, 2.139699935913086, -1.1431000232696533, -0.08420000225305557, 0.3707999885082245, 1.7203999757766724, 1.8513000011444092, 5.29580020904541, 5.258800029754639, 5.258800029754639, 5.232500076293945, 5.226099967956543, 5.225500106811523, 5.218900203704834, 5.157199859619141, 5.149400234222412, 5.1493000984191895, 5.117300033569336, 5.117300033569336, 5.089600086212158, 5.075200080871582, 5.0665998458862305, 5.036099910736084, 5.011600017547607, 4.982500076293945, 4.947500228881836, 4.947500228881836, 4.947500228881836, 4.947500228881836, 4.947500228881836, 4.947500228881836, 4.926400184631348, 4.904300212860107, 4.904300212860107, 4.904300212860107, 4.904300212860107, 4.8917999267578125, 4.864699840545654, 4.80709981918335, 4.6427998542785645, 4.735000133514404, 4.578199863433838, 4.309500217437744, 2.6986000537872314, 3.3232998847961426, 2.4279000759124756, 3.7548999786376953, 3.8945999145507812, 1.3955999612808228]}, \"token.table\": {\"Topic\": [10, 6, 14, 13, 14, 2, 4, 1, 2, 3, 10, 1, 2, 8, 9, 11, 14, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 1, 2, 3, 11, 1, 2, 6, 8, 1, 2, 3, 4, 6, 4, 6, 1, 2, 14, 14, 13, 11, 2, 3, 4, 11, 1, 2, 3, 4, 6, 7, 8, 9, 14, 4, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 6, 8, 3, 4, 4, 3, 11, 12, 8, 9, 2, 3, 4, 5, 7, 8, 8, 7, 1, 4, 9, 13, 1, 2, 4, 8, 12, 3, 5, 11, 3, 1, 2, 3, 4, 6, 10, 14, 1, 4, 5, 1, 4, 10, 12, 4, 5, 11, 1, 2, 3, 4, 6, 9, 10, 5, 2, 3, 4, 6, 8, 11, 11, 7, 1, 2, 3, 4, 6, 8, 14, 3, 4, 4, 4, 6, 11, 9, 1, 2, 6, 9, 14, 4, 10, 11, 1, 9, 11, 15, 5, 12, 8, 15, 3, 6, 5, 1, 2, 4, 6, 8, 14, 6, 11, 13, 10, 10, 6, 14, 1, 4, 12, 3, 7, 8, 1, 3, 4, 6, 7, 7, 12, 13, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 6, 14, 5, 3, 2, 4, 11, 2, 14, 13, 2, 1, 2, 3, 5, 6, 12, 14, 14, 2, 3, 7, 1, 2, 6, 7, 14, 7, 6, 2, 5, 15, 1, 2, 5, 7, 9, 10, 11, 15, 1, 8, 10, 3, 7, 13, 1, 3, 6, 10, 1, 5, 15, 1, 3, 5, 10, 7, 11, 1, 2, 4, 2, 3, 4, 6, 7, 9, 13, 1, 2, 4, 6, 8, 10, 14, 4, 4, 10, 11, 12, 10, 1, 2, 5, 9, 6, 8, 9, 1, 2, 3, 4, 5, 7, 8, 11, 14, 1, 2, 3, 4, 5, 6, 8, 10, 11, 14, 2, 3, 13, 13, 15, 1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 1, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 1, 4, 4, 15, 4, 1, 2, 4, 12, 1, 5, 8, 11, 1, 3, 4, 6, 8, 13, 10, 1, 2, 5, 6, 1, 2, 4, 9, 11, 1, 3, 4, 7, 1, 11, 9, 4, 8, 1, 8, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 11, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 9, 9, 9, 1, 4, 8, 1, 4, 10, 11, 13, 7, 5, 1, 2, 3, 4, 7, 11, 3, 4, 3, 5, 13, 2, 5, 1, 2, 3, 4, 5, 8, 9, 12, 9, 12, 1, 2, 3, 5, 9, 14, 7, 12, 15, 13, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 2, 3, 4, 8, 9, 12, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3, 4, 5, 7, 8, 9, 12, 13, 8, 1, 2, 3, 4, 5, 6, 8, 10, 12, 11, 1, 2, 3, 4, 5, 6, 8, 10, 11, 7, 15, 1, 2, 6, 7, 9, 10, 12, 1, 3, 1, 4, 8, 9, 5, 6, 1, 8, 15, 3, 6, 2, 4, 9, 4, 8, 1, 2, 3, 5, 6, 7, 10, 12, 6, 1, 14, 12, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14, 14, 5, 6, 6, 6, 3, 4, 2, 3, 7, 2, 14, 14, 11, 11, 10, 4, 4, 6, 1, 2, 14, 8, 5, 8, 1, 11, 2, 4, 10, 12, 3, 5, 10, 5, 6, 7, 11, 12, 13, 1, 3, 12, 3, 8, 1, 5, 9, 1, 12, 2, 3, 1, 2, 4, 12, 15, 10, 10, 2, 7, 8, 12, 14, 3, 10, 1, 3, 4, 10, 1, 3, 6, 15, 14, 5, 1, 2, 3, 4, 8, 11, 5, 10, 1, 5, 8, 11, 1, 2, 5, 8, 6, 1, 1, 1, 11, 15, 9, 3, 12, 1, 2, 3, 6, 8, 1, 2, 13, 12, 1, 2, 4, 5, 8, 9, 11, 12, 3, 7, 4, 2, 6, 11, 12, 1, 2, 3, 4, 5, 7, 8, 10, 11, 12, 1, 2, 3, 5, 7, 8, 11, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 2, 7, 2, 2, 5, 15, 9, 11, 15, 13, 1, 2, 3, 4, 5, 6, 8, 10, 11, 13, 1, 6, 7, 5, 6, 1, 2, 4, 5, 12, 2, 1, 2, 3, 4, 5, 6, 8, 14, 2, 2, 3, 6, 10, 2, 13, 1, 10, 12, 12, 7, 1, 2, 3, 10, 11, 5, 2, 5, 12, 14, 9, 1, 14, 1, 8, 12, 7, 14, 3, 13, 4, 1, 2, 3, 4, 8, 10, 12, 14, 5, 2, 3, 5, 11, 15, 1, 2, 4, 5, 7, 15, 1, 2, 3, 10, 12, 14, 1, 2, 3, 7, 4, 1, 2, 1, 2, 3, 4, 8, 10, 14, 15, 8, 10, 11, 5, 1, 4, 8, 10, 10, 1, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 10, 11, 1, 2, 3, 5, 8, 9, 11, 13, 15, 2, 9, 1, 10, 1, 2, 3, 5, 6, 8, 11, 14, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 1, 3, 4, 6, 8, 12, 14, 14, 1, 2, 3, 4, 5, 8, 11, 12, 14, 13, 1, 4, 8, 9, 10, 9, 1, 4, 8, 11, 2, 3, 1, 2, 3, 4, 6, 12, 3, 1, 3, 4, 5, 8, 12, 1, 3, 5, 7, 10, 13, 1, 2, 3, 4, 6, 7, 10, 11, 14, 10, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 14, 1, 2, 4, 5, 8, 9, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 2, 3, 6, 8, 10, 14, 1, 8, 10, 13, 2, 1, 6, 7, 8, 1, 2, 3, 4, 7, 8, 11, 13, 14, 15, 1, 6, 13, 6, 14, 6, 1, 1, 4, 10, 12, 7, 1, 4, 10, 11, 1, 2, 3, 4, 7, 11, 12, 13, 3, 8, 1, 2, 3, 4, 3, 3, 5, 9, 1, 7, 7, 12, 13, 1, 4, 8, 12, 8, 2, 10, 1, 1, 8, 11, 1, 2, 3, 4, 6, 8, 9, 12, 3, 12, 1, 2, 7, 12, 1, 3, 9, 3, 1, 2, 3, 4, 5, 8, 9, 11, 1, 8, 9, 1, 2, 3, 4, 8, 9, 11, 14, 1, 2, 4, 6, 8, 1, 2, 3, 4, 5, 8, 11, 15, 8, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 14, 1, 2, 4, 8, 14, 1, 4, 5, 10, 4, 5, 9, 1, 2, 3, 4, 12, 2, 7, 12, 13, 13, 12, 1, 2, 5, 6, 12, 2, 5, 6, 8, 9, 12, 5, 5, 7, 15, 5, 12, 1, 2, 3, 4, 8, 11, 14, 5, 1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 14, 8, 14, 14, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 1, 2, 4, 7, 9, 1, 2, 4, 7, 4, 11, 2, 3, 4, 5, 6, 11, 4, 15, 13, 13, 8, 11, 1, 2, 3, 4, 5, 6, 8, 11, 1, 2, 3, 4, 5, 7, 8, 14, 1, 2, 3, 4, 1, 2, 3, 7, 10, 7, 15, 12, 1, 15, 14, 2, 1, 2, 3, 4, 14, 13, 11, 1, 7, 2, 3, 7, 14, 6, 2, 13, 13, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 8, 1, 2, 3, 4, 5, 6, 10, 1, 4, 6, 13, 14, 6, 3, 11, 1, 4, 6, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 7, 9, 1, 6, 7, 11, 1, 3, 10, 11, 11, 13, 1, 3, 4, 6, 8, 11, 2, 4, 6, 12, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 8, 11, 12, 13, 12, 1, 5, 8, 12, 13, 1, 15, 4, 14, 1, 2, 3, 6, 7, 9, 2, 5, 6, 13, 10, 4, 12, 10, 14, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 1, 5, 6, 8, 12, 1, 2, 8, 9, 10, 2, 4, 6, 11, 3, 13, 11, 1, 2, 3, 4, 5, 8, 9, 10, 14, 1, 2, 3, 4, 5, 7, 8, 11, 12, 5, 8, 8, 3, 1, 12, 1, 2, 3, 4, 5, 8, 10, 12, 13, 7, 1, 2, 3, 4, 5, 1, 4, 14, 4, 6, 4, 3, 1, 2, 3, 8, 8, 7, 12, 14, 2, 7, 10, 7, 15, 1, 2, 4, 5, 6, 14, 4, 1, 4, 6, 3, 12, 3, 12, 13, 1, 2, 3, 4, 6, 12, 14, 3, 4, 5, 6, 8, 5, 15, 1, 2, 3, 4, 6, 7, 8, 8, 6, 14, 8, 15, 6, 10, 1, 4, 1, 5, 8, 4, 3, 3, 4, 11, 3, 15, 4, 6, 1, 2, 3, 4, 5, 7, 8, 11, 12, 13, 14, 11, 3, 7, 15, 3, 7, 13, 2, 3, 7, 1, 1, 3, 11, 12, 7, 1, 6, 8, 14, 1, 9, 2, 3, 3, 2, 5, 9, 12, 7, 1, 4, 1, 3, 5, 6, 1, 2, 3, 4, 7, 12, 14, 4, 2, 3, 4, 6, 12, 1, 14, 15, 9, 12, 1, 3, 4, 5, 8, 9, 11, 12, 7, 10, 12, 5, 4, 2, 1, 2, 3, 5, 6, 9, 12, 13, 1, 3, 4, 5, 9, 12, 4, 4, 8, 9, 11, 15, 10, 11, 1, 2, 3, 4, 5, 6, 13, 1, 5, 1, 2, 14, 7, 6, 2, 5, 12, 1, 8, 9, 12, 10, 1, 2, 3, 4, 7, 8, 9, 10, 11, 9, 9, 1, 2, 3, 4, 8, 9, 11, 14, 10, 10, 1, 3, 6, 1, 3, 4, 5, 8, 9, 10, 8, 14, 10, 2, 15, 1, 2, 4, 6, 8, 14, 2, 6, 2, 4, 14, 2, 3, 6, 9, 11, 2, 1, 2, 4, 6, 8, 10, 12, 15, 2, 3, 5, 6, 1, 2, 3, 4, 10, 12, 14, 4, 13, 2, 6, 9, 1, 3, 9, 12, 14, 1, 3, 5, 6, 11, 13, 1, 2, 3, 4, 9, 12, 13, 14, 1, 2, 3, 8, 9, 10, 11, 13, 3, 2, 15, 2, 3, 4, 8, 11, 1, 2, 4, 5, 9, 10, 9, 1, 2, 3, 4, 8, 10, 4, 6, 9, 4, 1, 2, 6, 8, 1, 10, 12, 1, 3, 10, 1, 4, 11, 1, 2, 3, 4, 5, 11, 12, 1, 2, 3, 5, 6, 7, 10, 12, 1, 2, 3, 4, 5, 6, 8, 9, 12, 13, 14, 6, 2, 3, 15, 8, 11, 2, 7, 1, 7, 8, 3, 11, 13, 15, 3, 9, 11, 7, 14, 2, 9, 8, 8, 11, 2, 7, 1, 2, 3, 4, 8, 11, 14, 1, 2, 4, 5, 11, 13, 14, 1, 4, 3, 1, 6, 11, 12, 6, 6, 2, 4, 8, 12, 2, 2, 5, 6, 14, 1, 2, 4, 9, 10, 12, 11, 12, 1, 2, 6, 13, 15, 15, 3, 13, 1, 2, 14, 1, 4, 11, 6, 3, 8, 11, 7, 11, 3, 6, 13, 1, 3, 13, 1, 7, 1, 2, 4, 1, 1, 2, 3, 6, 7, 8, 9, 12, 13, 1, 2, 3, 6, 7, 13, 1, 2, 3, 4, 6, 7, 10, 4, 5, 11, 12, 12, 6, 5, 6, 1, 3, 5, 8, 10, 11, 9, 5, 14, 7, 15, 2, 9, 15, 1, 3, 5, 4, 7, 5, 10, 6, 1, 2, 3, 4, 6, 8, 10, 11, 2, 1, 2, 3, 4, 1, 12, 1, 2, 1, 5, 1, 3, 4, 7, 9, 1, 2, 6, 13, 1, 5, 5, 2, 5, 13, 1, 2, 4, 6, 7, 9, 12, 13, 1, 4, 6, 10, 11, 4, 8, 1, 3, 4, 5, 9, 2, 5, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 14, 8, 7, 1, 2, 3, 8, 12, 15, 1, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 11, 1, 2, 3, 10, 1, 2, 4, 9, 10, 11, 8, 9, 12, 1, 9, 2, 1, 6, 8, 2, 9, 3, 10, 15, 5, 4, 10, 1, 4, 10, 1, 4, 12, 1, 2, 3, 4, 5, 6, 10, 11, 14, 2, 15, 2, 4, 6, 5, 14, 1, 2, 3, 5, 7, 13, 14, 15, 1, 1, 2, 3, 4, 5, 8, 9, 10, 12, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 1, 2, 4, 5, 10, 12, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 6, 1, 2, 3, 4, 5, 6, 8, 10, 14, 1, 2, 3, 4, 6, 7, 12, 1, 2, 10, 7, 1, 2, 3, 4, 9, 1, 3, 10, 3, 6, 3, 13, 13, 11, 1, 2, 4, 5, 14, 1, 3, 7, 11, 12, 12, 3, 5, 3, 4, 5, 8, 8, 3, 13, 1, 2, 5, 2, 2, 4, 5, 1, 3, 5, 6, 7, 8, 9, 11, 12, 3, 6, 7, 9, 15, 7, 9, 1, 2, 4, 11, 1, 2, 3, 4, 5, 6, 9, 14, 8, 3, 1, 2, 3, 4, 6, 8, 14, 1, 2, 4, 8, 12, 14, 3, 6, 11, 13, 9, 2, 1, 4, 10, 11, 1, 2, 3, 4, 5, 7, 11, 4, 15, 6, 10, 7, 1, 2, 3, 4, 5, 6, 8, 11, 12, 14, 4, 6, 10, 7, 2, 3, 4, 13, 1, 9, 10], \"Freq\": [0.9430745840072632, 0.4190838038921356, 0.5333793759346008, 0.2512766718864441, 0.6910108327865601, 0.8847927451133728, 0.1024496927857399, 0.22481371462345123, 0.32116246223449707, 0.1846684068441391, 0.2649590075016022, 0.06432193517684937, 0.27015212178230286, 0.05788974091410637, 0.46955013275146484, 0.01929658092558384, 0.11577948182821274, 0.035480402410030365, 0.035480402410030365, 0.8870100975036621, 0.24579262733459473, 0.13566002249717712, 0.19984327256679535, 0.27132004499435425, 0.059077754616737366, 0.016775164753198624, 0.005105484742671251, 0.03136226534843445, 0.009481614455580711, 0.013857744634151459, 0.012399034574627876, 0.0007293549715541303, 0.05128445103764534, 0.8923494219779968, 0.030770670622587204, 0.020513780415058136, 0.10887739807367325, 0.24615760147571564, 0.009467599913477898, 0.6295953989028931, 0.11188456416130066, 0.15899385511875153, 0.5623671412467957, 0.04710929095745087, 0.11777322739362717, 0.10893119126558304, 0.8714495301246643, 0.06779661774635315, 0.8305085897445679, 0.09322035312652588, 0.9363320469856262, 0.9422484636306763, 0.9692584276199341, 0.16765815019607544, 0.12742018699645996, 0.6974579095840454, 0.9432153105735779, 0.5366062521934509, 0.17369097471237183, 0.10732125490903854, 0.057896990329027176, 0.03530304506421089, 0.019769703969359398, 0.011296973563730717, 0.050836384296417236, 0.005648486781865358, 0.9735630750656128, 0.37072476744651794, 0.1683393120765686, 0.2402145266532898, 0.07187521457672119, 0.005674358922988176, 0.04539487138390541, 0.011348717845976353, 0.017023077234625816, 0.02648034133017063, 0.041611965745687485, 0.9327279329299927, 0.9926812648773193, 0.9880882501602173, 0.25088849663734436, 0.7440141439437866, 0.9503356218338013, 0.9846634864807129, 0.9843459725379944, 0.9570713639259338, 0.05797396972775459, 0.9275835156440735, 0.5490702390670776, 0.1314675211906433, 0.27840179204940796, 0.0077333832159638405, 0.0038666916079819202, 0.02706684172153473, 0.9637998938560486, 0.9288133978843689, 0.9799003005027771, 0.44335079193115234, 0.5066866278648376, 0.04222388565540314, 0.9542210102081299, 0.004259915091097355, 0.00851983018219471, 0.02981940656900406, 0.9756635427474976, 0.9680125713348389, 0.9825289249420166, 0.9446207284927368, 0.9851676225662231, 0.02333618700504303, 0.30725979804992676, 0.18280012905597687, 0.07389792054891586, 0.2061363160610199, 0.027225550264120102, 0.17502139508724213, 0.0988847091794014, 0.8899624347686768, 0.9360029101371765, 0.04023101553320885, 0.5230031609535217, 0.14080855250358582, 0.28161710500717163, 0.34765028953552246, 0.6257705092430115, 0.9432153105735779, 0.3822261095046997, 0.3579375147819519, 0.17129866778850555, 0.03835044801235199, 0.011505134403705597, 0.03323705494403839, 0.003835044801235199, 0.9298214912414551, 0.04935932904481888, 0.05552924796938896, 0.17275765538215637, 0.6786907911300659, 0.03701949864625931, 0.9432154297828674, 0.9021807312965393, 0.9369086027145386, 0.2512410581111908, 0.4462341368198395, 0.0918717309832573, 0.1031213328242302, 0.04687333479523659, 0.050623200833797455, 0.0074997334741055965, 0.9823940992355347, 0.943625271320343, 0.9761461615562439, 0.9063552021980286, 0.0591101236641407, 0.019703375175595284, 0.9794111847877502, 0.038689251989126205, 0.07737850397825241, 0.13541238009929657, 0.7350957989692688, 0.9482394456863403, 0.09180430322885513, 0.045902151614427567, 0.826238751411438, 0.1320040374994278, 0.7920241951942444, 0.022000672295689583, 0.022000672295689583, 0.9584137201309204, 0.9763423800468445, 0.9413155913352966, 0.9033598303794861, 0.22257278859615326, 0.7715856432914734, 0.9638385772705078, 0.09972620010375977, 0.036264073103666306, 0.18132035434246063, 0.12692424654960632, 0.34450867772102356, 0.19945240020751953, 0.9674028158187866, 0.09620790928602219, 0.8658711910247803, 0.9754476547241211, 0.8698214888572693, 0.09116799384355545, 0.8660959601402283, 0.9681145548820496, 0.9487001895904541, 0.8903673887252808, 0.9509738683700562, 0.04290107637643814, 0.9777722954750061, 0.11924129724502563, 0.40108436346054077, 0.010840117931365967, 0.18428200483322144, 0.28184306621551514, 0.9668062925338745, 0.22848524153232574, 0.7557588815689087, 0.9790382981300354, 0.013597753830254078, 0.4693923890590668, 0.13746848702430725, 0.1284705549478531, 0.11597341299057007, 0.03249255195260048, 0.05098831281065941, 0.007498281076550484, 0.024994270876049995, 0.014496676623821259, 0.000999770825728774, 0.016996104270219803, 0.17687582969665527, 0.7959412336349487, 0.9446489810943604, 0.9848693013191223, 0.8338806629180908, 0.1334208995103836, 0.02964909002184868, 0.12245690822601318, 0.8571983575820923, 0.9700681567192078, 0.9848995804786682, 0.07180915772914886, 0.1077137365937233, 0.07180915772914886, 0.13165012001991272, 0.5385686755180359, 0.011968192644417286, 0.05984096601605415, 0.9891935586929321, 0.08715595304965973, 0.8674092292785645, 0.04565311595797539, 0.1759548932313919, 0.25593438744544983, 0.19728276133537292, 0.3679056763648987, 0.9124518036842346, 0.9846305847167969, 0.9596601724624634, 0.3902689814567566, 0.29270175099372864, 0.1951344907283783, 0.14496807754039764, 0.3221512734889984, 0.01610756479203701, 0.3382588326931, 0.03221512958407402, 0.11275295168161392, 0.01610756479203701, 0.6649734973907471, 0.2225915491580963, 0.12141357362270355, 0.6475390195846558, 0.26936522126197815, 0.029929470270872116, 0.658448338508606, 0.08903518319129944, 0.2047809213399887, 0.31162315607070923, 0.39175480604171753, 0.9653768539428711, 0.2732342481613159, 0.5464684963226318, 0.07507728040218353, 0.25025758147239685, 0.6506697535514832, 0.9691488146781921, 0.8967599868774414, 0.9020863175392151, 0.1863955557346344, 0.13397179543972015, 0.675683856010437, 0.6996923685073853, 0.09436054527759552, 0.12640753388404846, 0.0017803877126425505, 0.0017803877126425505, 0.055192019790410995, 0.02136465162038803, 0.21065707504749298, 0.030978983268141747, 0.28500664234161377, 0.043370574712753296, 0.2571255564689636, 0.11772013455629349, 0.046468473970890045, 0.9487003684043884, 0.1716306507587433, 0.5148919224739075, 0.06436149030923843, 0.23599214851856232, 0.9521480798721313, 0.06579162180423737, 0.28509703278541565, 0.5263329744338989, 0.10965270549058914, 0.11862140148878098, 0.8303498029708862, 0.9672179818153381, 0.43616124987602234, 0.3904981315135956, 0.04566308856010437, 0.006298357155174017, 0.0015745892887935042, 0.04251391068100929, 0.040939319878816605, 0.012596714310348034, 0.02361883968114853, 0.5187568068504333, 0.18135400116443634, 0.05482795462012291, 0.006326302420347929, 0.05482795462012291, 0.04850165173411369, 0.01687013916671276, 0.014761371538043022, 0.08645946532487869, 0.0189789067953825, 0.08703583478927612, 0.04835323989391327, 0.8655229806900024, 0.966606616973877, 0.8188329339027405, 0.22081632912158966, 0.16623252630233765, 0.060786519199609756, 0.2642352879047394, 0.048381105065345764, 0.007443247362971306, 0.18360009789466858, 0.013645953498780727, 0.03225407004356384, 0.0024810824543237686, 0.29891595244407654, 0.01632734201848507, 0.17457695305347443, 0.2587255537509918, 0.07786885648965836, 0.03642253205180168, 0.03893442824482918, 0.03893442824482918, 0.012559494003653526, 0.04395822808146477, 0.003767848014831543, 0.9739310145378113, 0.01679191365838051, 0.29025033116340637, 0.5805006623268127, 0.9519739747047424, 0.9307103753089905, 0.006793506443500519, 0.013587012887001038, 0.04076103866100311, 0.31726738810539246, 0.4470585882663727, 0.10094871371984482, 0.11536996066570282, 0.042619794607162476, 0.344236820936203, 0.48520997166633606, 0.03278445824980736, 0.019670674577355385, 0.07212580740451813, 0.941709041595459, 0.002546898555010557, 0.7768040895462036, 0.11206353455781937, 0.10442284494638443, 0.2584487199783325, 0.1627269685268402, 0.12443827092647552, 0.134010449051857, 0.3254539370536804, 0.5069974064826965, 0.2824699878692627, 0.021728461608290672, 0.1883133202791214, 0.9867846965789795, 0.9021807312965393, 0.9677295088768005, 0.08586204051971436, 0.9015514254570007, 0.18398992717266083, 0.20698867738246918, 0.597967267036438, 0.5220038890838623, 0.05288027971982956, 0.11407031863927841, 0.08763018250465393, 0.05514657869935036, 0.02719557285308838, 0.03701619803905487, 0.04154879227280617, 0.030972735956311226, 0.021907545626163483, 0.007554325740784407, 0.0022662978153675795, 0.23796558380126953, 0.12964226305484772, 0.16709448397159576, 0.2892463207244873, 0.05416166037321091, 0.019014200195670128, 0.0017285635694861412, 0.029385581612586975, 0.030537957325577736, 0.01498088426887989, 0.025928454473614693, 0.9267364740371704, 0.20296373963356018, 0.0965234711766243, 0.20098038017749786, 0.2895704209804535, 0.07007868587970734, 0.021816948428750038, 0.0013222394045442343, 0.03503934293985367, 0.03173374384641647, 0.01983359083533287, 0.02710590697824955, 0.003305598394945264, 0.966766893863678, 0.966244637966156, 0.9803401231765747, 0.2743494510650635, 0.015241635963320732, 0.6858736276626587, 0.8924694061279297, 0.0037498713936656713, 0.0037498713936656713, 0.08249717205762863, 0.014999485574662685, 0.8967599868774414, 0.9335069060325623, 0.3600848615169525, 0.17104031145572662, 0.4140976071357727, 0.004501061048358679, 0.04050954803824425, 0.011252651922404766, 0.9830975532531738, 0.9829844832420349, 0.9662185311317444, 0.009961015544831753, 0.019922031089663506, 0.9761612415313721, 0.9003762006759644, 0.4047476053237915, 0.06024545058608055, 0.18073634803295135, 0.02884993515908718, 0.23928475379943848, 0.03733520954847336, 0.0008485274738632143, 0.04836606606841087, 0.9769278764724731, 0.9422598481178284, 0.051445912569761276, 0.011432425118982792, 0.45158079266548157, 0.44014838337898254, 0.04572970047593117, 0.9363328218460083, 0.9407986402511597, 0.9770611524581909, 0.7260361313819885, 0.941829264163971, 0.9366239309310913, 0.05960334464907646, 0.1748705357313156, 0.28087252378463745, 0.20856869220733643, 0.21838369965553284, 0.023065244778990746, 0.024046743288636208, 0.011287246830761433, 0.010305747389793396, 0.005398248787969351, 0.007524831220507622, 0.013740996830165386, 0.013413829728960991, 0.0042531657963991165, 0.004416748881340027, 0.47888273000717163, 0.19641675055027008, 0.21699373424053192, 0.02618889883160591, 0.02618889883160591, 0.024318262934684753, 0.011223814450204372, 0.01870635710656643, 0.3838355839252472, 0.11927491426467896, 0.12114956974983215, 0.10123136639595032, 0.027885491028428078, 0.007498619612306356, 0.014059911482036114, 0.032572127878665924, 0.04053940996527672, 0.12443021684885025, 0.019215213134884834, 0.007732951547950506, 0.00023433186288457364, 0.30782315135002136, 0.05841899290680885, 0.09436914324760437, 0.23592285811901093, 0.03145638108253479, 0.006740652956068516, 0.17975075542926788, 0.060665879398584366, 0.006740652956068516, 0.015728190541267395, 0.962348461151123, 0.35720354318618774, 0.3305244445800781, 0.0978233739733696, 0.001482172403484583, 0.12302030622959137, 0.04150082543492317, 0.014821724034845829, 0.014821724034845829, 0.017786068841814995, 0.9638767242431641, 0.003804007777944207, 0.2814965844154358, 0.07417815178632736, 0.24345649778842926, 0.12933626770973206, 0.11602223664522171, 0.05325610935688019, 0.05135410279035568, 0.04564809054136276, 0.8921170830726624, 0.8542282581329346, 0.0544721856713295, 0.02420986071228981, 0.012104930356144905, 0.02420986071228981, 0.14525915682315826, 0.060524649918079376, 0.6657711267471313, 0.9765121936798096, 0.9817541241645813, 0.09701207280158997, 0.026457838714122772, 0.2910362184047699, 0.5820724368095398, 0.9335069060325623, 0.9463112354278564, 0.1372075378894806, 0.8575471043586731, 0.9602873921394348, 0.9907923936843872, 0.9424435496330261, 0.018958089873194695, 0.2938503921031952, 0.6730121374130249, 0.9288419485092163, 0.061108022928237915, 0.08902014791965485, 0.2383929342031479, 0.001508816028945148, 0.010561713017523289, 0.039229217916727066, 0.022632241249084473, 0.5884382724761963, 0.010561713017523289, 0.976771354675293, 0.3906266391277313, 0.6009640693664551, 0.9138568639755249, 0.43072155117988586, 0.21821323037147522, 0.13977056741714478, 0.0014262303011491895, 0.0670328214764595, 0.02281968481838703, 0.05562297999858856, 0.035655755549669266, 0.002852460602298379, 0.00855738203972578, 0.01711476407945156, 0.9213519096374512, 0.9335067272186279, 0.959411084651947, 0.9671154022216797, 0.9828106164932251, 0.9646431803703308, 0.9815155267715454, 0.3749481737613678, 0.09373704344034195, 0.5238246321678162, 0.34181952476501465, 0.5981841683387756, 0.90412437915802, 0.9391865134239197, 0.9391865134239197, 0.9110847115516663, 0.9775820374488831, 0.9860575199127197, 0.007250423077493906, 0.9845265746116638, 0.9848797917366028, 0.9805594086647034, 0.9606176018714905, 0.9548576474189758, 0.9778074026107788, 0.9837571978569031, 0.9293994307518005, 0.984891951084137, 0.9918649792671204, 0.8939158320426941, 0.9467606544494629, 0.773138701915741, 0.0712822899222374, 0.14804783463478088, 0.8910767436027527, 0.9714722037315369, 0.052052900195121765, 0.052052900195121765, 0.7807934880256653, 0.07807935029268265, 0.039551328867673874, 0.9492319226264954, 0.913856565952301, 0.056354112923145294, 0.9016658067703247, 0.28627458214759827, 0.019084971398115158, 0.6679739952087402, 0.0998629555106163, 0.8987665772438049, 0.9819340705871582, 0.9736008048057556, 0.4412148594856262, 0.10181880742311478, 0.011313200928270817, 0.3846488296985626, 0.05656600743532181, 0.8939165472984314, 0.8939165472984314, 0.8755788803100586, 0.008584106341004372, 0.05150463804602623, 0.04292053356766701, 0.017168212682008743, 0.9835100769996643, 0.9765101075172424, 0.822675347328186, 0.07961374521255493, 0.017691943794488907, 0.07961374521255493, 0.18388046324253082, 0.10215581208467484, 0.6946595311164856, 0.7021777629852295, 0.9363328218460083, 0.9856038093566895, 0.10422465205192566, 0.6060470938682556, 0.21230947971343994, 0.003860172349959612, 0.023161035031080246, 0.04632207006216049, 0.17640356719493866, 0.793816089630127, 0.2740218937397003, 0.7242007255554199, 0.9337750673294067, 0.03458426147699356, 0.12371068447828293, 0.41611775755882263, 0.382378488779068, 0.06747855246067047, 0.9297645688056946, 0.9764795899391174, 0.9757188558578491, 0.7892796993255615, 0.06577330827713013, 0.13154661655426025, 0.9445406198501587, 0.12654240429401398, 0.8541612029075623, 0.16733795404434204, 0.01968681812286377, 0.3740495443344116, 0.43310999870300293, 0.9424347877502441, 0.9934447407722473, 0.9764856100082397, 0.9208877682685852, 0.9422598481178284, 0.058651890605688095, 0.20737631618976593, 0.05236775800585747, 0.04817833751440048, 0.16338740289211273, 0.31630125641822815, 0.08169370144605637, 0.07540956884622574, 0.9566540122032166, 0.03801274299621582, 0.9879029393196106, 0.28546279668807983, 0.04078039899468422, 0.6524863839149475, 0.9422598481178284, 0.5608466863632202, 0.11118105798959732, 0.12229916453361511, 0.10129829496145248, 0.028412936255335808, 0.006794397719204426, 0.04323707893490791, 0.006794397719204426, 0.003706035204231739, 0.014824140816926956, 0.2784106731414795, 0.04640178009867668, 0.24360933899879456, 0.02320089004933834, 0.01160044502466917, 0.08120311796665192, 0.31321200728416443, 0.09589944034814835, 0.32765641808509827, 0.18180935084819794, 0.2777087986469269, 0.03196648135781288, 0.025972764939069748, 0.023974860087037086, 0.019979050382971764, 0.00399581016972661, 0.009989525191485882, 0.9127469658851624, 0.078987717628479, 0.9938653111457825, 0.2316928207874298, 0.5406165719032288, 0.2316928207874298, 0.49242067337036133, 0.45302700996398926, 0.039393652230501175, 0.9875070452690125, 0.5061634182929993, 0.11429496854543686, 0.10613103955984116, 0.02040981501340866, 0.00816392619162798, 0.048983559012413025, 0.08980318903923035, 0.06327042728662491, 0.01632785238325596, 0.024491779506206512, 0.4223167300224304, 0.5602160692214966, 0.008618708699941635, 0.6880441308021545, 0.2814725935459137, 0.6928869485855103, 0.23295336961746216, 0.059731632471084595, 0.0059731630608439445, 0.0029865815304219723, 0.9962322115898132, 0.6435093879699707, 0.08244248479604721, 0.0847325548529625, 0.07786234468221664, 0.027480827644467354, 0.029770897701382637, 0.03206096589565277, 0.01832055300474167, 0.987714409828186, 0.1661764234304428, 0.0830882117152214, 0.18279406428337097, 0.5483821630477905, 0.09892169386148453, 0.840834379196167, 0.3662440776824951, 0.5993084907531738, 0.9422598481178284, 0.9422598481178284, 0.9641904830932617, 0.028055554255843163, 0.038846150040626526, 0.6711751818656921, 0.07769230008125305, 0.183440163731575, 0.8910715579986572, 0.21755026280879974, 0.06526508182287216, 0.5656306743621826, 0.13053016364574432, 0.966766893863678, 0.4995805323123932, 0.4995805323123932, 0.45672696828842163, 0.06343430280685425, 0.4694138169288635, 0.9531494379043579, 0.9770776629447937, 0.9865615367889404, 0.9703302383422852, 0.9890353083610535, 0.04503076896071434, 0.0032164836302399635, 0.18333956599235535, 0.6465131640434265, 0.02251538448035717, 0.07076263427734375, 0.012865934520959854, 0.016082417219877243, 0.9298211336135864, 0.019716063514351845, 0.3943212628364563, 0.5323336720466614, 0.019716063514351845, 0.03943212702870369, 0.2562278211116791, 0.329435795545578, 0.3733605444431305, 0.0073207952082157135, 0.0073207952082157135, 0.02196238562464714, 0.0036663857754319906, 0.6012872457504272, 0.3006436228752136, 0.054995786398649216, 0.036663856357336044, 0.0036663857754319906, 0.02810421772301197, 0.18267741799354553, 0.7822340726852417, 0.004684036131948233, 0.9879065752029419, 0.9766359329223633, 0.977333664894104, 0.23334045708179474, 0.07892397791147232, 0.32255885004997253, 0.13382761180400848, 0.02745181880891323, 0.010294431820511818, 0.18873125314712524, 0.003431477351114154, 0.8861579895019531, 0.9251231551170349, 0.9184907078742981, 0.950499951839447, 0.010081443935632706, 0.9879814982414246, 0.9287431240081787, 0.9194062352180481, 0.9194060564041138, 0.13507115840911865, 0.10974530875682831, 0.10130336135625839, 0.6373670101165771, 0.01688389480113983, 0.005331399850547314, 0.10129660367965698, 0.04265119880437851, 0.6610935926437378, 0.039985500276088715, 0.06664250046014786, 0.058645401149988174, 0.021325599402189255, 0.23106765747070312, 0.22653692960739136, 0.15404510498046875, 0.28996726870536804, 0.02718443050980568, 0.009061477147042751, 0.04077664762735367, 0.018122954294085503, 0.9030585289001465, 0.05546684190630913, 0.9152028560638428, 0.9866083264350891, 0.9568156003952026, 0.056858643889427185, 0.7107330560684204, 0.005685864482074976, 0.06823036819696426, 0.011371728964149952, 0.028429321944713593, 0.056858643889427185, 0.06254450976848602, 0.21106654405593872, 0.24713973701000214, 0.15043288469314575, 0.2095315158367157, 0.042980823665857315, 0.02225792594254017, 0.018420353531837463, 0.036073192954063416, 0.029165558516979218, 0.03070058859884739, 0.002302544191479683, 0.5740388035774231, 0.005171521101146936, 0.09825889766216278, 0.07757281512022018, 0.12928801774978638, 0.025857605040073395, 0.08791585266590118, 0.9363328218460083, 0.7821808457374573, 0.0904562920331955, 0.0177365280687809, 0.0532095804810524, 0.01596287451684475, 0.0124155692756176, 0.007094610948115587, 0.010641916655004025, 0.010641916655004025, 0.9541025161743164, 0.6082442998886108, 0.1706051230430603, 0.0608244314789772, 0.1468687504529953, 0.01335170492529869, 0.9764883518218994, 0.04006056860089302, 0.4273127317428589, 0.5207874178886414, 0.94321608543396, 0.9222963452339172, 0.06942015886306763, 0.22983726859092712, 0.17237795889377594, 0.030241746455430984, 0.4778195917606354, 0.07560436427593231, 0.015120873227715492, 0.9905914664268494, 0.1305597722530365, 0.5004791021347046, 0.010879981331527233, 0.16863970458507538, 0.04351992532610893, 0.141439750790596, 0.35331180691719055, 0.13249193131923676, 0.04416397586464882, 0.33859050273895264, 0.11777060478925705, 0.9542064070701599, 0.21543475985527039, 0.15336033701896667, 0.35418936610221863, 0.010954310186207294, 0.1752689629793167, 0.014605746604502201, 0.018257183954119682, 0.03286293148994446, 0.018257183954119682, 0.961874783039093, 0.3804140090942383, 0.1991579234600067, 0.2491338700056076, 0.03132821246981621, 0.04027913138270378, 0.017155924811959267, 0.024615023285150528, 0.027598662301898003, 0.015664106234908104, 0.003729549003764987, 0.00969682727009058, 0.011702604591846466, 0.09362083673477173, 0.23405209183692932, 0.08191823214292526, 0.4446989595890045, 0.11702604591846466, 0.8479781150817871, 0.14132969081401825, 0.20015469193458557, 0.1499030888080597, 0.1677892506122589, 0.28703033924102783, 0.07750670611858368, 0.030661994591355324, 0.0008517220849171281, 0.026403384283185005, 0.024699939414858818, 0.024699939414858818, 0.00936894305050373, 0.05020109564065933, 0.06275136768817902, 0.08157677948474884, 0.012550273910164833, 0.04392595961689949, 0.7467412948608398, 0.14761079847812653, 0.07948274165391922, 0.10219209641218185, 0.6699259281158447, 0.9890203475952148, 0.10247527807950974, 0.6763368248939514, 0.1366337090730667, 0.08198022097349167, 0.2737904191017151, 0.16427424550056458, 0.21903233230113983, 0.01216846238821745, 0.006084231194108725, 0.10343193262815475, 0.06084231287240982, 0.13993732631206512, 0.018252694979310036, 0.7260364890098572, 0.03751988336443901, 0.9505037069320679, 0.9077720642089844, 0.9463112354278564, 0.9363320469856262, 0.9689514636993408, 0.9870278239250183, 0.38196638226509094, 0.2913303077220917, 0.2395382523536682, 0.08416208624839783, 0.8967599868774414, 0.22173354029655457, 0.7301914691925049, 0.030583936721086502, 0.011468975804746151, 0.400593101978302, 0.002472796943038702, 0.09149348735809326, 0.4772498309612274, 0.012363985180854797, 0.004945593886077404, 0.009891187772154808, 0.9077735543251038, 0.9841160178184509, 0.986473798751831, 0.11094319820404053, 0.021132037043571472, 0.15849028527736664, 0.7026402354240417, 0.98847496509552, 0.9841160178184509, 0.9267280101776123, 0.9406159520149231, 0.1467491090297699, 0.807120144367218, 0.05128173530101776, 0.8034138679504395, 0.13675129413604736, 0.5333866477012634, 0.08728145062923431, 0.3588237464427948, 0.019395878538489342, 0.9841403365135193, 0.9070723056793213, 0.07838895916938782, 0.9904922842979431, 0.9448691606521606, 0.037863608449697495, 0.01548965834081173, 0.2771468758583069, 0.04470110684633255, 0.0849321037530899, 0.01341033261269331, 0.15198376774787903, 0.05364133045077324, 0.3441985249519348, 0.02682066522538662, 0.04997490346431732, 0.9245356917381287, 0.015465562231838703, 0.7887436747550964, 0.14692284166812897, 0.03866390511393547, 0.05209217965602875, 0.9463412761688232, 0.9842300415039062, 0.9855782389640808, 0.1737682968378067, 0.007898558862507343, 0.023695675656199455, 0.1263769418001175, 0.1263769418001175, 0.05528990924358368, 0.26065242290496826, 0.21326108276844025, 0.08462464809417725, 0.060446176677942276, 0.8341572284698486, 0.25045713782310486, 0.09967172145843506, 0.05111370235681534, 0.27856966853141785, 0.18912069499492645, 0.04855801537632942, 0.06900350004434586, 0.012778425589203835, 0.23466043174266815, 0.1656426638364792, 0.013803554698824883, 0.013803554698824883, 0.5383386611938477, 0.3675788938999176, 0.13062721490859985, 0.37061673402786255, 0.003037842223420739, 0.003037842223420739, 0.018227051943540573, 0.1002487912774086, 0.8131794929504395, 0.9389427304267883, 0.21510297060012817, 0.08570508658885956, 0.16804920136928558, 0.326015442609787, 0.0033609839156270027, 0.023526886478066444, 0.050414759665727615, 0.06217820197343826, 0.03024885430932045, 0.028568362817168236, 0.006721967831254005, 0.342766672372818, 0.20566000044345856, 0.022851111367344856, 0.04570222273468971, 0.3656177818775177, 0.9895378947257996, 0.9430180788040161, 0.022452810779213905, 0.8939064145088196, 0.05963997542858124, 0.8349596858024597, 0.966766893863678, 0.21545185148715973, 0.56496262550354, 0.040696460753679276, 0.17714929580688477, 0.9653086066246033, 0.800408661365509, 0.15207764506340027, 0.04002043232321739, 0.9535481333732605, 0.9247632622718811, 0.9315147399902344, 0.07432698458433151, 0.03303421288728714, 0.43770334124565125, 0.36337634921073914, 0.09084408730268478, 0.024526920169591904, 0.12263459712266922, 0.6213486194610596, 0.10628332197666168, 0.03270256146788597, 0.08993203938007355, 0.9666745662689209, 0.10843551903963089, 0.10843551903963089, 0.7590486407279968, 0.9671929478645325, 0.9697807431221008, 0.11280997842550278, 0.16568966209888458, 0.07403155416250229, 0.39130961894989014, 0.12338592112064362, 0.12338592112064362, 0.007050623651593924, 0.8910715579986572, 0.053607601672410965, 0.09381330013275146, 0.19030699133872986, 0.4583449959754944, 0.045566461980342865, 0.06968988478183746, 0.018762661144137383, 0.013401900418102741, 0.018762661144137383, 0.01608228124678135, 0.021443041041493416, 0.962348461151123, 0.9131210446357727, 0.9440321326255798, 0.933687150478363, 0.132389098405838, 0.13057555258274078, 0.1378297507762909, 0.2756595015525818, 0.027203239500522614, 0.056220028549432755, 0.009067746810615063, 0.06891487538814545, 0.14689749479293823, 0.016321944072842598, 0.13844676315784454, 0.5768615007400513, 0.011537229642271996, 0.24805043637752533, 0.02307445928454399, 0.21183449029922485, 0.01629495993256569, 0.37478408217430115, 0.39107903838157654, 0.06514298915863037, 0.9120018482208252, 0.05186401307582855, 0.07779601961374283, 0.15559203922748566, 0.4408440887928009, 0.16207504272460938, 0.1037280261516571, 0.307585746049881, 0.615171492099762, 0.9065523743629456, 0.9541022777557373, 0.9787367582321167, 0.9293992519378662, 0.27951347827911377, 0.2091323882341385, 0.2131541669368744, 0.06434842199087143, 0.04423954337835312, 0.16489283740520477, 0.010054441168904305, 0.014076218008995056, 0.1036248579621315, 0.3147125244140625, 0.01535183098167181, 0.15351830422878265, 0.17654605209827423, 0.18038401007652283, 0.023027746006846428, 0.03070366196334362, 0.057245805859565735, 0.9016214609146118, 0.014311451464891434, 0.014311451464891434, 0.18633413314819336, 0.13597355782985687, 0.3525240421295166, 0.08057691901922226, 0.24173076450824738, 0.8967599868774414, 0.664973258972168, 0.9420953989028931, 0.9219532608985901, 0.0768294408917427, 0.8824979066848755, 0.9769455194473267, 0.3561936318874359, 0.15182022750377655, 0.2218911051750183, 0.15765947103500366, 0.11094555258750916, 0.9743556976318359, 0.9184912443161011, 0.03955874964594841, 0.9494099617004395, 0.06704255193471909, 0.3016915023326874, 0.5698617100715637, 0.03352127596735954, 0.9766808152198792, 0.9850655198097229, 0.9445785880088806, 0.924764096736908, 0.06273162364959717, 0.04530617222189903, 0.06970180571079254, 0.0400785394012928, 0.04356362670660019, 0.12720578908920288, 0.5419315099716187, 0.015682905912399292, 0.052276354283094406, 0.001742545166052878, 0.9363203048706055, 0.0511966198682785, 0.27987486124038696, 0.0511966198682785, 0.290114164352417, 0.006826215889304876, 0.3105928301811218, 0.01023932360112667, 0.8775938153266907, 0.10637500882148743, 0.011397321708500385, 0.9448652863502502, 0.912585973739624, 0.9755949378013611, 0.036655548959970474, 0.9530442953109741, 0.024788592010736465, 0.7436577677726746, 0.01652572862803936, 0.20657159388065338, 0.9784659743309021, 0.7260363698005676, 0.2091815173625946, 0.38765749335289, 0.1007525622844696, 0.17655687034130096, 0.002878644736483693, 0.010555030778050423, 0.09883346408605576, 0.004797741305083036, 0.0009595482260920107, 0.002878644736483693, 0.005757289472967386, 0.0009595482260920107, 0.984571099281311, 0.9650784730911255, 0.9800822138786316, 0.00993653666228056, 0.9439709782600403, 0.03974614664912224, 0.9786804914474487, 0.00782944355159998, 0.9430737495422363, 0.9563928842544556, 0.9718229174613953, 0.9418289661407471, 0.021529793739318848, 0.11626088619232178, 0.6932593584060669, 0.06028342247009277, 0.06028342247009277, 0.043059587478637695, 0.15010610222816467, 0.8405941724777222, 0.9535295367240906, 0.8903669118881226, 0.12074757367372513, 0.03622427210211754, 0.22137054800987244, 0.07244854420423508, 0.05634886771440506, 0.004024919122457504, 0.05634886771440506, 0.43066635727882385, 0.0524427555501461, 0.3758397698402405, 0.1013893261551857, 0.29018324613571167, 0.07167176902294159, 0.02622137777507305, 0.008740459568798542, 0.07167176902294159, 0.9570713639259338, 0.6386017799377441, 0.10234002768993378, 0.0695912167429924, 0.17602485418319702, 0.012280803173780441, 0.9778826236724854, 0.7964935302734375, 0.977577805519104, 0.8512074947357178, 0.35744550824165344, 0.09722518175840378, 0.23162469267845154, 0.07148910313844681, 0.03717433288693428, 0.20302905142307281, 0.23001199960708618, 0.027060234919190407, 0.1488312929868698, 0.5817950367927551, 0.9430674910545349, 0.9781476259231567, 0.9420953989028931, 0.9319679141044617, 0.8800651431083679, 0.07530154287815094, 0.19363252818584442, 0.7207432985305786, 0.22488659620285034, 0.1957346349954605, 0.20989416539669037, 0.08995463699102402, 0.04331149160861969, 0.09578503668308258, 0.0008329133270308375, 0.011660786345601082, 0.01582535356283188, 0.06663306802511215, 0.0066633066162467, 0.03914692625403404, 0.10640779137611389, 0.042563118040561676, 0.844168484210968, 0.98011714220047, 0.8903663754463196, 0.2780902087688446, 0.13086599111557007, 0.10632861405611038, 0.4089561998844147, 0.06543299555778503, 0.9862890243530273, 0.12332156300544739, 0.3391343057155609, 0.5241166353225708, 0.9910439252853394, 0.978650689125061, 0.9021806120872498, 0.45259326696395874, 0.09991021454334259, 0.11090033501386642, 0.25277283787727356, 0.03496857360005379, 0.015985634177923203, 0.0029973064083606005, 0.020981144160032272, 0.007992817088961601, 0.3271494209766388, 0.27262452244758606, 0.12090304493904114, 0.0829726830124855, 0.07349009066820145, 0.061636846512556076, 0.0165945366024971, 0.014223888516426086, 0.028447777032852173, 0.9588002562522888, 0.9600433111190796, 0.9623745679855347, 0.9829960465431213, 0.9788088202476501, 0.9327080845832825, 0.7126492857933044, 0.0985335037112236, 0.04582953825592995, 0.01833181455731392, 0.00229147681966424, 0.05041249096393585, 0.02062329091131687, 0.03666362911462784, 0.016040338203310966, 0.928813636302948, 0.02191370353102684, 0.08765481412410736, 0.09861166775226593, 0.7450659275054932, 0.047479692846536636, 0.975153923034668, 0.9879677891731262, 0.8220379948616028, 0.15391762554645538, 0.8337204456329346, 0.9547407627105713, 0.9780598282814026, 0.752975344657898, 0.0036730505526065826, 0.11753761768341064, 0.12121066451072693, 0.886223554611206, 0.2054152488708496, 0.7531892657279968, 0.922476053237915, 0.11069260537624359, 0.7610116600990295, 0.12452918291091919, 0.8967599868774414, 0.6649731993675232, 0.010057794861495495, 0.035202279686927795, 0.744276762008667, 0.14583802223205566, 0.010057794861495495, 0.05028897151350975, 0.9731956720352173, 0.8672293424606323, 0.012388990260660648, 0.11150091141462326, 0.975057065486908, 0.9656854271888733, 0.9906671643257141, 0.8903666734695435, 0.9251040816307068, 0.09192746132612228, 0.15694932639598846, 0.5044800043106079, 0.1367701292037964, 0.044842664152383804, 0.04932693392038345, 0.011210666038095951, 0.8016186356544495, 0.09775837510824203, 0.019551673904061317, 0.07820669561624527, 0.9623745679855347, 0.21831457316875458, 0.6549437046051025, 0.3175777196884155, 0.028870699927210808, 0.28870701789855957, 0.005774140357971191, 0.11548279970884323, 0.017322421073913574, 0.22519145905971527, 0.951096773147583, 0.9393011927604675, 0.9010568857192993, 0.3803790807723999, 0.5325307250022888, 0.0988597646355629, 0.8897378444671631, 0.03351719677448273, 0.9719987511634827, 0.19711260497570038, 0.14783445000648499, 0.6406159996986389, 0.9841673970222473, 0.9937746524810791, 0.9466315507888794, 0.029352916404604912, 0.022014686837792397, 0.9778828620910645, 0.7260364890098572, 0.9493367671966553, 0.9766808748245239, 0.11257535219192505, 0.10202141106128693, 0.1758989840745926, 0.2603304982185364, 0.0035179797559976578, 0.14423716068267822, 0.05276969447731972, 0.0457337349653244, 0.021107878535985947, 0.010553939267992973, 0.07035959511995316, 0.9772851467132568, 0.9523922204971313, 0.026827950030565262, 0.013413975015282631, 0.9865913391113281, 0.9478301405906677, 0.9668965339660645, 0.026785923168063164, 0.7433093786239624, 0.22768034040927887, 0.9899459481239319, 0.8048456311225891, 0.12035074830055237, 0.007521921768784523, 0.060175374150276184, 0.962782084941864, 0.19506435096263885, 0.03546624630689621, 0.46106117963790894, 0.3014630973339081, 0.11625029891729355, 0.8788522481918335, 0.9689538478851318, 0.9730552434921265, 0.9775192737579346, 0.11034286022186279, 0.8441228866577148, 0.030344286933541298, 0.013792857527732849, 0.8967599868774414, 0.9669458866119385, 0.028231997042894363, 0.19718949496746063, 0.16581843793392181, 0.01792631670832634, 0.6139763593673706, 0.39489248394966125, 0.08313526213169098, 0.26788029074668884, 0.12470288574695587, 0.10622838884592056, 0.01385587640106678, 0.009237251244485378, 0.9781478643417358, 0.26895657181739807, 0.05763355270028114, 0.6483774781227112, 0.004802796058356762, 0.024013979360461235, 0.272510826587677, 0.6812770366668701, 0.034063853323459625, 0.9318828582763672, 0.9138570427894592, 0.03956599906086922, 0.09608884900808334, 0.3956599831581116, 0.07913199812173843, 0.10174113512039185, 0.09043657034635544, 0.028261426836252213, 0.16391627490520477, 0.8967599868774414, 0.2844894826412201, 0.6756625771522522, 0.9653394818305969, 0.9704843163490295, 0.9591816663742065, 0.042216114699840546, 0.25329670310020447, 0.6624683141708374, 0.0032473935279995203, 0.006494787055999041, 0.006494787055999041, 0.016236968338489532, 0.012989574111998081, 0.9218410849571228, 0.0076501332223415375, 0.015300266444683075, 0.015300266444683075, 0.03825066611170769, 0.9315147995948792, 0.9616551399230957, 0.389280766248703, 0.32058414816856384, 0.28623583912849426, 0.9021804332733154, 0.6710925102233887, 0.8939165472984314, 0.8659487366676331, 0.21807537972927094, 0.1944996565580368, 0.17092394828796387, 0.0058939289301633835, 0.0530453622341156, 0.3182721734046936, 0.029469644650816917, 0.9883745312690735, 0.9360032677650452, 0.7977095246315002, 0.03358776867389679, 0.15954190492630005, 0.9623085856437683, 0.9363811612129211, 0.10910583287477493, 0.8728466629981995, 0.913856565952301, 0.42517587542533875, 0.09743613749742508, 0.43403372168540955, 0.044289153069257736, 0.8622116446495056, 0.24642936885356903, 0.07467556744813919, 0.16055245697498322, 0.3117704689502716, 0.056006673723459244, 0.08027622848749161, 0.052272893488407135, 0.016802001744508743, 0.001866889069788158, 0.9772729873657227, 0.9935277104377747, 0.17645838856697083, 0.17645838856697083, 0.14809900522232056, 0.10398440062999725, 0.10083336383104324, 0.0850781500339508, 0.15440107882022858, 0.05041668191552162, 0.8951789140701294, 0.8881808519363403, 0.24573159217834473, 0.08672879636287689, 0.6649207472801208, 0.04830363765358925, 0.01449109148234129, 0.3284647464752197, 0.004830363672226667, 0.004830363672226667, 0.5893043875694275, 0.009660727344453335, 0.07441014051437378, 0.8929216861724854, 0.8939064145088196, 0.5678302049636841, 0.37855347990989685, 0.2301788479089737, 0.12862935662269592, 0.1760191172361374, 0.10154949128627777, 0.34526827931404114, 0.013539931736886501, 0.3605073094367981, 0.6351795196533203, 0.8367846012115479, 0.15214265882968903, 0.9363328218460083, 0.2777117192745209, 0.11901931464672089, 0.019836552441120148, 0.039673104882240295, 0.5157503485679626, 0.9926151633262634, 0.1398235410451889, 0.24219433963298798, 0.37452733516693115, 0.02496848814189434, 0.039949581027030945, 0.017477942630648613, 0.15979832410812378, 0.8573039770126343, 0.5106121897697449, 0.19716708362102509, 0.22750048339366913, 0.06066679581999779, 0.4508442282676697, 0.1816350817680359, 0.11027844250202179, 0.09730450809001923, 0.061626188457012177, 0.016217418015003204, 0.08108709007501602, 0.02915956825017929, 0.9331061840057373, 0.11721522361040115, 0.07032913714647293, 0.7970635294914246, 0.27948519587516785, 0.1746782511472702, 0.4454295337200165, 0.04366956278681755, 0.05240347608923912, 0.04513484239578247, 0.3272276222705841, 0.41185542941093445, 0.011283710598945618, 0.0677022635936737, 0.1297626793384552, 0.09860051423311234, 0.05688491091132164, 0.14031611382961273, 0.5347181558609009, 0.022753965109586716, 0.01896163821220398, 0.06446956843137741, 0.060677241533994675, 0.31973379850387573, 0.04263117164373398, 0.22381365299224854, 0.07815714925527573, 0.06749935448169708, 0.04263117164373398, 0.19894547760486603, 0.024868184700608253, 0.9712055921554565, 0.4371384382247925, 0.4371384382247925, 0.015479812398552895, 0.6965915560722351, 0.015479812398552895, 0.025799687951803207, 0.24251706898212433, 0.24165168404579163, 0.3486688733100891, 0.18296484649181366, 0.010356500744819641, 0.09666067361831665, 0.12082584202289581, 0.9389801025390625, 0.07752463221549988, 0.21235007047653198, 0.198867529630661, 0.43144142627716064, 0.04044763371348381, 0.03707699850201607, 0.9712351560592651, 0.8671936392784119, 0.10511438548564911, 0.9664556384086609, 0.1382634937763214, 0.30417969822883606, 0.013826349750161171, 0.5392276644706726, 0.22668971121311188, 0.6045058965682983, 0.15112647414207458, 0.9722096920013428, 0.008101747371256351, 0.016203494742512703, 0.946972131729126, 0.008306773379445076, 0.04153386503458023, 0.17597754299640656, 0.23463670909404755, 0.26983222365379333, 0.005865918006747961, 0.12318427860736847, 0.14664794504642487, 0.03519550710916519, 0.18245086073875427, 0.19648554921150208, 0.07719074934720993, 0.3368323743343353, 0.07017341256141663, 0.038595374673604965, 0.07719074934720993, 0.021052023395895958, 0.2429969310760498, 0.15636323392391205, 0.1479111760854721, 0.2440534383058548, 0.07395558804273605, 0.06550351530313492, 0.01901715062558651, 0.005282541736960411, 0.004226033575832844, 0.021130166947841644, 0.02007365971803665, 0.9381966590881348, 0.0836305022239685, 0.9094816446304321, 0.7950243353843689, 0.9363220930099487, 0.9432156682014465, 0.9751121401786804, 0.94870924949646, 0.9618373513221741, 0.016030622646212578, 0.9679433703422546, 0.2317839115858078, 0.03311198949813843, 0.6953517198562622, 0.8542868494987488, 0.03533044829964638, 0.8832611441612244, 0.07066089659929276, 0.8967599868774414, 0.9363328218460083, 0.9846466183662415, 0.966766893863678, 0.8862233757972717, 0.9167863726615906, 0.061119090765714645, 0.07740246504545212, 0.8514271378517151, 0.48864418268203735, 0.17483599483966827, 0.05827866494655609, 0.03586379438638687, 0.0851765125989914, 0.07621056586503983, 0.07621056586503983, 0.30766066908836365, 0.4135766327381134, 0.03530532121658325, 0.05043617635965347, 0.045392557978630066, 0.02017446979880333, 0.12609043717384338, 0.012997668236494064, 0.9748251438140869, 0.9771826863288879, 0.1899884045124054, 0.14927661418914795, 0.5021122097969055, 0.14927661418914795, 0.9713805913925171, 0.9662129878997803, 0.9756641983985901, 0.07894950360059738, 0.9079192280769348, 0.013158249668776989, 0.9919456839561462, 0.2555428147315979, 0.5110856294631958, 0.21903669834136963, 0.9320200085639954, 0.09279335290193558, 0.027838004752993584, 0.05567600950598717, 0.3247767388820648, 0.44540807604789734, 0.05567600950598717, 0.06406427174806595, 0.9243558645248413, 0.31014642119407654, 0.14097565412521362, 0.1691707819700241, 0.35243913531303406, 0.014097564853727818, 0.960304856300354, 0.06111788749694824, 0.9167683124542236, 0.0592595599591732, 0.8888933658599854, 0.0592595599591732, 0.3211899697780609, 0.6555795073509216, 0.021999312564730644, 0.9766808152198792, 0.481905996799469, 0.07529781013727188, 0.4367273151874542, 0.9369168281555176, 0.9681659936904907, 0.031861525028944016, 0.031861525028944016, 0.892122745513916, 0.22008945047855377, 0.4063189625740051, 0.3555290997028351, 0.5599691867828369, 0.42308783531188965, 0.014957278035581112, 0.05982911214232445, 0.9123939275741577, 0.9945178627967834, 0.03427879512310028, 0.05082717910408974, 0.47162893414497375, 0.004728109575808048, 0.07564975321292877, 0.007092164363712072, 0.02718663029372692, 0.004728109575808048, 0.3250575363636017, 0.12693966925144196, 0.017630508169531822, 0.786320686340332, 0.04231322184205055, 0.010578305460512638, 0.014104407280683517, 0.16925853490829468, 0.2801520526409149, 0.13423952460289001, 0.08754751831293106, 0.26847904920578003, 0.017509503290057182, 0.04085550829768181, 0.12475326657295227, 0.6819844841957092, 0.16633768379688263, 0.016633767634630203, 0.9763420224189758, 0.909335196018219, 0.8910778164863586, 0.9911538362503052, 0.1337181180715561, 0.030390482395887375, 0.6382001638412476, 0.03646858036518097, 0.08509334921836853, 0.07293716073036194, 0.9787790179252625, 0.9335065484046936, 0.9328683614730835, 0.922072172164917, 0.8538278937339783, 0.9448433518409729, 0.05249129608273506, 0.9755340814590454, 0.003104151226580143, 0.9343494772911072, 0.06208302453160286, 0.07191161811351776, 0.8988952040672302, 0.8808701038360596, 0.09272316843271255, 0.9440492391586304, 0.2590351104736328, 0.33753061294555664, 0.09942761808633804, 0.08634503930807114, 0.018315615132451057, 0.08634503930807114, 0.015699097886681557, 0.09681110829114914, 0.9772310853004456, 0.3092982769012451, 0.18374155461788177, 0.483852744102478, 0.02143651433289051, 0.9266400337219238, 0.06974709779024124, 0.9951930642127991, 0.9901196360588074, 0.1413300633430481, 0.8479803800582886, 0.11447665840387344, 0.7611693739891052, 0.03815888613462448, 0.002008362440392375, 0.08234286308288574, 0.47999510169029236, 0.11233928054571152, 0.010212661698460579, 0.37786850333213806, 0.9912247061729431, 0.9265545010566711, 0.8910728096961975, 0.9279454946517944, 0.0647403821349144, 0.9661292433738708, 0.7601484060287476, 0.08314123004674911, 0.015836425125598907, 0.055427487939596176, 0.0514683797955513, 0.027713743969798088, 0.003959106281399727, 0.9915061593055725, 0.733174204826355, 0.14663484692573547, 0.009261148050427437, 0.0015435246750712395, 0.10959024727344513, 0.8472988605499268, 0.1508888304233551, 0.1266613006591797, 0.18603378534317017, 0.6135156750679016, 0.07124698162078857, 0.9664796590805054, 0.9880101680755615, 0.9796162247657776, 0.19899405539035797, 0.35745227336883545, 0.11362314969301224, 0.13204853236675262, 0.06018956005573273, 0.0018425375455990434, 0.05036269500851631, 0.05589030683040619, 0.007984329015016556, 0.008598508313298225, 0.012283584102988243, 0.9362456798553467, 0.9790884256362915, 0.8050411939620972, 0.015541336499154568, 0.14608855545520782, 0.02175787091255188, 0.009324802085757256, 0.6649733781814575, 0.6018085479736328, 0.12669654190540314, 0.012669653631746769, 0.031674135476350784, 0.16470550000667572, 0.06334827095270157, 0.144974023103714, 0.2532213032245636, 0.17783480882644653, 0.09858233481645584, 0.08118545264005661, 0.14110805094242096, 0.0057989610359072685, 0.023195844143629074, 0.04639168828725815, 0.003865974023938179, 0.003865974023938179, 0.019329870119690895, 0.9813302755355835, 0.9699528217315674, 0.822282075881958, 0.16819407045841217, 0.8939164876937866, 0.10657264292240143, 0.28837302327156067, 0.21941426396369934, 0.04388285428285599, 0.28837302327156067, 0.05015183240175247, 0.009701070375740528, 0.9701070189476013, 0.01616845093667507, 0.9927483797073364, 0.9922491908073425, 0.9637597799301147, 0.03493168205022812, 0.9431554079055786, 0.962431013584137, 0.9813421964645386, 0.9050720930099487, 0.9636221528053284, 0.03401019424200058, 0.7950237989425659, 0.9547459483146667, 0.05618023872375488, 0.9377778768539429, 0.18182452023029327, 0.32728415727615356, 0.49092620611190796, 0.3897646963596344, 0.32342174649238586, 0.2819574177265167, 0.24363325536251068, 0.5226026177406311, 0.014878367073833942, 0.013018570840358734, 0.0781114250421524, 0.04835469275712967, 0.0390557125210762, 0.029756734147667885, 0.0111587755382061, 0.9890909194946289, 0.6133847236633301, 0.9865856170654297, 0.01363943237811327, 0.9450101256370544, 0.9335062503814697, 0.9125862717628479, 0.06509089469909668, 0.028477264568209648, 0.8787270188331604, 0.0040681809186935425, 0.020340904593467712, 0.9960755705833435, 0.003425294067710638, 0.6649734377861023, 0.9891673922538757, 0.3780461549758911, 0.14022482931613922, 0.11891066282987595, 0.17387880384922028, 0.04038475453853607, 0.057211734354496, 0.06506432592868805, 0.010096188634634018, 0.015705181285738945, 0.4105834364891052, 0.1368611454963684, 0.13579192757606506, 0.1785610318183899, 0.03314606100320816, 0.023523010313510895, 0.039561424404382706, 0.008553821593523026, 0.014969187788665295, 0.01603841595351696, 0.7411071062088013, 0.08196576684713364, 0.011953340843319893, 0.08196576684713364, 0.05293622240424156, 0.029029540717601776, 0.34014686942100525, 0.13657411932945251, 0.09792106598615646, 0.18553465604782104, 0.02061496116220951, 0.037364616990089417, 0.03092244267463684, 0.06957549601793289, 0.01417278591543436, 0.04122992232441902, 0.02061496116220951, 0.006442175712436438, 0.9714720845222473, 0.21566948294639587, 0.4631590247154236, 0.1502615213394165, 0.014142260886728764, 0.01237447839230299, 0.05480125918984413, 0.037123434245586395, 0.03535565361380577, 0.015910042449831963, 0.02550148405134678, 0.07905460149049759, 0.545731782913208, 0.058653414249420166, 0.1861608326435089, 0.06120356172323227, 0.040802374482154846, 0.9939610362052917, 0.6507322192192078, 0.3408597409725189, 0.9252159595489502, 0.7734068036079407, 0.08631772547960281, 0.10012856125831604, 0.03797980025410652, 0.980812132358551, 0.850125789642334, 0.09380698949098587, 0.052766431123018265, 0.07668589055538177, 0.9202306866645813, 0.9761608839035034, 0.021454086527228355, 0.9663324356079102, 0.9414346814155579, 0.13211974501609802, 0.3170873820781708, 0.0792718455195427, 0.3787432610988617, 0.08807983249425888, 0.1748579740524292, 0.5070881247520447, 0.00874289870262146, 0.00874289870262146, 0.29725855588912964, 0.9138568639755249, 0.3168981969356537, 0.6790675520896912, 0.18581748008728027, 0.7804334163665771, 0.004645437002182007, 0.02787262201309204, 0.9363215565681458, 0.44214850664138794, 0.5305781960487366, 0.0470423623919487, 0.9408472180366516, 0.015680788084864616, 0.9969519972801208, 0.5041719079017639, 0.13216157257556915, 0.3573257327079773, 0.022480985149741173, 0.11240492761135101, 0.014987323433160782, 0.037468310445547104, 0.08992394059896469, 0.254784494638443, 0.41215139627456665, 0.014987323433160782, 0.029974646866321564, 0.23183707892894745, 0.01931975595653057, 0.40571489930152893, 0.32843586802482605, 0.6649734973907471, 0.9288141131401062, 0.985770046710968, 0.8823849558830261, 0.08346884697675705, 0.029810301959514618, 0.005962060298770666, 0.15288032591342926, 0.10143021494150162, 0.13524028658866882, 0.40131083130836487, 0.033810071647167206, 0.036750078201293945, 0.13818028569221497, 0.9125862717628479, 0.9348093867301941, 0.9841809868812561, 0.298629492521286, 0.321601003408432, 0.06432019919157028, 0.11026319861412048, 0.00918859988451004, 0.0459429994225502, 0.14701759815216064, 0.10137671232223511, 0.00506883580237627, 0.7349811792373657, 0.12672089040279388, 0.025344178080558777, 0.9776135087013245, 0.03355115279555321, 0.7800642848014832, 0.13420461118221283, 0.041938938200473785, 0.9297881126403809, 0.9902029633522034, 0.8412351608276367, 0.051294825971126556, 0.07181275635957718, 0.03663916140794754, 0.09825575351715088, 0.05458652973175049, 0.30131766200065613, 0.4607103168964386, 0.04366922378540039, 0.0021834613289684057, 0.03711884096264839, 0.9948720932006836, 0.8873144388198853, 0.9522134065628052, 0.9430750608444214, 0.9288134574890137, 0.08752444386482239, 0.21023912727832794, 0.4502546191215515, 0.07850130647420883, 0.047822631895542145, 0.07489205151796341, 0.012632394209504128, 0.03067867085337639, 0.004511569160968065, 0.0018046277109533548, 0.20002512633800507, 0.7875989079475403, 0.9965312480926514, 0.8921167254447937, 0.6152114272117615, 0.09351213276386261, 0.2657713294029236, 0.019686764106154442, 0.9968883991241455, 0.966766893863678, 0.8951779007911682], \"Term\": [\"abandon\", \"abnormal\", \"abnormal\", \"academy\", \"academy\", \"accident\", \"accident\", \"account\", \"account\", \"account\", \"account\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"achieved\", \"achieved\", \"achieved\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"acknowledgement\", \"active\", \"active\", \"active\", \"active\", \"activity\", \"activity\", \"activity\", \"activity\", \"age\", \"age\", \"age\", \"age\", \"age\", \"agree\", \"agree\", \"air\", \"air\", \"air\", \"air_area_name\", \"alabama\", \"alexnet\", \"along\", \"along\", \"along\", \"alphapy\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"amazonaws\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"angeles\", \"angina\", \"anime\", \"answered\", \"answered\", \"appearance\", \"appointee\", \"arabic\", \"arb\", \"architecture\", \"architecture\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"arm\", \"arora\", \"array\", \"art\", \"art\", \"art\", \"article\", \"article\", \"article\", \"article\", \"artist\", \"asset\", \"asx\", \"athlete\", \"attainment\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribution\", \"attribution\", \"auction\", \"audio\", \"audio\", \"audio\", \"audio\", \"australian\", \"australian\", \"automl\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"ba\", \"back\", \"back\", \"back\", \"back\", \"back\", \"badminton\", \"bandwidth\", \"bari\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"beane\", \"becoming\", \"behave\", \"behind\", \"behind\", \"behind\", \"beneficial\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benign\", \"betting\", \"betting\", \"betting\", \"bias\", \"bias\", \"bias\", \"bias\", \"bid\", \"bike\", \"bing\", \"binghamton\", \"birth\", \"birth\", \"bitcoin\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"bone\", \"boston\", \"boston\", \"bottle\", \"boulder\", \"breast\", \"breast\", \"broad\", \"broadcast\", \"buffalo\", \"bureau\", \"bureau\", \"cache_dir\", \"calculated\", \"calculated\", \"calculated\", \"calculated\", \"calculated\", \"calculating\", \"california\", \"california\", \"campaign\", \"campaign\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"can\", \"cancer\", \"cancer\", \"candy\", \"caput\", \"car\", \"car\", \"car\", \"carbon\", \"carbon\", \"carolina\", \"catalog\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cell\", \"census\", \"census\", \"census\", \"center\", \"center\", \"center\", \"center\", \"centimetre\", \"centroid\", \"cervical\", \"champion\", \"champion\", \"champion\", \"chance\", \"chance\", \"chance\", \"chance\", \"chance\", \"chance\", \"chance\", \"cheltenham\", \"chest\", \"chest\", \"chest\", \"chicago\", \"chicago\", \"chicago\", \"child\", \"child\", \"child\", \"child\", \"chinese\", \"chocolate\", \"chocolate\", \"choose\", \"choose\", \"choose\", \"churn\", \"cikm\", \"circumstance\", \"citation\", \"citation\", \"citation\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classical\", \"click\", \"click\", \"click\", \"click\", \"clicked\", \"close\", \"close\", \"close\", \"close\", \"closed\", \"closed\", \"coco\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"collected\", \"college\", \"college\", \"college\", \"colorado\", \"columbus\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"combine\", \"combine\", \"comic\", \"comic\", \"command\", \"comment\", \"comment\", \"comment\", \"comment\", \"communication\", \"communication\", \"communication\", \"communication\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"commuter\", \"company\", \"company\", \"company\", \"company\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"complete\", \"complete\", \"complete\", \"complete\", \"component\", \"compress\", \"compute\", \"computing\", \"computing\", \"conference\", \"conference\", \"conference\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"contestant\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"convnet\", \"convolution\", \"convolutional\", \"copy\", \"copy\", \"copy\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"coskun\", \"costume\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"court\", \"cp\", \"crime\", \"crime\", \"crime\", \"critical\", \"cryptocurrencies\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"cuisine\", \"curr_facility\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"cytology\", \"daca\", \"dado\", \"dairy\", \"dakota\", \"damage\", \"damage\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets_dir\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"datos\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"daylight\", \"dayton\", \"de\", \"de\", \"de\", \"de\", \"de\", \"de\", \"de\", \"debate\", \"debt\", \"deep\", \"deep\", \"deep\", \"deep\", \"definite\", \"degenerating\", \"denotes\", \"denotes\", \"denver\", \"depodesta\", \"deposit\", \"depth\", \"depth\", \"depth\", \"describing\", \"describing\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"diabetes\", \"diagnosis\", \"diagnosis\", \"diciembre\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dioxide\", \"diploma\", \"disability\", \"disagree\", \"disc\", \"disclosure\", \"disk\", \"district\", \"district\", \"district\", \"divide\", \"divide\", \"dna\", \"dnn\", \"dnns\", \"documento\", \"doesn\", \"dog\", \"dog\", \"donation\", \"driving\", \"dry\", \"dsads\", \"dub\", \"dumbbell\", \"duplicate\", \"dutch\", \"earthquake\", \"easy\", \"eaxmple\", \"echo\", \"education\", \"education\", \"education\", \"edx\", \"effected\", \"el\", \"el\", \"el\", \"el\", \"election\", \"election\", \"eleitorais\", \"elementary\", \"elementary\", \"else\", \"else\", \"else\", \"emotion\", \"emotion\", \"employer\", \"employment\", \"en\", \"en\", \"en\", \"en\", \"en\", \"enade\", \"enem\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"enforcement\", \"engagement\", \"english\", \"english\", \"english\", \"english\", \"enjoy\", \"enjoy\", \"enjoy\", \"enru\", \"epithelial\", \"equity\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"exam\", \"exam\", \"exchange\", \"exchange\", \"exercise\", \"exercise\", \"experience\", \"experience\", \"experience\", \"experience\", \"exposure\", \"expression\", \"extension\", \"facebook\", \"facebook\", \"facebook\", \"faceoffs\", \"facility\", \"facility\", \"family\", \"family\", \"family\", \"family\", \"fashion\", \"fasttext\", \"fatality\", \"fbi\", \"fcity\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"federal\", \"federal\", \"felt\", \"fewer\", \"fewer\", \"fewer\", \"fieldnames\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"final\", \"final\", \"final\", \"final\", \"final\", \"final\", \"final\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fire\", \"fire\", \"flare\", \"flavor\", \"flavor\", \"flavor\", \"float\", \"float\", \"float\", \"florida\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"food\", \"food\", \"food\", \"forecasting\", \"forecasting\", \"format\", \"format\", \"format\", \"format\", \"format\", \"formula\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"fr\", \"frame\", \"frame\", \"frame\", \"frame\", \"francisco\", \"francisco\", \"frequent\", \"frequent\", \"fstreet\", \"fzip\", \"galaxy\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gann\", \"gas\", \"gas\", \"gas\", \"gas\", \"generalise\", \"genome\", \"genome\", \"genre\", \"genre\", \"genre\", \"geocode\", \"geocoded\", \"geocoder\", \"georgia\", \"gesture\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"giant\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"google\", \"google\", \"google\", \"google\", \"google\", \"google\", \"gov\", \"gov\", \"gov\", \"gov\", \"gov\", \"gov\", \"government\", \"government\", \"government\", \"government\", \"grabbed\", \"gram\", \"greatest\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"grouping\", \"guest\", \"halfway\", \"halloween\", \"hand\", \"hand\", \"har\", \"hashtag\", \"hashtags\", \"health\", \"health\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hitter\", \"horizontal\", \"horizontal\", \"hot\", \"hotel\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hyperplanes\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"illinois\", \"image\", \"image\", \"image\", \"image\", \"image\", \"imagenet\", \"imdb\", \"imdb\", \"imdb\", \"imo\", \"impact\", \"impact\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"income\", \"index\", \"index\", \"index\", \"index\", \"index\", \"index\", \"indian\", \"indian\", \"indian\", \"indian\", \"indian\", \"indiana\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"inep\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"inside\", \"inside\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"institute\", \"institute\", \"institute\", \"institute\", \"instrument\", \"integer\", \"integer\", \"integer\", \"integer\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"inversion\", \"investment\", \"investment\", \"iowa\", \"irritated\", \"jma\", \"joint\", \"joke\", \"json\", \"json\", \"json\", \"json\", \"junting\", \"just\", \"just\", \"just\", \"just\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kansa\", \"keeper\", \"kera\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kick\", \"kicker\", \"knowing\", \"krakow\", \"kumar\", \"kumar\", \"la\", \"la\", \"la\", \"label\", \"label\", \"label\", \"label\", \"lake\", \"land\", \"land\", \"lang\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"lat\", \"lat\", \"latitude\", \"latitude\", \"latitude\", \"latitude\", \"law\", \"law\", \"layer\", \"league\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learned\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"left\", \"left\", \"left\", \"left\", \"left\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"liberty\", \"lifting\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"literature\", \"ll\", \"ll\", \"lmv\", \"loan\", \"loan\", \"localisation\", \"location\", \"location\", \"location\", \"location\", \"lon\", \"longitude\", \"longitude\", \"longitude\", \"louis\", \"louisiana\", \"louisville\", \"low\", \"low\", \"low\", \"low\", \"low\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lstm\", \"ltd\", \"ltd\", \"ltd\", \"lub\", \"lyric\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machinelearningmastery\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"makedirs\", \"malignant\", \"mangasarian\", \"manipulate\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"map\", \"map\", \"map\", \"map\", \"map\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"marathon\", \"marathon\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"marvel\", \"marvel\", \"maryland\", \"massachusetts\", \"mat\", \"matthew\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"measured\", \"measured\", \"measured\", \"measured\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"meizhu\", \"membrane\", \"meneame\", \"message\", \"message\", \"metastasis\", \"meter\", \"method\", \"method\", \"method\", \"method\", \"method\", \"michigan\", \"micrometer\", \"migrant\", \"migrant\", \"migration\", \"migration\", \"migration\", \"migration\", \"mild\", \"mile\", \"mississippi\", \"missouri\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modify\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"movie\", \"movie\", \"movie\", \"msa\", \"multisurface\", \"muscle\", \"museum\", \"museum\", \"music\", \"music\", \"music\", \"music\", \"mutation\", \"nakai\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"nameprism\", \"nan\", \"narrative\", \"nationality\", \"nationality\", \"nationality\", \"natural\", \"natural\", \"nazi\", \"nba\", \"ncaa\", \"nebraska\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neighborhood\", \"neighborhood\", \"nerve\", \"nest\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newrow\", \"news\", \"news\", \"news\", \"news\", \"news\", \"newspaper\", \"newstest\", \"nice\", \"nitrogen\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"north\", \"north\", \"north\", \"north\", \"northeastern\", \"nothing\", \"noticias\", \"noun\", \"nucleus\", \"null\", \"null\", \"null\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numeric\", \"numeric\", \"numeric\", \"numerical\", \"oakland\", \"object\", \"object\", \"object\", \"object\", \"object\", \"observed\", \"odds\", \"odds\", \"odds\", \"officer\", \"oklahoma\", \"omitted\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"openaddresses\", \"opened\", \"opp_hl\", \"opponent\", \"optional\", \"organic\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"originates\", \"others\", \"others\", \"others\", \"others\", \"others\", \"overview\", \"owe\", \"oxide\", \"pain\", \"pain\", \"painting\", \"pakistan\", \"paper\", \"paper\", \"paper\", \"paper\", \"parallel\", \"particle\", \"particle\", \"particulate\", \"passenger\", \"passenger\", \"passenger\", \"passengerid\", \"passport\", \"past\", \"past\", \"past\", \"past\", \"past\", \"past\", \"path\", \"patient\", \"patient\", \"patient\", \"payment\", \"pd\", \"penalty\", \"penetrate\", \"pennsylvania\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"percentage\", \"percentage\", \"percentage\", \"percentage\", \"percom\", \"perl\", \"perl\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"pervasive\", \"pft\", \"pharmacy\", \"philadelphia\", \"philadelphia\", \"pill\", \"pill\", \"pipeline\", \"pipeline\", \"pixel\", \"pixel\", \"pixel\", \"plate\", \"played\", \"player\", \"player\", \"player\", \"playoff\", \"plc\", \"plot\", \"poetry\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pokemon\", \"policy\", \"policy\", \"policy\", \"political\", \"polygon\", \"polytechnic\", \"population\", \"population\", \"population\", \"possibility\", \"post\", \"post\", \"post\", \"post\", \"postcode\", \"pp\", \"pp\", \"pp\", \"pp\", \"pre\", \"pre\", \"precipitation\", \"premier\", \"president\", \"price\", \"price\", \"price\", \"price\", \"prism\", \"processing\", \"processing\", \"product\", \"product\", \"product\", \"product\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"promise\", \"property\", \"property\", \"property\", \"property\", \"property\", \"protein\", \"protein\", \"protein\", \"pushing\", \"pyaudioanalysis\", \"python\", \"python\", \"python\", \"python\", \"python\", \"python\", \"python\", \"python\", \"qin\", \"que\", \"que\", \"ra\", \"rain\", \"rarity\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"read_csv\", \"recall\", \"recognition\", \"recognition\", \"recognition\", \"recruiting\", \"ref\", \"refining\", \"regard\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"registration\", \"remix\", \"removed\", \"removed\", \"removed\", \"renewable\", \"rep\", \"repeat\", \"repeat\", \"repositorio\", \"representation\", \"representation\", \"representation\", \"representation\", \"requester\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"residual\", \"restaurant\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retweeted\", \"retweets\", \"revenue\", \"revenue\", \"revenue\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"revised\", \"revised\", \"rickshaw\", \"ride\", \"ride\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"risk\", \"risk\", \"road\", \"road\", \"rogue\", \"round\", \"round\", \"round\", \"round\", \"round\", \"route\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"ru\", \"sale\", \"sale\", \"sale\", \"sale\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"san\", \"san\", \"saving\", \"saving\", \"saving\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"science\", \"science\", \"science\", \"science\", \"science\", \"science\", \"science\", \"science\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scored\", \"scripps\", \"scripps\", \"season\", \"season\", \"season\", \"season\", \"season\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"secured\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seeing\", \"segmentation\", \"segmentation\", \"sense\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sent\", \"sent\", \"sent\", \"sentence\", \"sentence\", \"sentence\", \"sentiment\", \"sentiment\", \"sentiment\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"severe\", \"sex\", \"sex\", \"sgm\", \"shall\", \"shamela\", \"shapefiles\", \"shark\", \"sheet\", \"sheet\", \"shoe\", \"shooting\", \"shooting\", \"shooting\", \"shore\", \"shot\", \"shot\", \"shot\", \"shuchu\", \"siam\", \"sic\", \"simonyan\", \"simulate\", \"simulation\", \"simulation\", \"singh\", \"singh\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"sleep\", \"sleep\", \"slugging\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smoker\", \"smoking\", \"snowfall\", \"software\", \"software\", \"software\", \"solar\", \"sold\", \"sold\", \"sold\", \"somatic\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"song\", \"song\", \"south\", \"south\", \"south\", \"south\", \"south\", \"southeastern\", \"southern\", \"southern\", \"specie\", \"specie\", \"specie\", \"speech\", \"speech\", \"speech\", \"spine\", \"sport\", \"sport\", \"sport\", \"springfield\", \"squeezenet\", \"st\", \"st\", \"st\", \"stanford\", \"stanford\", \"stanford\", \"star\", \"star\", \"started\", \"started\", \"started\", \"startup\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"status\", \"status\", \"status\", \"status\", \"status\", \"status\", \"status\", \"stock\", \"stock\", \"stock\", \"stock\", \"str\", \"strain\", \"stratified\", \"strongly\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"submission\", \"succeeded\", \"sulfur\", \"sumit\", \"summoner\", \"sun\", \"sun\", \"suny\", \"survey\", \"survey\", \"survey\", \"survival\", \"survival\", \"switching\", \"switching\", \"symptom\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"ta\", \"table\", \"table\", \"table\", \"table\", \"tag\", \"tag\", \"talk\", \"taxi\", \"teacher\", \"teacher\", \"team\", \"team\", \"team\", \"team\", \"team\", \"technology\", \"technology\", \"technology\", \"technology\", \"ted\", \"telecom\", \"telstra\", \"temperature\", \"temperature\", \"tennessee\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"texas\", \"text\", \"text\", \"text\", \"text\", \"text\", \"thank\", \"thank\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thorough\", \"throwing\", \"ticker\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tissue\", \"titanic\", \"title\", \"title\", \"title\", \"title\", \"title\", \"tokenizer\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"tournament\", \"trademark\", \"traffic\", \"traffic\", \"trafiklab\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"transcript\", \"transferable\", \"transport\", \"treatment\", \"treatment\", \"trillion\", \"trip\", \"tripadvisor\", \"trump\", \"trump\", \"tsca\", \"tuition\", \"tweet\", \"tweet\", \"twitter\", \"twitter\", \"twitter\", \"txt\", \"txt\", \"txt\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"uber\", \"ucdp\", \"uk\", \"uk\", \"ultrasound\", \"undervalued\", \"uniformity\", \"united\", \"united\", \"united\", \"united\", \"united\", \"university\", \"university\", \"upc\", \"upload\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"vacant\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"vector\", \"vehicle\", \"vehicle\", \"velocity\", \"version\", \"version\", \"version\", \"version\", \"vgg\", \"video\", \"video\", \"video\", \"violation\", \"violation\", \"violent\", \"violent\", \"virginia\", \"vocalized\", \"volume\", \"volume\", \"volume\", \"volume\", \"volume\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"votos\", \"walmart\", \"walmart\", \"want\", \"want\", \"want\", \"want\", \"warranty\", \"washington\", \"washington\", \"water\", \"water\", \"water\", \"weapon\", \"weather\", \"weather\", \"weather\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"wetland\", \"wheeler\", \"whichever\", \"wikipedia\", \"wikipedia\", \"wikipedia\", \"wikipedia\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"wind_speed\", \"winter\", \"wise\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"without\", \"without\", \"without\", \"without\", \"without\", \"wolberg\", \"woman\", \"woman\", \"woman\", \"woman\", \"wondered\", \"wonderful\", \"word\", \"word\", \"word\", \"word\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"wouldn\", \"xavier\", \"xyz\", \"yale\", \"ye\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yes\", \"yes\", \"yet\", \"yifan\", \"york\", \"york\", \"york\", \"york\", \"youtube\", \"zisserman\", \"zurich\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [14, 15, 8, 4, 9, 12, 2, 6, 3, 13, 7, 5, 11, 1, 10]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1071406375988602963970707658\", ldavis_el1071406375988602963970707658_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1071406375988602963970707658\", ldavis_el1071406375988602963970707658_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1071406375988602963970707658\", ldavis_el1071406375988602963970707658_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=             x        y  topics  cluster     Freq\n",
              "topic                                            \n",
              "13    -0.18321 0.02016   1       1       26.05239\n",
              "14    -0.16052 0.04208   2       1       16.50779\n",
              "7     -0.15771 0.10911   3       1       15.52877\n",
              "3     -0.16985 0.03964   4       1       12.97389\n",
              "8     -0.02672 -0.01714  5       1       4.05159 \n",
              "11    -0.02483 0.00629   6       1       3.87243 \n",
              "1     0.17341  -0.09436  7       1       3.37515 \n",
              "5     -0.10172 -0.00504  8       1       3.28446 \n",
              "2     0.02169  -0.09052  9       1       3.18060 \n",
              "12    0.00819  -0.08202  10      1       2.85960 \n",
              "6     0.02572  -0.06414  11      1       2.25846 \n",
              "4     0.03482  -0.04311  12      1       2.11446 \n",
              "10    0.25268  0.32064   13      1       2.08576 \n",
              "0     0.09622  -0.04066  14      1       1.36834 \n",
              "9     0.21185  -0.10094  15      1       0.48631 , topic_info=             Term        Freq       Total Category  logprob  loglift\n",
              "768   university  1,459.00000 1,459.00000  Default 30.00000 30.00000\n",
              "38    dataset     4,267.00000 4,267.00000  Default 29.00000 29.00000\n",
              "747   state       846.00000   846.00000    Default 28.00000 28.00000\n",
              "1033  description 662.00000   662.00000    Default 27.00000 27.00000\n",
              "801   csv         1,178.00000 1,178.00000  Default 26.00000 26.00000\n",
              "...   ...                 ...         ...      ...      ...      ...\n",
              "3184  facebook    4.10369     30.40747     Topic15 -5.91400 3.32330 \n",
              "2186  en          4.87243     88.39231     Topic15 -5.74220 2.42790 \n",
              "3067  flavor      2.69066     12.94818     Topic15 -6.33610 3.75490 \n",
              "139   champion    2.44917     10.24934     Topic15 -6.43010 3.89460 \n",
              "2342  google      2.68199     136.59718    Topic15 -6.33930 1.39560 \n",
              "\n",
              "[1006 rows x 6 columns], token_table=       Topic    Freq       Term\n",
              "term                           \n",
              "12627  10    0.94307  abandon  \n",
              "3355   6     0.41908  abnormal \n",
              "3355   14    0.53338  abnormal \n",
              "8725   13    0.25128  academy  \n",
              "8725   14    0.69101  academy  \n",
              "...    ..        ...      ...  \n",
              "3161   4     0.26577  york     \n",
              "3161   13    0.01969  york     \n",
              "2985   1     0.99689  youtube  \n",
              "17324  9     0.96677  zisserman\n",
              "12806  10    0.89518  zurich   \n",
              "\n",
              "[2313 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[14, 15, 8, 4, 9, 12, 2, 6, 3, 13, 7, 5, 11, 1, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-DQyDdd16sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyh5a3nh16px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}