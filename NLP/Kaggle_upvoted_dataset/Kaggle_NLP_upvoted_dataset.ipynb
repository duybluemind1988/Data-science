{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_NLP_upvoted_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqiSooFMnFC/XImkQE5Kpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/Kaggle_NLP_upvoted_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE3HCPOVUhjz",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/canggih/voted-kaggle-dataset/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-vTIH-OUbRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz28thIKUhzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "fc383b41-dcee-4292-92a8-c6e325876aa8"
      },
      "source": [
        "data=pd.read_csv('https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/voted-kaggle-dataset.csv?raw=true')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Subtitle</th>\n",
              "      <th>Owner</th>\n",
              "      <th>Votes</th>\n",
              "      <th>Versions</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Data Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>License</th>\n",
              "      <th>Views</th>\n",
              "      <th>Download</th>\n",
              "      <th>Kernels</th>\n",
              "      <th>Topics</th>\n",
              "      <th>URL</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Credit Card Fraud Detection</td>\n",
              "      <td>Anonymized credit card transactions labeled as...</td>\n",
              "      <td>Machine Learning Group - ULB</td>\n",
              "      <td>1241</td>\n",
              "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
              "      <td>crime\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>442,136 views</td>\n",
              "      <td>53,128 downloads</td>\n",
              "      <td>1,782 kernels</td>\n",
              "      <td>26 topics</td>\n",
              "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
              "      <td>The datasets contains transactions made by cre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>European Soccer Database</td>\n",
              "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
              "      <td>Hugo Mathien</td>\n",
              "      <td>1046</td>\n",
              "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
              "      <td>association football\\neurope</td>\n",
              "      <td>SQLite</td>\n",
              "      <td>299 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>396,214 views</td>\n",
              "      <td>46,367 downloads</td>\n",
              "      <td>1,459 kernels</td>\n",
              "      <td>75 topics</td>\n",
              "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
              "      <td>The ultimate Soccer database for data analysis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TMDB 5000 Movie Dataset</td>\n",
              "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
              "      <td>The Movie Database (TMDb)</td>\n",
              "      <td>1024</td>\n",
              "      <td>Version 2,2017-09-28</td>\n",
              "      <td>film</td>\n",
              "      <td>CSV</td>\n",
              "      <td>44 MB</td>\n",
              "      <td>Other</td>\n",
              "      <td>446,255 views</td>\n",
              "      <td>62,002 downloads</td>\n",
              "      <td>1,394 kernels</td>\n",
              "      <td>46 topics</td>\n",
              "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
              "      <td>Background\\nWhat can we say about the success ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Global Terrorism Database</td>\n",
              "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
              "      <td>START Consortium</td>\n",
              "      <td>789</td>\n",
              "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
              "      <td>crime\\nterrorism\\ninternational relations</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>Other</td>\n",
              "      <td>187,877 views</td>\n",
              "      <td>26,309 downloads</td>\n",
              "      <td>608 kernels</td>\n",
              "      <td>11 topics</td>\n",
              "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
              "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bitcoin Historical Data</td>\n",
              "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
              "      <td>Zielak</td>\n",
              "      <td>618</td>\n",
              "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
              "      <td>history\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>119 MB</td>\n",
              "      <td>CC4</td>\n",
              "      <td>146,734 views</td>\n",
              "      <td>16,868 downloads</td>\n",
              "      <td>68 kernels</td>\n",
              "      <td>13 topics</td>\n",
              "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
              "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Title  ...                                        Description\n",
              "0  Credit Card Fraud Detection  ...  The datasets contains transactions made by cre...\n",
              "1     European Soccer Database  ...  The ultimate Soccer database for data analysis...\n",
              "2      TMDB 5000 Movie Dataset  ...  Background\\nWhat can we say about the success ...\n",
              "3    Global Terrorism Database  ...  Context\\nInformation on more than 170,000 Terr...\n",
              "4      Bitcoin Historical Data  ...  Context\\nBitcoin is the longest running and mo...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEVdPq3QU96N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "797720c0-39cf-454f-d95e-379a03955b92"
      },
      "source": [
        "data.Description[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhh0YWlxXLKG",
        "colab_type": "text"
      },
      "source": [
        "# Ch06b - Topic Modeling with gensim.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPj-D9NXXXiW",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgVxQFhcXhL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "89604aa2-14bc-48d7-9afd-25237ddda1ee"
      },
      "source": [
        "papers=data.Description\n",
        "print(len(papers))\n",
        "papers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       The datasets contains transactions made by cre...\n",
              "1       The ultimate Soccer database for data analysis...\n",
              "2       Background\\nWhat can we say about the success ...\n",
              "3       Context\\nInformation on more than 170,000 Terr...\n",
              "4       Context\\nBitcoin is the longest running and mo...\n",
              "                              ...                        \n",
              "2145    Context\\nFortnite: Battle Royale has over 20 m...\n",
              "2146    Context\\nThis dataset provides the nationaliti...\n",
              "2147    lem.json\\nThis file contains lementized englis...\n",
              "2148    Context\\nThis data set contains weather data f...\n",
              "2149    Context\\nBirths in U.S during 1994 to 2003.\\nC...\n",
              "Name: Description, Length: 2150, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyQh58zWYHTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "papers=papers.astype(str)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7frNCl40WDfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "42ec6598-b331-4987-f647-236f2f13928a"
      },
      "source": [
        "%%time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(papers):\n",
        "    norm_papers = []\n",
        "    for paper in papers:\n",
        "        paper = paper.lower()\n",
        "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "        paper_tokens = list(filter(None, paper_tokens))\n",
        "        if paper_tokens:\n",
        "            norm_papers.append(paper_tokens)\n",
        "            \n",
        "    return norm_papers\n",
        "    \n",
        "norm_papers = normalize_corpus(papers)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "CPU times: user 3.14 s, sys: 33.9 ms, total: 3.17 s\n",
            "Wall time: 3.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqy2Zt0VXnxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "896cca7c-7482-4632-d971-9afc9b0754f4"
      },
      "source": [
        "print(papers[0])\n",
        "print(norm_papers[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
            "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
            "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
            "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
            "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze66dZ44YlUq",
        "colab_type": "text"
      },
      "source": [
        "## Text Representation with Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO6346G5XpN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') \n",
        "# higher threshold fewer phrases.\n",
        "bigram_model = gensim.models.phrases.Phraser(bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hupfI97NYyY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "412accac-80aa-4f6f-9c39-53b0b40cfa1d"
      },
      "source": [
        "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
        "print(norm_corpus_bigrams[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine_learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please_cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C01Gv8vBZBrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "# Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
        "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHKgFEkuZNoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e50dcada-ffe8-4cbd-f9ca-6cbb94433965"
      },
      "source": [
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, 'ac'), (1, 'account'), (2, 'accuracy'), (3, 'amount'), (4, 'analysed'), (5, 'andrea'), (6, 'area'), (7, 'artml'), (8, 'auprc'), (9, 'available'), (10, 'background'), (11, 'big'), (12, 'bontempi'), (13, 'brufence'), (14, 'bruxelles')]\n",
            "Total Vocabulary Size: 32097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX-lAwjdZPqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "881b100c-30b0-4b6c-9c43-9922692392ca"
      },
      "source": [
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
        "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
        "print('Total Vocabulary Size:', len(dictionary))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample word to number mappings: [(0, 'ac'), (1, 'account'), (2, 'accuracy'), (3, 'amount'), (4, 'area'), (5, 'available'), (6, 'background'), (7, 'big'), (8, 'cannot'), (9, 'card'), (10, 'case'), (11, 'class'), (12, 'classification'), (13, 'collected'), (14, 'component')]\n",
            "Total Vocabulary Size: 1821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BufKRboIZVoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "98096c4e-0d0b-4149-9169-3cdfbafbe967"
      },
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
        "print(bow_corpus[0][:50])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 3), (1, 1), (2, 2), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 3), (12, 2), (13, 1), (14, 1), (15, 1), (16, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 7), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbnvw8ROZpX9",
        "colab_type": "text"
      },
      "source": [
        "doc2bow: Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method.\n",
        "\n",
        "If allow_update is set, then also update dictionary in the process: create ids for new words. At the same time, update document frequencies -- for each word appearing in this document, increase its document frequency (self.dfs) by one.\n",
        "\n",
        "If allow_update is not set, this function is const, aka read-only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBqSH8EsZe3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7fd6bd05-cf67-4f3d-e034-fbd7d33e0e3c"
      },
      "source": [
        "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('card', 1), ('case', 1), ('class', 2), ('day', 1), ('european', 1), ('feature', 1), ('first', 1), ('ha', 3), ('http', 2), ('machine_learning', 1), ('original', 2), ('probability', 2), ('project', 1), ('time', 2), ('using', 2), ('value', 1), ('able', 2), ('access', 2), ('achieved', 1), ('across', 1), ('adding', 1), ('algorithm', 1), ('also', 3), ('analysis', 1), ('api', 1), ('appears', 1), ('ask', 1), ('asset', 1), ('attribute', 5), ('away', 1), ('base', 1), ('called', 1), ('changed', 1), ('classifier', 1), ('click', 2), ('co', 1), ('collection', 1), ('column', 1), ('com', 3), ('come', 1), ('commercial', 1), ('compare', 1), ('containing', 1), ('coordinate', 1), ('could', 1), ('country', 1), ('crawling', 2), ('cross', 1), ('database', 2), ('design', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZhG9ywZt_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3417334-7cd0-460d-a962-b77cdad9c47f"
      },
      "source": [
        "print('Total number of papers:', len(bow_corpus))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of papers: 2150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksCjrdIwZxgZ",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meXt3wwNZvr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e80037d0-e56a-42fe-8496-8184b70117fc"
      },
      "source": [
        "%%time\n",
        "TOTAL_TOPICS = 20\n",
        "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
        "                                 onepass=True, chunksize=1740, power_iters=1000)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min, sys: 46.6 s, total: 2min 47s\n",
            "Wall time: 1min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-VT5R5Z2AI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d108f00-cb22-44d2-b200-6cdb6c87750a"
      },
      "source": [
        "for topic_id, topic in lsi_bow.print_topics(num_topics=20, num_words=20):\n",
        "    print('Topic #'+str(topic_id+1)+':')\n",
        "    print(topic)\n",
        "    print()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "0.879*\"university\" + 0.124*\"player\" + 0.101*\"number\" + 0.091*\"wa\" + 0.090*\"college\" + 0.088*\"year\" + 0.088*\"time\" + 0.085*\"file\" + 0.079*\"team\" + 0.075*\"csv\" + 0.069*\"one\" + 0.066*\"information\" + 0.064*\"name\" + 0.063*\"ha\" + 0.060*\"date\" + 0.049*\"value\" + 0.046*\"institute\" + 0.045*\"used\" + 0.045*\"new\" + 0.045*\"university_california\"\n",
            "\n",
            "Topic #2:\n",
            "-0.452*\"university\" + 0.294*\"player\" + 0.207*\"number\" + 0.183*\"team\" + 0.176*\"year\" + 0.176*\"time\" + 0.172*\"wa\" + 0.168*\"file\" + 0.146*\"csv\" + 0.134*\"one\" + 0.130*\"name\" + 0.124*\"date\" + 0.122*\"information\" + 0.121*\"ha\" + 0.100*\"goal\" + 0.098*\"value\" + 0.088*\"game\" + 0.086*\"code\" + 0.085*\"used\" + 0.080*\"use\"\n",
            "\n",
            "Topic #3:\n",
            "-0.678*\"player\" + -0.394*\"team\" + -0.237*\"goal\" + -0.151*\"zone\" + 0.135*\"file\" + -0.134*\"game\" + -0.129*\"allowed\" + 0.110*\"csv\" + 0.102*\"year\" + 0.091*\"one\" + 0.087*\"date\" + 0.080*\"information\" + -0.076*\"percentage\" + 0.075*\"name\" + 0.072*\"value\" + -0.068*\"individual\" + -0.065*\"taken\" + -0.065*\"scored\" + -0.059*\"relative\" + 0.058*\"time\"\n",
            "\n",
            "Topic #4:\n",
            "0.607*\"year\" + -0.252*\"date\" + -0.230*\"file\" + 0.188*\"total\" + -0.155*\"csv\" + -0.154*\"one\" + -0.150*\"registration\" + -0.145*\"zero\" + -0.142*\"start\" + -0.134*\"application\" + -0.132*\"time\" + 0.123*\"given\" + -0.119*\"position\" + -0.119*\"numeric\" + -0.115*\"containing\" + 0.107*\"energy\" + -0.104*\"text\" + 0.095*\"state\" + -0.094*\"element\" + 0.088*\"child\"\n",
            "\n",
            "Topic #5:\n",
            "0.633*\"csv\" + -0.302*\"date\" + -0.200*\"number\" + -0.191*\"year\" + 0.169*\"integer\" + -0.166*\"registration\" + -0.163*\"zero\" + -0.153*\"time\" + -0.140*\"start\" + -0.135*\"application\" + -0.121*\"position\" + -0.119*\"one\" + 0.115*\"movie\" + -0.114*\"code\" + -0.113*\"containing\" + -0.102*\"element\" + 0.097*\"user\" + -0.095*\"section\" + 0.087*\"numeric\" + -0.083*\"mark\"\n",
            "\n",
            "Topic #6:\n",
            "0.856*\"integer\" + -0.356*\"csv\" + -0.118*\"year\" + 0.107*\"movie\" + 0.092*\"point\" + 0.092*\"people\" + 0.088*\"categorical\" + 0.085*\"always\" + 0.076*\"music\" + 0.063*\"preference\" + 0.062*\"interest\" + 0.059*\"lot\" + 0.056*\"item\" + 0.048*\"enjoy\" + -0.048*\"player\" + 0.041*\"money\" + 0.039*\"team\" + 0.039*\"life\" + 0.038*\"time\" + 0.037*\"often\"\n",
            "\n",
            "Topic #7:\n",
            "0.774*\"numeric\" + 0.499*\"text\" + -0.272*\"csv\" + -0.111*\"integer\" + 0.069*\"word\" + -0.062*\"date\" + -0.054*\"file\" + 0.051*\"open\" + -0.045*\"time\" + 0.043*\"food\" + -0.042*\"movie\" + 0.039*\"language\" + 0.039*\"product\" + 0.037*\"student\" + -0.036*\"number\" + -0.035*\"one\" + -0.035*\"registration\" + 0.035*\"database\" + -0.034*\"zero\" + 0.034*\"use\"\n",
            "\n",
            "Topic #8:\n",
            "-0.499*\"csv\" + -0.354*\"year\" + -0.293*\"integer\" + -0.289*\"numeric\" + -0.164*\"date\" + 0.143*\"image\" + 0.123*\"word\" + 0.111*\"use\" + -0.104*\"total\" + -0.103*\"registration\" + -0.099*\"zero\" + 0.098*\"user\" + 0.095*\"language\" + 0.091*\"information\" + 0.090*\"wa\" + 0.084*\"using\" + -0.083*\"time\" + -0.083*\"position\" + -0.081*\"given\" + -0.080*\"application\"\n",
            "\n",
            "Topic #9:\n",
            "0.312*\"year\" + 0.262*\"file\" + -0.262*\"name\" + -0.246*\"value\" + -0.240*\"de\" + 0.213*\"image\" + 0.165*\"movie\" + -0.160*\"number\" + 0.157*\"total\" + -0.143*\"fire\" + 0.138*\"user\" + 0.128*\"word\" + -0.112*\"unit\" + -0.109*\"csv\" + -0.108*\"code\" + -0.105*\"state\" + 0.104*\"given\" + -0.101*\"event\" + -0.101*\"day\" + -0.100*\"city\"\n",
            "\n",
            "Topic #10:\n",
            "0.588*\"image\" + -0.314*\"de\" + -0.247*\"user\" + -0.206*\"movie\" + -0.165*\"en\" + 0.150*\"label\" + -0.142*\"per\" + 0.134*\"class\" + 0.127*\"value\" + -0.105*\"number\" + -0.101*\"word\" + -0.100*\"le\" + -0.100*\"rating\" + 0.099*\"sample\" + -0.096*\"language\" + 0.096*\"point\" + -0.095*\"com\" + -0.095*\"tag\" + -0.093*\"la\" + -0.089*\"ha\"\n",
            "\n",
            "Topic #11:\n",
            "-0.444*\"de\" + -0.277*\"value\" + -0.264*\"image\" + -0.226*\"en\" + -0.223*\"per\" + 0.168*\"child\" + -0.150*\"le\" + 0.142*\"police\" + 0.134*\"fire\" + 0.129*\"age\" + -0.127*\"file\" + -0.123*\"year\" + -0.122*\"la\" + -0.116*\"com\" + 0.115*\"information\" + -0.111*\"station\" + 0.109*\"woman\" + 0.106*\"death\" + 0.105*\"state\" + 0.098*\"survey\"\n",
            "\n",
            "Topic #12:\n",
            "0.320*\"de\" + 0.290*\"image\" + -0.277*\"user\" + -0.240*\"value\" + -0.190*\"file\" + -0.188*\"station\" + 0.180*\"en\" + -0.176*\"movie\" + 0.170*\"child\" + 0.168*\"number\" + 0.165*\"per\" + -0.139*\"id\" + 0.132*\"word\" + -0.131*\"name\" + 0.110*\"age\" + -0.107*\"event\" + 0.100*\"le\" + -0.097*\"tag\" + 0.097*\"la\" + 0.096*\"language\"\n",
            "\n",
            "Topic #13:\n",
            "-0.343*\"child\" + -0.280*\"number\" + -0.276*\"word\" + 0.210*\"fire\" + -0.183*\"age\" + -0.177*\"language\" + 0.150*\"de\" + -0.141*\"value\" + -0.136*\"name\" + 0.134*\"national\" + 0.118*\"department\" + 0.118*\"code\" + -0.112*\"sample\" + 0.110*\"information\" + -0.108*\"station\" + 0.103*\"en\" + 0.103*\"state\" + -0.102*\"corpus\" + 0.101*\"service\" + 0.100*\"center\"\n",
            "\n",
            "Topic #14:\n",
            "0.376*\"race\" + 0.319*\"time\" + -0.283*\"image\" + -0.236*\"file\" + -0.234*\"name\" + 0.174*\"word\" + 0.170*\"point\" + -0.159*\"number\" + -0.149*\"child\" + -0.145*\"player\" + 0.143*\"section\" + -0.136*\"age\" + 0.118*\"taken\" + -0.109*\"code\" + 0.102*\"match\" + 0.100*\"team\" + 0.092*\"language\" + 0.089*\"position\" + -0.087*\"rating\" + -0.085*\"movie\"\n",
            "\n",
            "Topic #15:\n",
            "0.365*\"word\" + -0.365*\"user\" + -0.340*\"race\" + 0.248*\"language\" + -0.196*\"time\" + -0.175*\"number\" + 0.171*\"file\" + -0.155*\"image\" + 0.131*\"corpus\" + -0.123*\"tweet\" + -0.116*\"movie\" + -0.112*\"age\" + -0.109*\"id\" + 0.105*\"code\" + -0.104*\"section\" + 0.097*\"fire\" + -0.097*\"rating\" + 0.092*\"team\" + -0.089*\"game\" + -0.087*\"police\"\n",
            "\n",
            "Topic #16:\n",
            "0.317*\"number\" + -0.285*\"file\" + -0.248*\"team\" + -0.218*\"police\" + 0.178*\"player\" + 0.165*\"game\" + 0.163*\"fire\" + -0.162*\"woman\" + -0.157*\"station\" + -0.154*\"point\" + -0.149*\"age\" + -0.146*\"total\" + -0.145*\"child\" + 0.143*\"user\" + -0.140*\"property\" + -0.126*\"goal\" + 0.122*\"word\" + -0.117*\"population\" + 0.116*\"wa\" + 0.113*\"language\"\n",
            "\n",
            "Topic #17:\n",
            "-0.229*\"race\" + -0.223*\"fire\" + -0.222*\"user\" + 0.198*\"station\" + -0.190*\"word\" + -0.178*\"point\" + -0.176*\"image\" + -0.155*\"name\" + -0.153*\"child\" + 0.153*\"feature\" + 0.152*\"game\" + -0.139*\"taken\" + -0.130*\"time\" + -0.125*\"team\" + -0.114*\"unit\" + 0.108*\"match\" + -0.107*\"language\" + -0.106*\"file\" + -0.106*\"state\" + 0.102*\"player\"\n",
            "\n",
            "Topic #18:\n",
            "-0.292*\"police\" + -0.248*\"station\" + 0.236*\"drug\" + 0.215*\"plan\" + 0.207*\"survey\" + 0.192*\"rating\" + 0.192*\"race\" + 0.190*\"health\" + -0.184*\"crime\" + -0.144*\"fire\" + -0.132*\"location\" + -0.127*\"user\" + -0.117*\"woman\" + -0.116*\"death\" + -0.113*\"wa\" + 0.112*\"sample\" + 0.111*\"disease\" + 0.105*\"taken\" + -0.100*\"feature\" + -0.099*\"incident\"\n",
            "\n",
            "Topic #19:\n",
            "0.350*\"race\" + 0.279*\"file\" + 0.278*\"station\" + -0.252*\"user\" + 0.181*\"feature\" + 0.176*\"fire\" + -0.150*\"event\" + -0.134*\"team\" + -0.131*\"date\" + -0.121*\"sample\" + -0.115*\"tweet\" + -0.112*\"monitor\" + 0.107*\"player\" + -0.106*\"value\" + 0.105*\"section\" + 0.100*\"id\" + 0.099*\"contains\" + -0.098*\"parameter\" + 0.098*\"number\" + 0.097*\"game\"\n",
            "\n",
            "Topic #20:\n",
            "0.313*\"police\" + -0.240*\"price\" + -0.236*\"name\" + -0.234*\"team\" + -0.215*\"point\" + -0.211*\"match\" + 0.168*\"player\" + 0.162*\"crime\" + 0.162*\"woman\" + -0.159*\"sale\" + 0.146*\"value\" + 0.119*\"drug\" + 0.112*\"plan\" + 0.107*\"survey\" + 0.107*\"health\" + 0.107*\"word\" + 0.102*\"death\" + 0.098*\"taken\" + 0.097*\"station\" + 0.097*\"race\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnToggzkaZAj",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a moment to understand these results. A brief recap on the LSI model— it is based on the principle that words that are used in the same contexts tend to have similar meanings. You can observe in this output that each topic is a combination of terms (which basically tend to convey an overall sense of the topic) and weights. Now the problem here is that we have both positive and negative weights. What does that mean?\n",
        "\n",
        "Based on existing research and my interpretations, considering we are reducing the dimensionality here to a 10-dimensional space based on the number of topics, the sign on each term indicates a sense of direction or orientation in the vector space for a particular topic. The higher the weight, the more important the contribution. So similar correlated terms have the same sign or direction. Hence, it is perfectly possible for a topic to have two different sub-themes based on the sign or orientation of terms. Let’s separate these terms and try to interpret the topics again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSdChuYwaLmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d75a9aaa-dfc7-47dd-e474-fb3a1b2c0606"
      },
      "source": [
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('university', 0.879), ('player', 0.124), ('number', 0.101), ('wa', 0.091), ('college', 0.09), ('year', 0.088), ('time', 0.088), ('file', 0.085), ('team', 0.079), ('csv', 0.075), ('one', 0.069), ('information', 0.066), ('name', 0.064), ('ha', 0.063), ('date', 0.06), ('value', 0.049), ('institute', 0.046), ('used', 0.045), ('new', 0.045), ('university_california', 0.045)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.294), ('number', 0.207), ('team', 0.183), ('year', 0.176), ('time', 0.176), ('wa', 0.172), ('file', 0.168), ('csv', 0.146), ('one', 0.134), ('name', 0.13), ('date', 0.124), ('information', 0.122), ('ha', 0.121), ('goal', 0.1), ('value', 0.098), ('game', 0.088), ('code', 0.086), ('used', 0.085), ('use', 0.08)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('university', -0.452)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('file', 0.135), ('csv', 0.11), ('year', 0.102), ('one', 0.091), ('date', 0.087), ('information', 0.08), ('name', 0.075), ('value', 0.072), ('time', 0.058)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('player', -0.678), ('team', -0.394), ('goal', -0.237), ('zone', -0.151), ('game', -0.134), ('allowed', -0.129), ('percentage', -0.076), ('individual', -0.068), ('taken', -0.065), ('scored', -0.065), ('relative', -0.059)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.607), ('total', 0.188), ('given', 0.123), ('energy', 0.107), ('state', 0.095), ('child', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('date', -0.252), ('file', -0.23), ('csv', -0.155), ('one', -0.154), ('registration', -0.15), ('zero', -0.145), ('start', -0.142), ('application', -0.134), ('time', -0.132), ('position', -0.119), ('numeric', -0.119), ('containing', -0.115), ('text', -0.104), ('element', -0.094)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('csv', 0.633), ('integer', 0.169), ('movie', 0.115), ('user', 0.097), ('numeric', 0.087)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('date', -0.302), ('number', -0.2), ('year', -0.191), ('registration', -0.166), ('zero', -0.163), ('time', -0.153), ('start', -0.14), ('application', -0.135), ('position', -0.121), ('one', -0.119), ('code', -0.114), ('containing', -0.113), ('element', -0.102), ('section', -0.095), ('mark', -0.083)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('integer', 0.856), ('movie', 0.107), ('point', 0.092), ('people', 0.092), ('categorical', 0.088), ('always', 0.085), ('music', 0.076), ('preference', 0.063), ('interest', 0.062), ('lot', 0.059), ('item', 0.056), ('enjoy', 0.048), ('money', 0.041), ('team', 0.039), ('life', 0.039), ('time', 0.038), ('often', 0.037)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.356), ('year', -0.118), ('player', -0.048)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.774), ('text', 0.499), ('word', 0.069), ('open', 0.051), ('food', 0.043), ('language', 0.039), ('product', 0.039), ('student', 0.037), ('database', 0.035), ('use', 0.034)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.272), ('integer', -0.111), ('date', -0.062), ('file', -0.054), ('time', -0.045), ('movie', -0.042), ('number', -0.036), ('one', -0.035), ('registration', -0.035), ('zero', -0.034)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.143), ('word', 0.123), ('use', 0.111), ('user', 0.098), ('language', 0.095), ('information', 0.091), ('wa', 0.09), ('using', 0.084)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.499), ('year', -0.354), ('integer', -0.293), ('numeric', -0.289), ('date', -0.164), ('total', -0.104), ('registration', -0.103), ('zero', -0.099), ('time', -0.083), ('position', -0.083), ('given', -0.081), ('application', -0.08)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.312), ('file', 0.262), ('image', 0.213), ('movie', 0.165), ('total', 0.157), ('user', 0.138), ('word', 0.128), ('given', 0.104)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('name', -0.262), ('value', -0.246), ('de', -0.24), ('number', -0.16), ('fire', -0.143), ('unit', -0.112), ('csv', -0.109), ('code', -0.108), ('state', -0.105), ('event', -0.101), ('day', -0.101), ('city', -0.1)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.588), ('label', 0.15), ('class', 0.134), ('value', 0.127), ('sample', 0.099), ('point', 0.096)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.314), ('user', -0.247), ('movie', -0.206), ('en', -0.165), ('per', -0.142), ('number', -0.105), ('word', -0.101), ('le', -0.1), ('rating', -0.1), ('language', -0.096), ('com', -0.095), ('tag', -0.095), ('la', -0.093), ('ha', -0.089)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #11:\n",
            "==================================================\n",
            "Direction 1: [('child', 0.168), ('police', 0.142), ('fire', 0.134), ('age', 0.129), ('information', 0.115), ('woman', 0.109), ('death', 0.106), ('state', 0.105), ('survey', 0.098)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.444), ('value', -0.277), ('image', -0.264), ('en', -0.226), ('per', -0.223), ('le', -0.15), ('file', -0.127), ('year', -0.123), ('la', -0.122), ('com', -0.116), ('station', -0.111)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #12:\n",
            "==================================================\n",
            "Direction 1: [('de', 0.32), ('image', 0.29), ('en', 0.18), ('child', 0.17), ('number', 0.168), ('per', 0.165), ('word', 0.132), ('age', 0.11), ('le', 0.1), ('la', 0.097), ('language', 0.096)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.277), ('value', -0.24), ('file', -0.19), ('station', -0.188), ('movie', -0.176), ('id', -0.139), ('name', -0.131), ('event', -0.107), ('tag', -0.097)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #13:\n",
            "==================================================\n",
            "Direction 1: [('fire', 0.21), ('de', 0.15), ('national', 0.134), ('department', 0.118), ('code', 0.118), ('information', 0.11), ('en', 0.103), ('state', 0.103), ('service', 0.101), ('center', 0.1)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('child', -0.343), ('number', -0.28), ('word', -0.276), ('age', -0.183), ('language', -0.177), ('value', -0.141), ('name', -0.136), ('sample', -0.112), ('station', -0.108), ('corpus', -0.102)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #14:\n",
            "==================================================\n",
            "Direction 1: [('race', 0.376), ('time', 0.319), ('word', 0.174), ('point', 0.17), ('section', 0.143), ('taken', 0.118), ('match', 0.102), ('team', 0.1), ('language', 0.092), ('position', 0.089)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -0.283), ('file', -0.236), ('name', -0.234), ('number', -0.159), ('child', -0.149), ('player', -0.145), ('age', -0.136), ('code', -0.109), ('rating', -0.087), ('movie', -0.085)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #15:\n",
            "==================================================\n",
            "Direction 1: [('word', 0.365), ('language', 0.248), ('file', 0.171), ('corpus', 0.131), ('code', 0.105), ('fire', 0.097), ('team', 0.092)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.365), ('race', -0.34), ('time', -0.196), ('number', -0.175), ('image', -0.155), ('tweet', -0.123), ('movie', -0.116), ('age', -0.112), ('id', -0.109), ('section', -0.104), ('rating', -0.097), ('game', -0.089), ('police', -0.087)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #16:\n",
            "==================================================\n",
            "Direction 1: [('number', 0.317), ('player', 0.178), ('game', 0.165), ('fire', 0.163), ('user', 0.143), ('word', 0.122), ('wa', 0.116), ('language', 0.113)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('file', -0.285), ('team', -0.248), ('police', -0.218), ('woman', -0.162), ('station', -0.157), ('point', -0.154), ('age', -0.149), ('total', -0.146), ('child', -0.145), ('property', -0.14), ('goal', -0.126), ('population', -0.117)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #17:\n",
            "==================================================\n",
            "Direction 1: [('station', 0.198), ('feature', 0.153), ('game', 0.152), ('match', 0.108), ('player', 0.102)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('race', -0.229), ('fire', -0.223), ('user', -0.222), ('word', -0.19), ('point', -0.178), ('image', -0.176), ('name', -0.155), ('child', -0.153), ('taken', -0.139), ('time', -0.13), ('team', -0.125), ('unit', -0.114), ('language', -0.107), ('file', -0.106), ('state', -0.106)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #18:\n",
            "==================================================\n",
            "Direction 1: [('drug', 0.236), ('plan', 0.215), ('survey', 0.207), ('rating', 0.192), ('race', 0.192), ('health', 0.19), ('sample', 0.112), ('disease', 0.111), ('taken', 0.105)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('police', -0.292), ('station', -0.248), ('crime', -0.184), ('fire', -0.144), ('location', -0.132), ('user', -0.127), ('woman', -0.117), ('death', -0.116), ('wa', -0.113), ('feature', -0.1), ('incident', -0.099)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #19:\n",
            "==================================================\n",
            "Direction 1: [('race', 0.35), ('file', 0.279), ('station', 0.278), ('feature', 0.181), ('fire', 0.176), ('player', 0.107), ('section', 0.105), ('id', 0.1), ('contains', 0.099), ('number', 0.098), ('game', 0.097)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.252), ('event', -0.15), ('team', -0.134), ('date', -0.131), ('sample', -0.121), ('tweet', -0.115), ('monitor', -0.112), ('value', -0.106), ('parameter', -0.098)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #20:\n",
            "==================================================\n",
            "Direction 1: [('police', 0.313), ('player', 0.168), ('crime', 0.162), ('woman', 0.162), ('value', 0.146), ('drug', 0.119), ('plan', 0.112), ('survey', 0.107), ('health', 0.107), ('word', 0.107), ('death', 0.102), ('taken', 0.098), ('station', 0.097), ('race', 0.097)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('price', -0.24), ('name', -0.236), ('team', -0.234), ('point', -0.215), ('match', -0.211), ('sale', -0.159)]\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFiq2ihqa8RA",
        "colab_type": "text"
      },
      "source": [
        "Does this make things better? Well, it’s definitely a lot better than the previous interpretation. Here we can see clear themes of modeling being applied in chips and electronic devices, classification and recognition models, neural models talking about the human brain components like cells, stimuli, neurons, cortical components, and even themes around reinforcement learning! We explore these in detail later in a more structured way. Let’s try to get the three major matrices (U, S, and VT) from our topic model, which uses SVD (based on the foundational concepts mentioned earlier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9E85ziEaaDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0a39c159-4a94-440a-f311-04f8c3ed7880"
      },
      "source": [
        "term_topic = lsi_bow.projection.u\n",
        "singular_values = lsi_bow.projection.s\n",
        "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
        "term_topic.shape, singular_values.shape, topic_document.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:502: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1821, 20), (20,), (20, 2150))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRNTimEma9LJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "f910bd16-9b91-419b-f023-687981cbb044"
      },
      "source": [
        "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
        "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "print(document_topics.shape)\n",
        "document_topics.head(15)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "      <th>T11</th>\n",
              "      <th>T12</th>\n",
              "      <th>T13</th>\n",
              "      <th>T14</th>\n",
              "      <th>T15</th>\n",
              "      <th>T16</th>\n",
              "      <th>T17</th>\n",
              "      <th>T18</th>\n",
              "      <th>T19</th>\n",
              "      <th>T20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.025</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.020</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.041</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.033</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.015</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.019</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>-0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.032</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.005</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>0.047</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.032</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.019</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.011</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.016</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.040</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.010</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.009</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.018</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.002</td>\n",
              "      <td>-0.021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.018</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>-0.020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>-0.011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.018</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>0.118</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.874</td>\n",
              "      <td>-0.289</td>\n",
              "      <td>-0.065</td>\n",
              "      <td>0.044</td>\n",
              "      <td>-0.032</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.027</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       T1     T2     T3     T4     T5  ...    T16    T17    T18    T19    T20\n",
              "0   0.006  0.012  0.009 -0.002  0.000  ... -0.016  0.025 -0.005  0.017  0.000\n",
              "1   0.020  0.043 -0.041 -0.002  0.014  ...  0.003  0.064 -0.012 -0.017 -0.031\n",
              "2   0.015  0.027  0.018 -0.007  0.007  ...  0.002  0.017 -0.000 -0.021 -0.005\n",
              "3   0.032  0.035  0.023 -0.016  0.000  ...  0.032 -0.017 -0.012 -0.067  0.006\n",
              "4   0.007  0.013  0.008 -0.004  0.019  ... -0.002  0.011 -0.008 -0.014 -0.008\n",
              "5   0.016  0.032  0.020 -0.003  0.040  ... -0.007  0.017  0.013 -0.013 -0.000\n",
              "6   0.002  0.004  0.003 -0.001  0.001  ...  0.001  0.004 -0.001 -0.001  0.000\n",
              "7   0.002  0.004  0.003 -0.000  0.001  ... -0.004  0.008  0.004 -0.002 -0.006\n",
              "8   0.009  0.017  0.012 -0.009  0.016  ... -0.002  0.020 -0.008 -0.012 -0.004\n",
              "9   0.009  0.017  0.007  0.006  0.004  ...  0.004  0.008  0.002  0.002 -0.021\n",
              "10  0.004  0.007  0.006 -0.008 -0.001  ... -0.012  0.001  0.003  0.018 -0.006\n",
              "11  0.010  0.020  0.010  0.009  0.004  ...  0.007  0.000  0.003 -0.004 -0.020\n",
              "12  0.010  0.018  0.013 -0.000  0.020  ...  0.000  0.004 -0.007 -0.012 -0.011\n",
              "13  0.018  0.035  0.043 -0.133  0.118  ... -0.024 -0.000  0.002  0.012 -0.013\n",
              "14  0.007  0.008  0.004  0.023 -0.003  ...  0.008  0.013  0.003  0.010  0.005\n",
              "\n",
              "[15 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omNHa8tcd5Uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d806e30d-da9b-4fcf-cb2e-d4c62fe14eb6"
      },
      "source": [
        "top_topics[0][1:]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'19'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwK6FPqNa-3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "51036474-e612-459a-ba4e-fe7c6764de92"
      },
      "source": [
        "document_numbers = [1, 5, 10]\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print('Topic model '+top_topics[0][1:]+':',lsi_bow.show_topic(int(top_topics[0][1:]), topn=20))\n",
        "    print('Topic model '+top_topics[1][1:]+':',lsi_bow.show_topic(int(top_topics[1][1:]), topn=20))\n",
        "    print()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document #1:\n",
            "Dominant Topics (top 3): ['T17', 'T2', 'T3']\n",
            "Paper Summary:\n",
            "The ultimate Soccer database for data analysis and machine learning\n",
            "What you get:\n",
            "+25,000 matches\n",
            "+10,000 players\n",
            "11 European Countries with their lead championship\n",
            "Seasons 2008 to 2016\n",
            "Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates\n",
            "Team line up with squad formation (X, Y coordinates)\n",
            "Betting odds from up to 10 providers\n",
            "Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
            "*16th Oct 201\n",
            "Topic model 17: [('police', -0.2924384771617504), ('station', -0.24798426375102275), ('drug', 0.23643131690674565), ('plan', 0.21513466376974605), ('survey', 0.20661591026593037), ('rating', 0.19236671004567105), ('race', 0.1919343437447153), ('health', 0.18984901662405568), ('crime', -0.18405253536345698), ('fire', -0.14441848483960898), ('location', -0.1315384180137746), ('user', -0.1273879204036706), ('woman', -0.11664968655438007), ('death', -0.11638703260677678), ('wa', -0.1128321451398375), ('sample', 0.11205724639860422), ('disease', 0.11146711894606401), ('taken', 0.1050480894353286), ('feature', -0.09967846262371209), ('incident', -0.09935598940277286)]\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "\n",
            "Document #5:\n",
            "Dominant Topics (top 3): ['T5', 'T2', 'T11']\n",
            "Paper Summary:\n",
            "Context\n",
            "For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.\n",
            "To share some of the initial insights from the survey, we’ve worked with the folks from The Pudding to put together \n",
            "Topic model 5: [('integer', 0.8558303369204103), ('csv', -0.3560007655627152), ('year', -0.11843953713157371), ('movie', 0.106975688485878), ('point', 0.09238243803741718), ('people', 0.09161095744046643), ('categorical', 0.08822485877639168), ('always', 0.08510579110355394), ('music', 0.07618614080130515), ('preference', 0.06265756436109131), ('interest', 0.062228398544929925), ('lot', 0.059128426516308646), ('item', 0.05563414372912065), ('enjoy', 0.048183264967643975), ('player', -0.047582647417016455), ('money', 0.040527058791298635), ('team', 0.039237088394806025), ('life', 0.0391879794239862), ('time', 0.03752584807592715), ('often', 0.03658451227435939)]\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "\n",
            "Document #10:\n",
            "Dominant Topics (top 3): ['T19', 'T14', 'T16']\n",
            "Paper Summary:\n",
            "These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of a\n",
            "Topic model 19: [('police', 0.3128588551437026), ('price', -0.2404099736902586), ('name', -0.2362804984051518), ('team', -0.23388998567359673), ('point', -0.21470422083397508), ('match', -0.21138040287139537), ('player', 0.16810992021928678), ('crime', 0.1621768561107652), ('woman', 0.161825010221465), ('sale', -0.15926053300798132), ('value', 0.1461893502348703), ('drug', 0.11916556822921215), ('plan', 0.11151090366441535), ('survey', 0.10717370360550377), ('health', 0.1068285362030665), ('word', 0.10662870393637465), ('death', 0.10167088503277935), ('taken', 0.09832158507176714), ('station', 0.09740267955044828), ('race', 0.09710118307388292)]\n",
            "Topic model 14: [('word', 0.3647410730005829), ('user', -0.3645749216730648), ('race', -0.3399994383347151), ('language', 0.2477049704013479), ('time', -0.1962980398941309), ('number', -0.17451247778404122), ('file', 0.17100183502430066), ('image', -0.15508001106020472), ('corpus', 0.1306953130534607), ('tweet', -0.1228217884113363), ('movie', -0.11588357615553925), ('age', -0.11211632390658081), ('id', -0.10924241236924614), ('code', 0.10451163565355795), ('section', -0.1042149691550576), ('fire', 0.09689701399383074), ('rating', -0.09682795666565036), ('team', 0.09167078537234875), ('game', -0.08924001235890998), ('police', -0.0871052119172898)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ySUzqzenp3",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykl0jA0lbCgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "720104db-ed02-4e1d-cd39-90529d683673"
      },
      "source": [
        "%%time\n",
        "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, \n",
        "                                   alpha='auto', eta='auto', random_state=42,\n",
        "                                   iterations=500, num_topics=TOTAL_TOPICS, \n",
        "                                   passes=20, eval_every=None)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 32.3 s, sys: 17.1 ms, total: 32.3 s\n",
            "Wall time: 32.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZpWk53derMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "517c30a8-e130-46a2-c9e4-acba3abf4b6d"
      },
      "source": [
        "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
        "    print('Topic #'+str(topic_id+1)+':')\n",
        "    print(topic)\n",
        "    print()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #18:\n",
            "0.061*\"numeric\" + 0.026*\"temperature\" + 0.024*\"university_california\" + 0.017*\"traffic\" + 0.016*\"ic_uci\" + 0.015*\"data_set\" + 0.015*\"text\" + 0.015*\"edu_ml\" + 0.015*\"location\" + 0.015*\"http_archive\" + 0.014*\"wind\" + 0.013*\"fatality\" + 0.013*\"school\" + 0.012*\"uci_machine\" + 0.012*\"weather\" + 0.011*\"learning_repository\" + 0.011*\"incident\" + 0.010*\"acknowledgement\" + 0.009*\"inspiration\" + 0.008*\"predict\"\n",
            "\n",
            "Topic #15:\n",
            "0.033*\"year\" + 0.025*\"energy\" + 0.021*\"total\" + 0.020*\"animal\" + 0.019*\"india\" + 0.017*\"solar\" + 0.016*\"rate\" + 0.014*\"response\" + 0.013*\"outcome\" + 0.013*\"customer\" + 0.012*\"natural\" + 0.012*\"space\" + 0.011*\"earth\" + 0.010*\"range\" + 0.009*\"wa\" + 0.008*\"acknowledgement\" + 0.008*\"type\" + 0.008*\"death\" + 0.008*\"based\" + 0.008*\"inspiration\"\n",
            "\n",
            "Topic #20:\n",
            "0.041*\"crime\" + 0.041*\"time\" + 0.038*\"score\" + 0.028*\"attack\" + 0.018*\"number\" + 0.018*\"defense\" + 0.014*\"submission\" + 0.014*\"non\" + 0.014*\"csv\" + 0.014*\"request\" + 0.014*\"statistic\" + 0.013*\"image\" + 0.013*\"matrix\" + 0.012*\"police\" + 0.012*\"file\" + 0.011*\"run\" + 0.010*\"name\" + 0.010*\"date\" + 0.009*\"id\" + 0.009*\"team\"\n",
            "\n",
            "Topic #12:\n",
            "0.033*\"column\" + 0.032*\"activity\" + 0.021*\"sensor\" + 0.020*\"feature\" + 0.018*\"signal\" + 0.017*\"right\" + 0.014*\"using\" + 0.013*\"left\" + 0.013*\"driving\" + 0.011*\"label\" + 0.010*\"subject\" + 0.010*\"participant\" + 0.009*\"open\" + 0.009*\"information\" + 0.009*\"class\" + 0.009*\"recognition\" + 0.008*\"time\" + 0.008*\"help\" + 0.008*\"database\" + 0.008*\"datasets\"\n",
            "\n",
            "Topic #1:\n",
            "0.231*\"university\" + 0.042*\"college\" + 0.028*\"map\" + 0.014*\"stop\" + 0.013*\"institute\" + 0.012*\"chicago\" + 0.010*\"technology\" + 0.009*\"file\" + 0.008*\"washington\" + 0.008*\"national\" + 0.008*\"archive\" + 0.008*\"new\" + 0.007*\"record\" + 0.007*\"population\" + 0.007*\"district\" + 0.007*\"project\" + 0.007*\"center\" + 0.007*\"research\" + 0.006*\"state\" + 0.006*\"analysis\"\n",
            "\n",
            "Topic #9:\n",
            "0.020*\"tweet\" + 0.018*\"wa\" + 0.013*\"com\" + 0.012*\"twitter\" + 0.012*\"one\" + 0.011*\"time\" + 0.011*\"like\" + 0.011*\"many\" + 0.011*\"inspiration\" + 0.010*\"song\" + 0.010*\"user\" + 0.010*\"column\" + 0.010*\"used\" + 0.009*\"would\" + 0.009*\"site\" + 0.009*\"acknowledgement\" + 0.008*\"people\" + 0.008*\"could\" + 0.007*\"find\" + 0.007*\"nltk\"\n",
            "\n",
            "Topic #17:\n",
            "0.023*\"city\" + 0.019*\"time\" + 0.018*\"information\" + 0.013*\"number\" + 0.012*\"location\" + 0.012*\"date\" + 0.010*\"new_york\" + 0.010*\"property\" + 0.010*\"acknowledgement\" + 0.009*\"election\" + 0.009*\"event\" + 0.009*\"restaurant\" + 0.008*\"name\" + 0.008*\"includes\" + 0.008*\"one\" + 0.007*\"inspiration\" + 0.007*\"station\" + 0.007*\"wa\" + 0.006*\"food\" + 0.006*\"day\"\n",
            "\n",
            "Topic #19:\n",
            "0.025*\"year\" + 0.016*\"country\" + 0.015*\"survey\" + 0.014*\"age\" + 0.013*\"number\" + 0.012*\"state\" + 0.011*\"name\" + 0.011*\"variable\" + 0.010*\"population\" + 0.008*\"month\" + 0.007*\"source\" + 0.007*\"information\" + 0.007*\"education\" + 0.007*\"student\" + 0.007*\"gender\" + 0.006*\"policy\" + 0.006*\"acknowledgement\" + 0.006*\"wa\" + 0.006*\"data_set\" + 0.006*\"rate\"\n",
            "\n",
            "Topic #2:\n",
            "0.010*\"wa\" + 0.009*\"time\" + 0.009*\"information\" + 0.008*\"source\" + 0.008*\"many\" + 0.008*\"world\" + 0.008*\"acknowledgement\" + 0.007*\"inspiration\" + 0.007*\"available\" + 0.007*\"project\" + 0.007*\"ha\" + 0.006*\"database\" + 0.006*\"one\" + 0.006*\"date\" + 0.006*\"also\" + 0.006*\"use\" + 0.006*\"country\" + 0.006*\"report\" + 0.005*\"number\" + 0.005*\"people\"\n",
            "\n",
            "Topic #13:\n",
            "0.011*\"acknowledgement\" + 0.010*\"information\" + 0.010*\"health\" + 0.009*\"wa\" + 0.009*\"state\" + 0.009*\"ha\" + 0.008*\"inspiration\" + 0.007*\"government\" + 0.007*\"company\" + 0.007*\"federal\" + 0.007*\"service\" + 0.006*\"published\" + 0.006*\"patient\" + 0.006*\"database\" + 0.006*\"year\" + 0.006*\"department\" + 0.006*\"vehicle\" + 0.006*\"risk\" + 0.005*\"doe\" + 0.005*\"gov\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inEFsCEseuXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f2a1ad7-aef3-450b-f0c9-6f6efa8df70f"
      },
      "source": [
        "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
        "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
        "print('Avg. Coherence Score:', avg_coherence_score)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Coherence Score: -2.5180356822814263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA28bQ3ieuc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02b51bfe-7795-4101-f2b1-72bb6e3f1942"
      },
      "source": [
        "topics_with_wts = [item[0] for item in topics_coherences]\n",
        "print('LDA Topics with Weights')\n",
        "print('='*50)\n",
        "for idx, topic in enumerate(topics_with_wts):\n",
        "    print('Topic #'+str(idx+1)+':')\n",
        "    print([(term, round(wt, 3)) for wt, term in topic])\n",
        "    print()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA Topics with Weights\n",
            "==================================================\n",
            "Topic #1:\n",
            "[('wa', 0.01), ('time', 0.009), ('information', 0.009), ('source', 0.008), ('many', 0.008), ('world', 0.008), ('acknowledgement', 0.008), ('inspiration', 0.007), ('available', 0.007), ('project', 0.007), ('ha', 0.007), ('database', 0.006), ('one', 0.006), ('date', 0.006), ('also', 0.006), ('use', 0.006), ('country', 0.006), ('report', 0.006), ('number', 0.005), ('people', 0.005)]\n",
            "\n",
            "Topic #2:\n",
            "[('file', 0.036), ('user', 0.026), ('movie', 0.025), ('csv', 0.024), ('tag', 0.015), ('rating', 0.013), ('id', 0.012), ('contains', 0.012), ('research', 0.011), ('use', 0.011), ('paper', 0.01), ('information', 0.01), ('ha', 0.009), ('one', 0.009), ('format', 0.009), ('value', 0.009), ('data_set', 0.008), ('used', 0.008), ('following', 0.007), ('system', 0.007)]\n",
            "\n",
            "Topic #3:\n",
            "[('image', 0.048), ('class', 0.016), ('label', 0.011), ('instance', 0.011), ('cell', 0.01), ('integer', 0.01), ('sample', 0.01), ('model', 0.01), ('number', 0.008), ('training', 0.008), ('using', 0.008), ('file', 0.007), ('classifier', 0.007), ('example', 0.007), ('different', 0.007), ('wa', 0.007), ('two', 0.007), ('machine_learning', 0.006), ('used', 0.006), ('test', 0.006)]\n",
            "\n",
            "Topic #4:\n",
            "[('attribute', 0.019), ('area', 0.012), ('variable', 0.011), ('building', 0.01), ('product', 0.01), ('per', 0.01), ('ha', 0.009), ('type', 0.009), ('day', 0.009), ('model', 0.008), ('value', 0.008), ('number', 0.008), ('feature', 0.008), ('cluster', 0.007), ('using', 0.007), ('use', 0.006), ('database', 0.006), ('one', 0.006), ('food', 0.006), ('point', 0.006)]\n",
            "\n",
            "Topic #5:\n",
            "[('year', 0.025), ('country', 0.016), ('survey', 0.015), ('age', 0.014), ('number', 0.013), ('state', 0.012), ('name', 0.011), ('variable', 0.011), ('population', 0.01), ('month', 0.008), ('source', 0.007), ('information', 0.007), ('education', 0.007), ('student', 0.007), ('gender', 0.007), ('policy', 0.006), ('acknowledgement', 0.006), ('wa', 0.006), ('data_set', 0.006), ('rate', 0.006)]\n",
            "\n",
            "Topic #6:\n",
            "[('text', 0.026), ('article', 0.019), ('title', 0.019), ('sentence', 0.016), ('speech', 0.013), ('file', 0.013), ('post', 0.012), ('line', 0.012), ('wa', 0.012), ('id', 0.01), ('using', 0.01), ('news', 0.009), ('corpus', 0.008), ('ha', 0.008), ('used', 0.007), ('inspiration', 0.007), ('word', 0.007), ('en', 0.007), ('google', 0.007), ('comment', 0.007)]\n",
            "\n",
            "Topic #7:\n",
            "[('city', 0.023), ('time', 0.019), ('information', 0.018), ('number', 0.013), ('location', 0.012), ('date', 0.012), ('new_york', 0.01), ('property', 0.01), ('acknowledgement', 0.01), ('election', 0.009), ('event', 0.009), ('restaurant', 0.009), ('name', 0.008), ('includes', 0.008), ('one', 0.008), ('inspiration', 0.007), ('station', 0.007), ('wa', 0.007), ('food', 0.006), ('day', 0.006)]\n",
            "\n",
            "Topic #8:\n",
            "[('tweet', 0.02), ('wa', 0.018), ('com', 0.013), ('twitter', 0.012), ('one', 0.012), ('time', 0.011), ('like', 0.011), ('many', 0.011), ('inspiration', 0.011), ('song', 0.01), ('user', 0.01), ('column', 0.01), ('used', 0.01), ('would', 0.009), ('site', 0.009), ('acknowledgement', 0.009), ('people', 0.008), ('could', 0.008), ('find', 0.007), ('nltk', 0.007)]\n",
            "\n",
            "Topic #9:\n",
            "[('name', 0.021), ('code', 0.019), ('value', 0.015), ('fire', 0.013), ('county', 0.012), ('state', 0.012), ('air', 0.011), ('row', 0.01), ('csv', 0.01), ('day', 0.01), ('year', 0.009), ('number', 0.009), ('specie', 0.009), ('event', 0.008), ('unit', 0.008), ('hour', 0.008), ('time', 0.008), ('mean', 0.008), ('date', 0.008), ('measured', 0.008)]\n",
            "\n",
            "Topic #10:\n",
            "[('word', 0.06), ('review', 0.029), ('language', 0.026), ('corpus', 0.025), ('name', 0.023), ('file', 0.022), ('http', 0.02), ('license', 0.017), ('text', 0.014), ('code', 0.014), ('english', 0.013), ('copyright', 0.012), ('version', 0.011), ('com', 0.011), ('list', 0.011), ('use', 0.009), ('org', 0.009), ('metadata', 0.009), ('unzip', 0.009), ('frequency', 0.008)]\n",
            "\n",
            "Topic #11:\n",
            "[('column', 0.033), ('activity', 0.032), ('sensor', 0.021), ('feature', 0.02), ('signal', 0.018), ('right', 0.017), ('using', 0.014), ('left', 0.013), ('driving', 0.013), ('label', 0.011), ('subject', 0.01), ('participant', 0.01), ('open', 0.009), ('information', 0.009), ('class', 0.009), ('recognition', 0.009), ('time', 0.008), ('help', 0.008), ('database', 0.008), ('datasets', 0.008)]\n",
            "\n",
            "Topic #12:\n",
            "[('price', 0.06), ('csv', 0.037), ('de', 0.028), ('stock', 0.023), ('com', 0.021), ('question', 0.02), ('date', 0.019), ('market', 0.015), ('acknowledgement', 0.012), ('http_www', 0.011), ('day', 0.011), ('en', 0.011), ('inspiration', 0.01), ('company', 0.01), ('close', 0.009), ('open', 0.009), ('volume', 0.008), ('la', 0.008), ('index', 0.008), ('per', 0.008)]\n",
            "\n",
            "Topic #13:\n",
            "[('university', 0.231), ('college', 0.042), ('map', 0.028), ('stop', 0.014), ('institute', 0.013), ('chicago', 0.012), ('technology', 0.01), ('file', 0.009), ('washington', 0.008), ('national', 0.008), ('archive', 0.008), ('new', 0.008), ('record', 0.007), ('population', 0.007), ('district', 0.007), ('project', 0.007), ('center', 0.007), ('research', 0.007), ('state', 0.006), ('analysis', 0.006)]\n",
            "\n",
            "Topic #14:\n",
            "[('acknowledgement', 0.011), ('information', 0.01), ('health', 0.01), ('wa', 0.009), ('state', 0.009), ('ha', 0.009), ('inspiration', 0.008), ('government', 0.007), ('company', 0.007), ('federal', 0.007), ('service', 0.007), ('published', 0.006), ('patient', 0.006), ('database', 0.006), ('year', 0.006), ('department', 0.006), ('vehicle', 0.006), ('risk', 0.006), ('doe', 0.005), ('gov', 0.005)]\n",
            "\n",
            "Topic #15:\n",
            "[('crime', 0.041), ('time', 0.041), ('score', 0.038), ('attack', 0.028), ('number', 0.018), ('defense', 0.018), ('submission', 0.014), ('non', 0.014), ('csv', 0.014), ('request', 0.014), ('statistic', 0.014), ('image', 0.013), ('matrix', 0.013), ('police', 0.012), ('file', 0.012), ('run', 0.011), ('name', 0.01), ('date', 0.01), ('id', 0.009), ('team', 0.009)]\n",
            "\n",
            "Topic #16:\n",
            "[('player', 0.039), ('team', 0.034), ('game', 0.028), ('match', 0.021), ('season', 0.014), ('goal', 0.01), ('wa', 0.01), ('com', 0.008), ('table', 0.007), ('point', 0.007), ('result', 0.007), ('play', 0.007), ('information', 0.006), ('league', 0.006), ('pre_trained', 0.006), ('sport', 0.006), ('use', 0.006), ('played', 0.006), ('model', 0.006), ('win', 0.006)]\n",
            "\n",
            "Topic #17:\n",
            "[('car', 0.033), ('inspiration', 0.027), ('largest', 0.021), ('past_research', 0.021), ('science_community', 0.021), ('acquired', 0.02), ('front_world', 0.02), ('question_want', 0.02), ('see_answered', 0.02), ('data_set', 0.02), ('help_others', 0.019), ('represents', 0.019), ('acknowledgement_without', 0.019), ('along_citation', 0.018), ('owe_attribution', 0.018), ('thanks_include', 0.018), ('time_period', 0.018), ('package_id', 0.016), ('content_inside', 0.015), ('every', 0.015)]\n",
            "\n",
            "Topic #18:\n",
            "[('year', 0.033), ('energy', 0.025), ('total', 0.021), ('animal', 0.02), ('india', 0.019), ('solar', 0.017), ('rate', 0.016), ('response', 0.014), ('outcome', 0.013), ('customer', 0.013), ('natural', 0.012), ('space', 0.012), ('earth', 0.011), ('range', 0.01), ('wa', 0.009), ('acknowledgement', 0.008), ('type', 0.008), ('death', 0.008), ('based', 0.008), ('inspiration', 0.008)]\n",
            "\n",
            "Topic #19:\n",
            "[('yet', 0.041), ('doe_description', 0.036), ('collection', 0.026), ('text', 0.026), ('txt', 0.02), ('use', 0.017), ('street', 0.017), ('south', 0.016), ('american', 0.015), ('csv', 0.014), ('library', 0.013), ('number', 0.012), ('city', 0.011), ('name', 0.009), ('digital', 0.009), ('tool', 0.009), ('http', 0.009), ('license', 0.008), ('open', 0.008), ('new_york', 0.007)]\n",
            "\n",
            "Topic #20:\n",
            "[('numeric', 0.061), ('temperature', 0.026), ('university_california', 0.024), ('traffic', 0.017), ('ic_uci', 0.016), ('data_set', 0.015), ('text', 0.015), ('edu_ml', 0.015), ('location', 0.015), ('http_archive', 0.015), ('wind', 0.014), ('fatality', 0.013), ('school', 0.013), ('uci_machine', 0.012), ('weather', 0.012), ('learning_repository', 0.011), ('incident', 0.011), ('acknowledgement', 0.01), ('inspiration', 0.009), ('predict', 0.008)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8TOC8axezZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e69e42dd-d343-4cd2-9af2-792a5e49ada2"
      },
      "source": [
        "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
        "                                                      texts=norm_corpus_bigrams,\n",
        "                                                      dictionary=dictionary, \n",
        "                                                      coherence='c_v')\n",
        "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
        "\n",
        "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
        "                                                         texts=norm_corpus_bigrams,\n",
        "                                                         dictionary=dictionary, \n",
        "                                                         coherence='u_mass')\n",
        "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
        "\n",
        "perplexity = lda_model.log_perplexity(bow_corpus)\n",
        "\n",
        "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
        "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
        "print('Model Perplexity:', perplexity)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Coherence Score (Cv): 0.42230731344440936\n",
            "Avg. Coherence Score (UMass): -2.5180356822814263\n",
            "Model Perplexity: -6.85161118426254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnWEo5vse85t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics = [[(term, round(wt, 3)) \n",
        "               for term, wt in lda_model.show_topic(n, topn=20)] \n",
        "                   for n in range(0, lda_model.num_topics)]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfUS4_F8frVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85e35cd0-c25f-4cd9-bd3b-b64135c3927a"
      },
      "source": [
        "for idx, topic in enumerate(topics):\n",
        "    print('Topic #'+str(idx+1)+':')\n",
        "    print([term for term, wt in topic])\n",
        "    print()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "['university', 'college', 'map', 'stop', 'institute', 'chicago', 'technology', 'file', 'washington', 'national', 'archive', 'new', 'record', 'population', 'district', 'project', 'center', 'research', 'state', 'analysis']\n",
            "\n",
            "Topic #2:\n",
            "['wa', 'time', 'information', 'source', 'many', 'world', 'acknowledgement', 'inspiration', 'available', 'project', 'ha', 'database', 'one', 'date', 'also', 'use', 'country', 'report', 'number', 'people']\n",
            "\n",
            "Topic #3:\n",
            "['image', 'class', 'label', 'instance', 'cell', 'integer', 'sample', 'model', 'number', 'training', 'using', 'file', 'classifier', 'example', 'different', 'wa', 'two', 'machine_learning', 'used', 'test']\n",
            "\n",
            "Topic #4:\n",
            "['word', 'review', 'language', 'corpus', 'name', 'file', 'http', 'license', 'text', 'code', 'english', 'copyright', 'version', 'com', 'list', 'use', 'org', 'metadata', 'unzip', 'frequency']\n",
            "\n",
            "Topic #5:\n",
            "['text', 'article', 'title', 'sentence', 'speech', 'file', 'post', 'line', 'wa', 'id', 'using', 'news', 'corpus', 'ha', 'used', 'inspiration', 'word', 'en', 'google', 'comment']\n",
            "\n",
            "Topic #6:\n",
            "['price', 'csv', 'de', 'stock', 'com', 'question', 'date', 'market', 'acknowledgement', 'http_www', 'day', 'en', 'inspiration', 'company', 'close', 'open', 'volume', 'la', 'index', 'per']\n",
            "\n",
            "Topic #7:\n",
            "['yet', 'doe_description', 'collection', 'text', 'txt', 'use', 'street', 'south', 'american', 'csv', 'library', 'number', 'city', 'name', 'digital', 'tool', 'http', 'license', 'open', 'new_york']\n",
            "\n",
            "Topic #8:\n",
            "['file', 'user', 'movie', 'csv', 'tag', 'rating', 'id', 'contains', 'research', 'use', 'paper', 'information', 'ha', 'one', 'format', 'value', 'data_set', 'used', 'following', 'system']\n",
            "\n",
            "Topic #9:\n",
            "['tweet', 'wa', 'com', 'twitter', 'one', 'time', 'like', 'many', 'inspiration', 'song', 'user', 'column', 'used', 'would', 'site', 'acknowledgement', 'people', 'could', 'find', 'nltk']\n",
            "\n",
            "Topic #10:\n",
            "['attribute', 'area', 'variable', 'building', 'product', 'per', 'ha', 'type', 'day', 'model', 'value', 'number', 'feature', 'cluster', 'using', 'use', 'database', 'one', 'food', 'point']\n",
            "\n",
            "Topic #11:\n",
            "['car', 'inspiration', 'largest', 'past_research', 'science_community', 'acquired', 'front_world', 'question_want', 'see_answered', 'data_set', 'help_others', 'represents', 'acknowledgement_without', 'along_citation', 'owe_attribution', 'thanks_include', 'time_period', 'package_id', 'content_inside', 'every']\n",
            "\n",
            "Topic #12:\n",
            "['column', 'activity', 'sensor', 'feature', 'signal', 'right', 'using', 'left', 'driving', 'label', 'subject', 'participant', 'open', 'information', 'class', 'recognition', 'time', 'help', 'database', 'datasets']\n",
            "\n",
            "Topic #13:\n",
            "['acknowledgement', 'information', 'health', 'wa', 'state', 'ha', 'inspiration', 'government', 'company', 'federal', 'service', 'published', 'patient', 'database', 'year', 'department', 'vehicle', 'risk', 'doe', 'gov']\n",
            "\n",
            "Topic #14:\n",
            "['name', 'code', 'value', 'fire', 'county', 'state', 'air', 'row', 'csv', 'day', 'year', 'number', 'specie', 'event', 'unit', 'hour', 'time', 'mean', 'date', 'measured']\n",
            "\n",
            "Topic #15:\n",
            "['year', 'energy', 'total', 'animal', 'india', 'solar', 'rate', 'response', 'outcome', 'customer', 'natural', 'space', 'earth', 'range', 'wa', 'acknowledgement', 'type', 'death', 'based', 'inspiration']\n",
            "\n",
            "Topic #16:\n",
            "['player', 'team', 'game', 'match', 'season', 'goal', 'wa', 'com', 'table', 'point', 'result', 'play', 'information', 'league', 'pre_trained', 'sport', 'use', 'played', 'model', 'win']\n",
            "\n",
            "Topic #17:\n",
            "['city', 'time', 'information', 'number', 'location', 'date', 'new_york', 'property', 'acknowledgement', 'election', 'event', 'restaurant', 'name', 'includes', 'one', 'inspiration', 'station', 'wa', 'food', 'day']\n",
            "\n",
            "Topic #18:\n",
            "['numeric', 'temperature', 'university_california', 'traffic', 'ic_uci', 'data_set', 'text', 'edu_ml', 'location', 'http_archive', 'wind', 'fatality', 'school', 'uci_machine', 'weather', 'learning_repository', 'incident', 'acknowledgement', 'inspiration', 'predict']\n",
            "\n",
            "Topic #19:\n",
            "['year', 'country', 'survey', 'age', 'number', 'state', 'name', 'variable', 'population', 'month', 'source', 'information', 'education', 'student', 'gender', 'policy', 'acknowledgement', 'wa', 'data_set', 'rate']\n",
            "\n",
            "Topic #20:\n",
            "['crime', 'time', 'score', 'attack', 'number', 'defense', 'submission', 'non', 'csv', 'request', 'statistic', 'image', 'matrix', 'police', 'file', 'run', 'name', 'date', 'id', 'team']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eI8KbDmfvmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "c8ef99bd-92d8-4de6-d47c-8ff51d205630"
      },
      "source": [
        "topics_df = pd.DataFrame([[term for term, wt in topic] \n",
        "                              for topic in topics], \n",
        "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
        "                         index=['Topic '+str(t) for t in range(1, lda_model.num_topics+1)]).T\n",
        "topics_df"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "      <th>Topic 4</th>\n",
              "      <th>Topic 5</th>\n",
              "      <th>Topic 6</th>\n",
              "      <th>Topic 7</th>\n",
              "      <th>Topic 8</th>\n",
              "      <th>Topic 9</th>\n",
              "      <th>Topic 10</th>\n",
              "      <th>Topic 11</th>\n",
              "      <th>Topic 12</th>\n",
              "      <th>Topic 13</th>\n",
              "      <th>Topic 14</th>\n",
              "      <th>Topic 15</th>\n",
              "      <th>Topic 16</th>\n",
              "      <th>Topic 17</th>\n",
              "      <th>Topic 18</th>\n",
              "      <th>Topic 19</th>\n",
              "      <th>Topic 20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Term1</th>\n",
              "      <td>university</td>\n",
              "      <td>wa</td>\n",
              "      <td>image</td>\n",
              "      <td>word</td>\n",
              "      <td>text</td>\n",
              "      <td>price</td>\n",
              "      <td>yet</td>\n",
              "      <td>file</td>\n",
              "      <td>tweet</td>\n",
              "      <td>attribute</td>\n",
              "      <td>car</td>\n",
              "      <td>column</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>name</td>\n",
              "      <td>year</td>\n",
              "      <td>player</td>\n",
              "      <td>city</td>\n",
              "      <td>numeric</td>\n",
              "      <td>year</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term2</th>\n",
              "      <td>college</td>\n",
              "      <td>time</td>\n",
              "      <td>class</td>\n",
              "      <td>review</td>\n",
              "      <td>article</td>\n",
              "      <td>csv</td>\n",
              "      <td>doe_description</td>\n",
              "      <td>user</td>\n",
              "      <td>wa</td>\n",
              "      <td>area</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>activity</td>\n",
              "      <td>information</td>\n",
              "      <td>code</td>\n",
              "      <td>energy</td>\n",
              "      <td>team</td>\n",
              "      <td>time</td>\n",
              "      <td>temperature</td>\n",
              "      <td>country</td>\n",
              "      <td>time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term3</th>\n",
              "      <td>map</td>\n",
              "      <td>information</td>\n",
              "      <td>label</td>\n",
              "      <td>language</td>\n",
              "      <td>title</td>\n",
              "      <td>de</td>\n",
              "      <td>collection</td>\n",
              "      <td>movie</td>\n",
              "      <td>com</td>\n",
              "      <td>variable</td>\n",
              "      <td>largest</td>\n",
              "      <td>sensor</td>\n",
              "      <td>health</td>\n",
              "      <td>value</td>\n",
              "      <td>total</td>\n",
              "      <td>game</td>\n",
              "      <td>information</td>\n",
              "      <td>university_california</td>\n",
              "      <td>survey</td>\n",
              "      <td>score</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term4</th>\n",
              "      <td>stop</td>\n",
              "      <td>source</td>\n",
              "      <td>instance</td>\n",
              "      <td>corpus</td>\n",
              "      <td>sentence</td>\n",
              "      <td>stock</td>\n",
              "      <td>text</td>\n",
              "      <td>csv</td>\n",
              "      <td>twitter</td>\n",
              "      <td>building</td>\n",
              "      <td>past_research</td>\n",
              "      <td>feature</td>\n",
              "      <td>wa</td>\n",
              "      <td>fire</td>\n",
              "      <td>animal</td>\n",
              "      <td>match</td>\n",
              "      <td>number</td>\n",
              "      <td>traffic</td>\n",
              "      <td>age</td>\n",
              "      <td>attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term5</th>\n",
              "      <td>institute</td>\n",
              "      <td>many</td>\n",
              "      <td>cell</td>\n",
              "      <td>name</td>\n",
              "      <td>speech</td>\n",
              "      <td>com</td>\n",
              "      <td>txt</td>\n",
              "      <td>tag</td>\n",
              "      <td>one</td>\n",
              "      <td>product</td>\n",
              "      <td>science_community</td>\n",
              "      <td>signal</td>\n",
              "      <td>state</td>\n",
              "      <td>county</td>\n",
              "      <td>india</td>\n",
              "      <td>season</td>\n",
              "      <td>location</td>\n",
              "      <td>ic_uci</td>\n",
              "      <td>number</td>\n",
              "      <td>number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term6</th>\n",
              "      <td>chicago</td>\n",
              "      <td>world</td>\n",
              "      <td>integer</td>\n",
              "      <td>file</td>\n",
              "      <td>file</td>\n",
              "      <td>question</td>\n",
              "      <td>use</td>\n",
              "      <td>rating</td>\n",
              "      <td>time</td>\n",
              "      <td>per</td>\n",
              "      <td>acquired</td>\n",
              "      <td>right</td>\n",
              "      <td>ha</td>\n",
              "      <td>state</td>\n",
              "      <td>solar</td>\n",
              "      <td>goal</td>\n",
              "      <td>date</td>\n",
              "      <td>data_set</td>\n",
              "      <td>state</td>\n",
              "      <td>defense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term7</th>\n",
              "      <td>technology</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>sample</td>\n",
              "      <td>http</td>\n",
              "      <td>post</td>\n",
              "      <td>date</td>\n",
              "      <td>street</td>\n",
              "      <td>id</td>\n",
              "      <td>like</td>\n",
              "      <td>ha</td>\n",
              "      <td>front_world</td>\n",
              "      <td>using</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>air</td>\n",
              "      <td>rate</td>\n",
              "      <td>wa</td>\n",
              "      <td>new_york</td>\n",
              "      <td>text</td>\n",
              "      <td>name</td>\n",
              "      <td>submission</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term8</th>\n",
              "      <td>file</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>model</td>\n",
              "      <td>license</td>\n",
              "      <td>line</td>\n",
              "      <td>market</td>\n",
              "      <td>south</td>\n",
              "      <td>contains</td>\n",
              "      <td>many</td>\n",
              "      <td>type</td>\n",
              "      <td>question_want</td>\n",
              "      <td>left</td>\n",
              "      <td>government</td>\n",
              "      <td>row</td>\n",
              "      <td>response</td>\n",
              "      <td>com</td>\n",
              "      <td>property</td>\n",
              "      <td>edu_ml</td>\n",
              "      <td>variable</td>\n",
              "      <td>non</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term9</th>\n",
              "      <td>washington</td>\n",
              "      <td>available</td>\n",
              "      <td>number</td>\n",
              "      <td>text</td>\n",
              "      <td>wa</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>american</td>\n",
              "      <td>research</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>day</td>\n",
              "      <td>see_answered</td>\n",
              "      <td>driving</td>\n",
              "      <td>company</td>\n",
              "      <td>csv</td>\n",
              "      <td>outcome</td>\n",
              "      <td>table</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>location</td>\n",
              "      <td>population</td>\n",
              "      <td>csv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term10</th>\n",
              "      <td>national</td>\n",
              "      <td>project</td>\n",
              "      <td>training</td>\n",
              "      <td>code</td>\n",
              "      <td>id</td>\n",
              "      <td>http_www</td>\n",
              "      <td>csv</td>\n",
              "      <td>use</td>\n",
              "      <td>song</td>\n",
              "      <td>model</td>\n",
              "      <td>data_set</td>\n",
              "      <td>label</td>\n",
              "      <td>federal</td>\n",
              "      <td>day</td>\n",
              "      <td>customer</td>\n",
              "      <td>point</td>\n",
              "      <td>election</td>\n",
              "      <td>http_archive</td>\n",
              "      <td>month</td>\n",
              "      <td>request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term11</th>\n",
              "      <td>archive</td>\n",
              "      <td>ha</td>\n",
              "      <td>using</td>\n",
              "      <td>english</td>\n",
              "      <td>using</td>\n",
              "      <td>day</td>\n",
              "      <td>library</td>\n",
              "      <td>paper</td>\n",
              "      <td>user</td>\n",
              "      <td>value</td>\n",
              "      <td>help_others</td>\n",
              "      <td>subject</td>\n",
              "      <td>service</td>\n",
              "      <td>year</td>\n",
              "      <td>natural</td>\n",
              "      <td>result</td>\n",
              "      <td>event</td>\n",
              "      <td>wind</td>\n",
              "      <td>source</td>\n",
              "      <td>statistic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term12</th>\n",
              "      <td>new</td>\n",
              "      <td>database</td>\n",
              "      <td>file</td>\n",
              "      <td>copyright</td>\n",
              "      <td>news</td>\n",
              "      <td>en</td>\n",
              "      <td>number</td>\n",
              "      <td>information</td>\n",
              "      <td>column</td>\n",
              "      <td>number</td>\n",
              "      <td>represents</td>\n",
              "      <td>participant</td>\n",
              "      <td>published</td>\n",
              "      <td>number</td>\n",
              "      <td>space</td>\n",
              "      <td>play</td>\n",
              "      <td>restaurant</td>\n",
              "      <td>fatality</td>\n",
              "      <td>information</td>\n",
              "      <td>image</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term13</th>\n",
              "      <td>record</td>\n",
              "      <td>one</td>\n",
              "      <td>classifier</td>\n",
              "      <td>version</td>\n",
              "      <td>corpus</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>city</td>\n",
              "      <td>ha</td>\n",
              "      <td>used</td>\n",
              "      <td>feature</td>\n",
              "      <td>acknowledgement_without</td>\n",
              "      <td>open</td>\n",
              "      <td>patient</td>\n",
              "      <td>specie</td>\n",
              "      <td>earth</td>\n",
              "      <td>information</td>\n",
              "      <td>name</td>\n",
              "      <td>school</td>\n",
              "      <td>education</td>\n",
              "      <td>matrix</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term14</th>\n",
              "      <td>population</td>\n",
              "      <td>date</td>\n",
              "      <td>example</td>\n",
              "      <td>com</td>\n",
              "      <td>ha</td>\n",
              "      <td>company</td>\n",
              "      <td>name</td>\n",
              "      <td>one</td>\n",
              "      <td>would</td>\n",
              "      <td>cluster</td>\n",
              "      <td>along_citation</td>\n",
              "      <td>information</td>\n",
              "      <td>database</td>\n",
              "      <td>event</td>\n",
              "      <td>range</td>\n",
              "      <td>league</td>\n",
              "      <td>includes</td>\n",
              "      <td>uci_machine</td>\n",
              "      <td>student</td>\n",
              "      <td>police</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term15</th>\n",
              "      <td>district</td>\n",
              "      <td>also</td>\n",
              "      <td>different</td>\n",
              "      <td>list</td>\n",
              "      <td>used</td>\n",
              "      <td>close</td>\n",
              "      <td>digital</td>\n",
              "      <td>format</td>\n",
              "      <td>site</td>\n",
              "      <td>using</td>\n",
              "      <td>owe_attribution</td>\n",
              "      <td>class</td>\n",
              "      <td>year</td>\n",
              "      <td>unit</td>\n",
              "      <td>wa</td>\n",
              "      <td>pre_trained</td>\n",
              "      <td>one</td>\n",
              "      <td>weather</td>\n",
              "      <td>gender</td>\n",
              "      <td>file</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term16</th>\n",
              "      <td>project</td>\n",
              "      <td>use</td>\n",
              "      <td>wa</td>\n",
              "      <td>use</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>open</td>\n",
              "      <td>tool</td>\n",
              "      <td>value</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>use</td>\n",
              "      <td>thanks_include</td>\n",
              "      <td>recognition</td>\n",
              "      <td>department</td>\n",
              "      <td>hour</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>sport</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>learning_repository</td>\n",
              "      <td>policy</td>\n",
              "      <td>run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term17</th>\n",
              "      <td>center</td>\n",
              "      <td>country</td>\n",
              "      <td>two</td>\n",
              "      <td>org</td>\n",
              "      <td>word</td>\n",
              "      <td>volume</td>\n",
              "      <td>http</td>\n",
              "      <td>data_set</td>\n",
              "      <td>people</td>\n",
              "      <td>database</td>\n",
              "      <td>time_period</td>\n",
              "      <td>time</td>\n",
              "      <td>vehicle</td>\n",
              "      <td>time</td>\n",
              "      <td>type</td>\n",
              "      <td>use</td>\n",
              "      <td>station</td>\n",
              "      <td>incident</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term18</th>\n",
              "      <td>research</td>\n",
              "      <td>report</td>\n",
              "      <td>machine_learning</td>\n",
              "      <td>metadata</td>\n",
              "      <td>en</td>\n",
              "      <td>la</td>\n",
              "      <td>license</td>\n",
              "      <td>used</td>\n",
              "      <td>could</td>\n",
              "      <td>one</td>\n",
              "      <td>package_id</td>\n",
              "      <td>help</td>\n",
              "      <td>risk</td>\n",
              "      <td>mean</td>\n",
              "      <td>death</td>\n",
              "      <td>played</td>\n",
              "      <td>wa</td>\n",
              "      <td>acknowledgement</td>\n",
              "      <td>wa</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term19</th>\n",
              "      <td>state</td>\n",
              "      <td>number</td>\n",
              "      <td>used</td>\n",
              "      <td>unzip</td>\n",
              "      <td>google</td>\n",
              "      <td>index</td>\n",
              "      <td>open</td>\n",
              "      <td>following</td>\n",
              "      <td>find</td>\n",
              "      <td>food</td>\n",
              "      <td>content_inside</td>\n",
              "      <td>database</td>\n",
              "      <td>doe</td>\n",
              "      <td>date</td>\n",
              "      <td>based</td>\n",
              "      <td>model</td>\n",
              "      <td>food</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>data_set</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term20</th>\n",
              "      <td>analysis</td>\n",
              "      <td>people</td>\n",
              "      <td>test</td>\n",
              "      <td>frequency</td>\n",
              "      <td>comment</td>\n",
              "      <td>per</td>\n",
              "      <td>new_york</td>\n",
              "      <td>system</td>\n",
              "      <td>nltk</td>\n",
              "      <td>point</td>\n",
              "      <td>every</td>\n",
              "      <td>datasets</td>\n",
              "      <td>gov</td>\n",
              "      <td>measured</td>\n",
              "      <td>inspiration</td>\n",
              "      <td>win</td>\n",
              "      <td>day</td>\n",
              "      <td>predict</td>\n",
              "      <td>rate</td>\n",
              "      <td>team</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Topic 1          Topic 2  ...         Topic 19    Topic 20\n",
              "Term1   university               wa  ...             year       crime\n",
              "Term2      college             time  ...          country        time\n",
              "Term3          map      information  ...           survey       score\n",
              "Term4         stop           source  ...              age      attack\n",
              "Term5    institute             many  ...           number      number\n",
              "Term6      chicago            world  ...            state     defense\n",
              "Term7   technology  acknowledgement  ...             name  submission\n",
              "Term8         file      inspiration  ...         variable         non\n",
              "Term9   washington        available  ...       population         csv\n",
              "Term10    national          project  ...            month     request\n",
              "Term11     archive               ha  ...           source   statistic\n",
              "Term12         new         database  ...      information       image\n",
              "Term13      record              one  ...        education      matrix\n",
              "Term14  population             date  ...          student      police\n",
              "Term15    district             also  ...           gender        file\n",
              "Term16     project              use  ...           policy         run\n",
              "Term17      center          country  ...  acknowledgement        name\n",
              "Term18    research           report  ...               wa        date\n",
              "Term19       state           number  ...         data_set          id\n",
              "Term20    analysis           people  ...             rate        team\n",
              "\n",
              "[20 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blVMu1Npf0y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb7c51ce-5c9d-45a3-dd0e-e9e26b468e6e"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
        "                              for topic in topics],\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, lda_model.num_topics+1)]\n",
        "                         )\n",
        "topics_df"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>university, college, map, stop, institute, chicago, technology, file, washington, national, archive, new, record, population, district, project, center, research, state, analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>image, class, label, instance, cell, integer, sample, model, number, training, using, file, classifier, example, different, wa, two, machine_learning, used, test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>text, article, title, sentence, speech, file, post, line, wa, id, using, news, corpus, ha, used, inspiration, word, en, google, comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>price, csv, de, stock, com, question, date, market, acknowledgement, http_www, day, en, inspiration, company, close, open, volume, la, index, per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>yet, doe_description, collection, text, txt, use, street, south, american, csv, library, number, city, name, digital, tool, http, license, open, new_york</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>file, user, movie, csv, tag, rating, id, contains, research, use, paper, information, ha, one, format, value, data_set, used, following, system</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>car, inspiration, largest, past_research, science_community, acquired, front_world, question_want, see_answered, data_set, help_others, represents, acknowledgement_without, along_citation, owe_attribution, thanks_include, time_period, package_id, content_inside, every</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>column, activity, sensor, feature, signal, right, using, left, driving, label, subject, participant, open, information, class, recognition, time, help, database, datasets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>name, code, value, fire, county, state, air, row, csv, day, year, number, specie, event, unit, hour, time, mean, date, measured</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>year, energy, total, animal, india, solar, rate, response, outcome, customer, natural, space, earth, range, wa, acknowledgement, type, death, based, inspiration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>city, time, information, number, location, date, new_york, property, acknowledgement, election, event, restaurant, name, includes, one, inspiration, station, wa, food, day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowledgement, inspiration, predict</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>crime, time, score, attack, number, defense, submission, non, csv, request, statistic, image, matrix, police, file, run, name, date, id, team</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                      Terms per Topic\n",
              "Topic1   university, college, map, stop, institute, chicago, technology, file, washington, national, archive, new, record, population, district, project, center, research, state, analysis                                                                                          \n",
              "Topic2   wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people                                                                                                           \n",
              "Topic3   image, class, label, instance, cell, integer, sample, model, number, training, using, file, classifier, example, different, wa, two, machine_learning, used, test                                                                                                           \n",
              "Topic4   word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency                                                                                                                         \n",
              "Topic5   text, article, title, sentence, speech, file, post, line, wa, id, using, news, corpus, ha, used, inspiration, word, en, google, comment                                                                                                                                     \n",
              "Topic6   price, csv, de, stock, com, question, date, market, acknowledgement, http_www, day, en, inspiration, company, close, open, volume, la, index, per                                                                                                                           \n",
              "Topic7   yet, doe_description, collection, text, txt, use, street, south, american, csv, library, number, city, name, digital, tool, http, license, open, new_york                                                                                                                   \n",
              "Topic8   file, user, movie, csv, tag, rating, id, contains, research, use, paper, information, ha, one, format, value, data_set, used, following, system                                                                                                                             \n",
              "Topic9   tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk                                                                                                                              \n",
              "Topic10  attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point                                                                                                                            \n",
              "Topic11  car, inspiration, largest, past_research, science_community, acquired, front_world, question_want, see_answered, data_set, help_others, represents, acknowledgement_without, along_citation, owe_attribution, thanks_include, time_period, package_id, content_inside, every\n",
              "Topic12  column, activity, sensor, feature, signal, right, using, left, driving, label, subject, participant, open, information, class, recognition, time, help, database, datasets                                                                                                  \n",
              "Topic13  acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov                                                                                            \n",
              "Topic14  name, code, value, fire, county, state, air, row, csv, day, year, number, specie, event, unit, hour, time, mean, date, measured                                                                                                                                             \n",
              "Topic15  year, energy, total, animal, india, solar, rate, response, outcome, customer, natural, space, earth, range, wa, acknowledgement, type, death, based, inspiration                                                                                                            \n",
              "Topic16  player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win                                                                                                                              \n",
              "Topic17  city, time, information, number, location, date, new_york, property, acknowledgement, election, event, restaurant, name, includes, one, inspiration, station, wa, food, day                                                                                                 \n",
              "Topic18  numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowledgement, inspiration, predict                                            \n",
              "Topic19  year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate                                                                                                  \n",
              "Topic20  crime, time, score, attack, number, defense, submission, non, csv, request, statistic, image, matrix, police, file, run, name, date, id, team                                                                                                                               "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK67-x2dgULw",
        "colab_type": "text"
      },
      "source": [
        "## Interpreting Topic Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-pNi8H0gBli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tm_results = lda_model[bow_corpus]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4mfPgPKgYPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cf45e005-99d4-4aee-e04f-feae6702c053"
      },
      "source": [
        "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] \n",
        "                     for topics in tm_results]\n",
        "corpus_topics[:5]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, 0.5704999),\n",
              " (15, 0.5066352),\n",
              " (1, 0.4261907),\n",
              " (1, 0.32978037),\n",
              " (5, 0.447578)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PtIwnb3gZJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_topic_df = pd.DataFrame()\n",
        "corpus_topic_df['Document'] = range(0, len(papers))\n",
        "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
        "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
        "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
        "corpus_topic_df['Paper'] = papers"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mZl0bvDgcGX",
        "colab_type": "text"
      },
      "source": [
        "### Dominant Topics Distribution across Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srKjC-qFjy0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "1eade1db-0fea-4651-a89f-c0f29e926cde"
      },
      "source": [
        "corpus_topic_df.head(2)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>57.05</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>50.66</td>\n",
              "      <td>player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win</td>\n",
              "      <td>The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlay...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document  ...                                                                                                                                                                                                    Paper\n",
              "0         0  ...  The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...\n",
              "1         1  ...  The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlay...\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePOdQJxFkS-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1faba6f-4721-4b3b-8c03-73cd9455f91c"
      },
      "source": [
        "corpus_topic_df.columns"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Document', 'Dominant Topic', 'Contribution %', 'Topic Desc', 'Paper'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzvm7j3Og77a",
        "colab_type": "text"
      },
      "source": [
        "### Dominant Topics in Specific Research Papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodNBQD5geiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2c1cb1d-d2eb-45fc-a32e-f6ba057dc669"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 200)\n",
        "(corpus_topic_df[corpus_topic_df['Document']\n",
        "                 .isin([681, 9, 392, 1622, 17, \n",
        "                        906, 996, 503, 13, 733])])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>43.52</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>99.67</td>\n",
              "      <td>numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowled...</td>\n",
              "      <td>A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>53.98</td>\n",
              "      <td>word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency</td>\n",
              "      <td>Context\\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>392</td>\n",
              "      <td>2</td>\n",
              "      <td>34.49</td>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "      <td>Context\\nBible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>503</td>\n",
              "      <td>19</td>\n",
              "      <td>44.98</td>\n",
              "      <td>year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate</td>\n",
              "      <td>Description:\\nHappyDB is a corpus of more than 100,000 happy moments crowd-sourced via Amazon’s Mechanical Turk.\\nEach worker is given the following task: What made you happy today? Reflect on the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681</th>\n",
              "      <td>681</td>\n",
              "      <td>10</td>\n",
              "      <td>56.89</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>Build the proposed optimized pricing model: Determine the largest or key value driver from the data Build price segments using product characteristics, distribution channel, behavior and demograph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>733</td>\n",
              "      <td>2</td>\n",
              "      <td>43.54</td>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "      <td>Context\\nThis dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>906</td>\n",
              "      <td>13</td>\n",
              "      <td>33.67</td>\n",
              "      <td>acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov</td>\n",
              "      <td>Context\\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>9</td>\n",
              "      <td>64.77</td>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "      <td>Context\\nAs a huge LOTR fan, I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here: https://github.com/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>1622</td>\n",
              "      <td>9</td>\n",
              "      <td>44.45</td>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "      <td>Context\\nI started to scrape TripAdvisor reviews for a personal project on Sentiment Analysis. I thought it could be good to share my data on Kaggle, since this can help other with similar ideas.\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Document  ...                                                                                                                                                                                                    Paper\n",
              "9            9  ...  This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use ...\n",
              "13          13  ...  A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...\n",
              "17          17  ...  Context\\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and us...\n",
              "392        392  ...  Context\\nBible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between G...\n",
              "503        503  ...  Description:\\nHappyDB is a corpus of more than 100,000 happy moments crowd-sourced via Amazon’s Mechanical Turk.\\nEach worker is given the following task: What made you happy today? Reflect on the...\n",
              "681        681  ...  Build the proposed optimized pricing model: Determine the largest or key value driver from the data Build price segments using product characteristics, distribution channel, behavior and demograph...\n",
              "733        733  ...  Context\\nThis dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelo...\n",
              "906        906  ...  Context\\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includ...\n",
              "996        996  ...  Context\\nAs a huge LOTR fan, I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here: https://github.com/...\n",
              "1622      1622  ...  Context\\nI started to scrape TripAdvisor reviews for a personal project on Sentiment Analysis. I thought it could be good to share my data on Kaggle, since this can help other with similar ideas.\\...\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXXJjDFng5Jq",
        "colab_type": "text"
      },
      "source": [
        "### Relevant Research Papers per Topic based on Dominance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb56VzB4gu3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c19ca03b-d610-436b-c2c9-2b4138eb28a7"
      },
      "source": [
        "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], \n",
        "                                                                                         ascending=False)\n",
        "                                                                             .iloc[0]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>722</td>\n",
              "      <td>1</td>\n",
              "      <td>99.35</td>\n",
              "      <td>university, college, map, stop, institute, chicago, technology, file, washington, national, archive, new, record, population, district, project, center, research, state, analysis</td>\n",
              "      <td>Context\\nInformation reproduced from the National Archives:\\n\"The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58,220 U.S. mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1930</td>\n",
              "      <td>2</td>\n",
              "      <td>98.50</td>\n",
              "      <td>wa, time, information, source, many, world, acknowledgement, inspiration, available, project, ha, database, one, date, also, use, country, report, number, people</td>\n",
              "      <td>82.558 Human Instructions in Chinese Extracted from wikiHow\\nStep-by-step instructions in Chinese extracted from wikiHow and decomposed into a formal graph representation in RDF.\\nThis is one of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>610</td>\n",
              "      <td>3</td>\n",
              "      <td>98.90</td>\n",
              "      <td>image, class, label, instance, cell, integer, sample, model, number, training, using, file, classifier, example, different, wa, two, machine_learning, used, test</td>\n",
              "      <td>This dataset contains 16,000 images of four shapes; square, star, circle, and triangle. Each image is 200x200 pixels.\\nThe data was collected using a Garmin Virb 1080p action camera. The shapes we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2081</td>\n",
              "      <td>4</td>\n",
              "      <td>97.81</td>\n",
              "      <td>word, review, language, corpus, name, file, http, license, text, code, english, copyright, version, com, list, use, org, metadata, unzip, frequency</td>\n",
              "      <td>Context\\nFastText word embeddings trained on English wikipedia\\nFastText embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.\\nContent\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1860</td>\n",
              "      <td>5</td>\n",
              "      <td>99.00</td>\n",
              "      <td>text, article, title, sentence, speech, file, post, line, wa, id, using, news, corpus, ha, used, inspiration, word, en, google, comment</td>\n",
              "      <td>Context:\\nYoutube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Robert Phoenix The 11th House upload...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>812</td>\n",
              "      <td>6</td>\n",
              "      <td>99.59</td>\n",
              "      <td>price, csv, de, stock, com, question, date, market, acknowledgement, http_www, day, en, inspiration, company, close, open, volume, la, index, per</td>\n",
              "      <td>«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1197</td>\n",
              "      <td>7</td>\n",
              "      <td>99.17</td>\n",
              "      <td>yet, doe_description, collection, text, txt, use, street, south, american, csv, library, number, city, name, digital, tool, http, license, open, new_york</td>\n",
              "      <td>Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1963</td>\n",
              "      <td>8</td>\n",
              "      <td>98.54</td>\n",
              "      <td>file, user, movie, csv, tag, rating, id, contains, research, use, paper, information, ha, one, format, value, data_set, used, following, system</td>\n",
              "      <td>Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2064</td>\n",
              "      <td>9</td>\n",
              "      <td>98.78</td>\n",
              "      <td>tweet, wa, com, twitter, one, time, like, many, inspiration, song, user, column, used, would, site, acknowledgement, people, could, find, nltk</td>\n",
              "      <td>Context\\nWalking around a Total Wine one day I wondered if there was any data I could find that would help me figure out what new rums to try, I later was able to find some information on rumratin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1805</td>\n",
              "      <td>10</td>\n",
              "      <td>97.90</td>\n",
              "      <td>attribute, area, variable, building, product, per, ha, type, day, model, value, number, feature, cluster, using, use, database, one, food, point</td>\n",
              "      <td>Context\\nThere are 4933 pharmacies in Belgium, and each pharmacy (in groups) are obliged to create a network of night-guard pharmacies covering complete Belgium. Compare it with a hospital that ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2134</td>\n",
              "      <td>11</td>\n",
              "      <td>98.10</td>\n",
              "      <td>car, inspiration, largest, past_research, science_community, acquired, front_world, question_want, see_answered, data_set, help_others, represents, acknowledgement_without, along_citation, owe_att...</td>\n",
              "      <td>Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2103</td>\n",
              "      <td>12</td>\n",
              "      <td>95.41</td>\n",
              "      <td>column, activity, sensor, feature, signal, right, using, left, driving, label, subject, participant, open, information, class, recognition, time, help, database, datasets</td>\n",
              "      <td>This directory contains the cross-position activity recognition datasets used in the following paper. Please consider citing this article if you want to use the datasets.\\nJindong Wang, Yiqiang Ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2131</td>\n",
              "      <td>13</td>\n",
              "      <td>99.07</td>\n",
              "      <td>acknowledgement, information, health, wa, state, ha, inspiration, government, company, federal, service, published, patient, database, year, department, vehicle, risk, doe, gov</td>\n",
              "      <td>310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>416</td>\n",
              "      <td>14</td>\n",
              "      <td>99.88</td>\n",
              "      <td>name, code, value, fire, county, state, air, row, csv, day, year, number, specie, event, unit, hour, time, mean, date, measured</td>\n",
              "      <td>Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>238</td>\n",
              "      <td>15</td>\n",
              "      <td>96.84</td>\n",
              "      <td>year, energy, total, animal, india, solar, rate, response, outcome, customer, natural, space, earth, range, wa, acknowledgement, type, death, based, inspiration</td>\n",
              "      <td>The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1881</td>\n",
              "      <td>16</td>\n",
              "      <td>99.62</td>\n",
              "      <td>player, team, game, match, season, goal, wa, com, table, point, result, play, information, league, pre_trained, sport, use, played, model, win</td>\n",
              "      <td>Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1907</td>\n",
              "      <td>17</td>\n",
              "      <td>98.22</td>\n",
              "      <td>city, time, information, number, location, date, new_york, property, acknowledgement, election, event, restaurant, name, includes, one, inspiration, station, wa, food, day</td>\n",
              "      <td>Context\\nThis dataset contains a subset of information, pertaining to the weather patterns during Januaray 2016 - June 2016 in NYC. This may be important to those who are looking to add columns to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>99.67</td>\n",
              "      <td>numeric, temperature, university_california, traffic, ic_uci, data_set, text, edu_ml, location, http_archive, wind, fatality, school, uci_machine, weather, learning_repository, incident, acknowled...</td>\n",
              "      <td>A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2067</td>\n",
              "      <td>19</td>\n",
              "      <td>99.26</td>\n",
              "      <td>year, country, survey, age, number, state, name, variable, population, month, source, information, education, student, gender, policy, acknowledgement, wa, data_set, rate</td>\n",
              "      <td>Context\\nThe United States Census Bureau conducts regular surveys to assess education levels in the U.S. These surveys sample participants' highest levels of education (i.e. high school diploma, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1794</td>\n",
              "      <td>20</td>\n",
              "      <td>99.60</td>\n",
              "      <td>crime, time, score, attack, number, defense, submission, non, csv, request, statistic, image, matrix, police, file, run, name, date, id, team</td>\n",
              "      <td>This dataset contains run time statistics and details about scores for the second development round of NIPS 2017 Adversarial learning competition\\nContent\\nMatrices with intermediate results\\nFoll...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Document  ...                                                                                                                                                                                                    Paper\n",
              "Dominant Topic            ...                                                                                                                                                                                                         \n",
              "1                    722  ...  Context\\nInformation reproduced from the National Archives:\\n\"The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58,220 U.S. mi...\n",
              "2                   1930  ...  82.558 Human Instructions in Chinese Extracted from wikiHow\\nStep-by-step instructions in Chinese extracted from wikiHow and decomposed into a formal graph representation in RDF.\\nThis is one of m...\n",
              "3                    610  ...  This dataset contains 16,000 images of four shapes; square, star, circle, and triangle. Each image is 200x200 pixels.\\nThe data was collected using a Garmin Virb 1080p action camera. The shapes we...\n",
              "4                   2081  ...  Context\\nFastText word embeddings trained on English wikipedia\\nFastText embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.\\nContent\\n...\n",
              "5                   1860  ...  Context:\\nYoutube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Robert Phoenix The 11th House upload...\n",
              "6                    812  ...  «Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...\n",
              "7                   1197  ...  Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...\n",
              "8                   1963  ...  Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...\n",
              "9                   2064  ...  Context\\nWalking around a Total Wine one day I wondered if there was any data I could find that would help me figure out what new rums to try, I later was able to find some information on rumratin...\n",
              "10                  1805  ...  Context\\nThere are 4933 pharmacies in Belgium, and each pharmacy (in groups) are obliged to create a network of night-guard pharmacies covering complete Belgium. Compare it with a hospital that ha...\n",
              "11                  2134  ...  Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...\n",
              "12                  2103  ...  This directory contains the cross-position activity recognition datasets used in the following paper. Please consider citing this article if you want to use the datasets.\\nJindong Wang, Yiqiang Ch...\n",
              "13                  2131  ...  310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...\n",
              "14                   416  ...  Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...\n",
              "15                   238  ...  The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...\n",
              "16                  1881  ...  Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...\n",
              "17                  1907  ...  Context\\nThis dataset contains a subset of information, pertaining to the weather patterns during Januaray 2016 - June 2016 in NYC. This may be important to those who are looking to add columns to...\n",
              "18                    13  ...  A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...\n",
              "19                  2067  ...  Context\\nThe United States Census Bureau conducts regular surveys to assess education levels in the U.S. These surveys sample participants' highest levels of education (i.e. high school diploma, b...\n",
              "20                  1794  ...  This dataset contains run time statistics and details about scores for the second development round of NIPS 2017 Adversarial learning competition\\nContent\\nMatrices with intermediate results\\nFoll...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3OYd7GlEIk",
        "colab_type": "text"
      },
      "source": [
        "#Ch06c-Topic model with sk learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqny43nigwki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "9b70197d-ba53-4fb3-97fd-6a2a353e8f83"
      },
      "source": [
        "data=pd.read_csv('https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_upvoted_dataset/voted-kaggle-dataset.csv?raw=true')\n",
        "print(data.shape)\n",
        "data.head(1)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2150, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Subtitle</th>\n",
              "      <th>Owner</th>\n",
              "      <th>Votes</th>\n",
              "      <th>Versions</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Data Type</th>\n",
              "      <th>Size</th>\n",
              "      <th>License</th>\n",
              "      <th>Views</th>\n",
              "      <th>Download</th>\n",
              "      <th>Kernels</th>\n",
              "      <th>Topics</th>\n",
              "      <th>URL</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Credit Card Fraud Detection</td>\n",
              "      <td>Anonymized credit card transactions labeled as fraudulent or genuine</td>\n",
              "      <td>Machine Learning Group - ULB</td>\n",
              "      <td>1241</td>\n",
              "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
              "      <td>crime\\nfinance</td>\n",
              "      <td>CSV</td>\n",
              "      <td>144 MB</td>\n",
              "      <td>ODbL</td>\n",
              "      <td>442,136 views</td>\n",
              "      <td>53,128 downloads</td>\n",
              "      <td>1,782 kernels</td>\n",
              "      <td>26 topics</td>\n",
              "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
              "      <td>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Title  ...                                                                                                                                                                                              Description\n",
              "0  Credit Card Fraud Detection  ...  The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284...\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMcVpjAVlrz5",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54T9gLb-lXYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ac0b347d-66eb-4739-d9a5-9038a10b7b1a"
      },
      "source": [
        "%%time\n",
        "papers=papers.astype(str)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(papers):\n",
        "    norm_papers = []\n",
        "    for paper in papers:\n",
        "        paper = paper.lower()\n",
        "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "        paper_tokens = list(filter(None, paper_tokens))\n",
        "        if paper_tokens:\n",
        "            norm_papers.append(paper_tokens)\n",
        "            \n",
        "    return norm_papers\n",
        "    \n",
        "norm_papers = normalize_corpus(papers)\n",
        "print(len(norm_papers))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "2150\n",
            "CPU times: user 2.85 s, sys: 19.4 ms, total: 2.87 s\n",
            "Wall time: 2.96 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIM0mr3ellUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "23f55276-a55f-496f-a999-4d71144b1e7f"
      },
      "source": [
        "print(norm_papers[0])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'v1', 'v2', 'v28', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'ha', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ZY5s7blt0T",
        "colab_type": "text"
      },
      "source": [
        "## Text Representation with Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16LMfEZ4lpuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "748655dc-d54b-4df9-c36c-9f3c141292b2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2),\n",
        "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
        "                     preprocessor=lambda doc: doc)\n",
        "cv_features = cv.fit_transform(norm_papers)\n",
        "cv_features.shape"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2150, 2097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU9NQzDilwT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf6e9a91-3cb7-4468-a983-cf5dcf910c2f"
      },
      "source": [
        "vocabulary = np.array(cv.get_feature_names())\n",
        "print('Total Vocabulary Size:', len(vocabulary))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Vocabulary Size: 2097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yPJx29QmN1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dde559b3-a356-4a3f-bddc-6efac1bcc8fa"
      },
      "source": [
        "vocabulary"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1st', '2nd', 'ab', ..., 'zip code', 'zip file', 'zone'],\n",
              "      dtype='<U26')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXS0V8IYmb1n",
        "colab_type": "text"
      },
      "source": [
        "# Topic Models with Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0f7apLamPgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "432f7ae8-51ea-4d33-cf52-0dde0f506456"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "TOTAL_TOPICS = 20\n",
        "\n",
        "lsi_model = TruncatedSVD(n_components=TOTAL_TOPICS, n_iter=500, random_state=42)\n",
        "document_topics = lsi_model.fit_transform(cv_features)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10 s, sys: 7.57 s, total: 17.6 s\n",
            "Wall time: 9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4-tZppBmeBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e11b340f-a2b9-4794-8603-0ed2921acb1d"
      },
      "source": [
        "document_topics.shape\n",
        "# 2150 document"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2150, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt7cQu2jmgK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74f1f370-ed0d-4844-8974-7ed6e7033859"
      },
      "source": [
        "topic_terms = lsi_model.components_\n",
        "topic_terms.shape\n",
        "# 2097 words"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 2097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3J3oHV6mgSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5a3d378c-530e-4d23-e406-a138334d4ce8"
      },
      "source": [
        "topic_terms"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00080267,  0.00052692,  0.00037655, ...,  0.00028539,\n",
              "         0.00027183,  0.00325737],\n",
              "       [ 0.00885621,  0.00568069,  0.00364484, ...,  0.00214013,\n",
              "         0.00257936,  0.0626348 ],\n",
              "       [-0.00477828, -0.00230495, -0.00306242, ..., -0.00162598,\n",
              "        -0.00236657,  0.1410079 ],\n",
              "       ...,\n",
              "       [-0.03070314, -0.02270857,  0.00195795, ...,  0.00182991,\n",
              "         0.00139959, -0.04528696],\n",
              "       [-0.01256348, -0.00855104, -0.00197959, ..., -0.00108223,\n",
              "         0.00107812,  0.01667599],\n",
              "       [-0.01868419, -0.01859952, -0.00289716, ..., -0.00260013,\n",
              "        -0.0046619 ,  0.0088497 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgbB8qzCmgd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0edd5895-1377-467a-ebb4-de26e51c0439"
      },
      "source": [
        "top_terms = 20\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterm_weights = np.array([topic_terms[row, columns] \n",
        "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    terms, weights = topic_keyterms_weights[n]\n",
        "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
        "                          key=lambda row: -abs(row[1]))\n",
        "    for term, wt in term_weights:\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('university', 0.969), ('state', 0.175), ('college', 0.076), ('california', 0.049), ('university california', 0.041), ('institute', 0.039), ('new', 0.032), ('wa', 0.029), ('technology', 0.028), ('north', 0.027), ('year', 0.02), ('number', 0.02), ('file', 0.02), ('set', 0.02), ('san', 0.019), ('time', 0.018), ('st', 0.018), ('international', 0.018), ('csv', 0.017), ('player', 0.017)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.305), ('wa', 0.276), ('number', 0.21), ('team', 0.195), ('file', 0.189), ('time', 0.186), ('year', 0.174), ('csv', 0.17), ('one', 0.129), ('ha', 0.126), ('name', 0.125), ('information', 0.119), ('date', 0.115), ('http', 0.114), ('goal', 0.111), ('contains', 0.106), ('per', 0.1), ('value', 0.098), ('taken', 0.094)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('university', -0.121)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.587), ('team', 0.345), ('goal', 0.217), ('attempt', 0.187), ('weighted', 0.152), ('taken', 0.148), ('wa', 0.144), ('zone', 0.141), ('allowed', 0.121), ('minute', 0.109), ('game', 0.107), ('average', 0.089), ('per', 0.089)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('file', -0.156), ('csv', -0.136), ('one', -0.094), ('year', -0.093), ('date', -0.09), ('information', -0.081), ('contains', -0.077)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('integer', 0.806), ('interested', 0.353), ('enjoy', 0.347), ('much', 0.164), ('movie', 0.08), ('categorical', 0.068), ('always', 0.065), ('people', 0.062), ('music', 0.06), ('preference', 0.05), ('interest', 0.048), ('lot', 0.044), ('item', 0.041), ('money', 0.032), ('point', 0.032)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.052), ('date', -0.051), ('number', -0.036), ('file', -0.034), ('code', -0.031)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('date', 0.319), ('element', 0.259), ('tag', 0.201), ('file', 0.191), ('registration', 0.187), ('zero', 0.181), ('end', 0.176), ('one', 0.175), ('start', 0.171), ('application', 0.162), ('time', 0.158), ('position', 0.145), ('containing', 0.138), ('section', 0.1), ('mark', 0.096), ('version', 0.093)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.444), ('total', -0.154), ('state', -0.098), ('child', -0.093)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('year', 0.406), ('number', 0.213), ('date', 0.206), ('element', 0.151), ('total', 0.145), ('registration', 0.112), ('zero', 0.111), ('child', 0.107), ('time', 0.104), ('end', 0.102), ('code', 0.099), ('start', 0.091), ('application', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.535), ('file', -0.146), ('image', -0.1), ('http', -0.097), ('text', -0.095), ('numeric', -0.093), ('user', -0.087)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.54), ('text', 0.364), ('word', 0.12), ('model', 0.092), ('trained', 0.08), ('wa', 0.072), ('language', 0.07), ('use', 0.068), ('image', 0.066), ('http', 0.061), ('using', 0.06), ('set', 0.054), ('reading', 0.05), ('real', 0.05)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('csv', -0.581), ('year', -0.167), ('file', -0.081), ('total', -0.059), ('station', -0.059), ('date', -0.051)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('numeric', 0.638), ('csv', 0.396), ('text', 0.309), ('year', 0.243), ('total', 0.08), ('reading', 0.052)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('model', -0.144), ('image', -0.133), ('trained', -0.124), ('http', -0.095), ('feature', -0.087), ('pre', -0.08), ('wa', -0.078), ('pre trained', -0.075), ('trained model', -0.072), ('set', -0.071), ('use', -0.062), ('word', -0.061), ('using', -0.059), ('ha', -0.056)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('model', 0.314), ('trained', 0.301), ('year', 0.244), ('image', 0.216), ('pre trained', 0.187), ('pre', 0.187), ('feature', 0.186), ('trained model', 0.184), ('csv', 0.176), ('network', 0.128), ('layer', 0.107), ('total', 0.104), ('time', 0.103), ('numeric', 0.084)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('name', -0.132), ('information', -0.111), ('user', -0.1), ('com', -0.08), ('fire', -0.077), ('de', -0.076)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('value', 0.345), ('station', 0.257), ('name', 0.244), ('feature', 0.15), ('air', 0.135), ('mean', 0.127), ('day', 0.12), ('monitor', 0.119), ('hour', 0.117), ('event', 0.112), ('parameter', 0.111), ('site', 0.1), ('code', 0.1)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.218), ('movie', -0.154), ('word', -0.152), ('http', -0.143), ('total', -0.138), ('com', -0.109), ('tag', -0.103)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #11:\n",
            "==================================================\n",
            "Direction 1: [('child', 0.518), ('number', 0.278), ('word', 0.23), ('age', 0.227), ('name', 0.173), ('language', 0.149), ('sold', 0.146), ('sale', 0.11), ('month', 0.105), ('woman', 0.098), ('csv', 0.087), ('price', 0.086)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('year', -0.213), ('de', -0.161), ('http', -0.142), ('com', -0.126), ('value', -0.103), ('en', -0.1), ('per', -0.097), ('www', -0.092)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #12:\n",
            "==================================================\n",
            "Direction 1: [('de', 0.426), ('en', 0.23), ('com', 0.23), ('number', 0.222), ('per', 0.21), ('http', 0.207), ('csv', 0.152), ('le', 0.145), ('la', 0.13), ('www', 0.119), ('http www', 0.117)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('file', -0.27), ('station', -0.173), ('year', -0.166), ('user', -0.163), ('set', -0.115), ('movie', -0.113), ('id', -0.11), ('wa', -0.102), ('value', -0.088)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #13:\n",
            "==================================================\n",
            "Direction 1: [('state', 0.226), ('fire', 0.188), ('department', 0.138), ('csv', 0.137), ('police', 0.133), ('national', 0.114), ('code', 0.109), ('wa', 0.109), ('time', 0.106)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('de', -0.299), ('file', -0.222), ('value', -0.213), ('number', -0.207), ('station', -0.176), ('per', -0.173), ('en', -0.158), ('movie', -0.135), ('com', -0.129), ('user', -0.121), ('word', -0.107)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #14:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.655), ('label', 0.17), ('class', 0.168), ('number', 0.135), ('sample', 0.08), ('point', 0.077), ('pixel', 0.069)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('movie', -0.238), ('user', -0.21), ('trained', -0.149), ('model', -0.135), ('tag', -0.111), ('rating', -0.104), ('feature', -0.099), ('pre trained', -0.096), ('ha', -0.095), ('trained model', -0.093), ('pre', -0.092), ('state', -0.071), ('station', -0.069)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #15:\n",
            "==================================================\n",
            "Direction 1: [('user', 0.27), ('number', 0.212), ('movie', 0.176), ('game', 0.166), ('player', 0.159), ('id', 0.119), ('rating', 0.105), ('numeric', 0.103), ('set', 0.102)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -0.516), ('language', -0.254), ('wa', -0.144), ('corpus', -0.141), ('csv', -0.121), ('per', -0.113), ('text', -0.112), ('vector', -0.099), ('goal', -0.089), ('taken', -0.088), ('de', -0.086)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #16:\n",
            "==================================================\n",
            "Direction 1: [('file', 0.312), ('image', 0.307), ('child', 0.226), ('name', 0.158), ('movie', 0.151), ('age', 0.135), ('state', 0.1), ('per', 0.097), ('tag', 0.094), ('attempt', 0.09), ('weighted', 0.088), ('health', 0.087)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('time', -0.275), ('word', -0.233), ('race', -0.196), ('number', -0.174), ('game', -0.144), ('match', -0.119), ('language', -0.102), ('csv', -0.1)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #17:\n",
            "==================================================\n",
            "Direction 1: [('player', 0.254), ('number', 0.243), ('code', 0.181), ('fire', 0.148), ('game', 0.14), ('name', 0.126), ('state', 0.115)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('race', -0.259), ('time', -0.225), ('police', -0.21), ('station', -0.193), ('section', -0.182), ('point', -0.152), ('team', -0.139), ('taken', -0.136), ('woman', -0.13), ('per', -0.12), ('age', -0.108), ('child', -0.106), ('total', -0.106)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #18:\n",
            "==================================================\n",
            "Direction 1: [('station', 0.395), ('file', 0.191), ('player', 0.187), ('feature', 0.177), ('weather', 0.141), ('null', 0.136), ('http', 0.134), ('game', 0.124), ('police', 0.101)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('user', -0.263), ('movie', -0.176), ('taken', -0.156), ('race', -0.154), ('time', -0.15), ('set', -0.148), ('per', -0.129), ('mean', -0.12), ('sample', -0.114), ('hour', -0.105), ('wa', -0.105)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #19:\n",
            "==================================================\n",
            "Direction 1: [('police', 0.296), ('user', 0.248), ('de', 0.23), ('number', 0.219), ('crime', 0.174), ('image', 0.168), ('total', 0.131), ('fire', 0.128), ('tweet', 0.123), ('station', 0.121), ('department', 0.118), ('location', 0.113), ('section', 0.108), ('id', 0.108)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('match', -0.189), ('http', -0.162), ('team', -0.162), ('name', -0.138), ('child', -0.132), ('price', -0.128)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #20:\n",
            "==================================================\n",
            "Direction 1: [('health', 0.272), ('drug', 0.215), ('plan', 0.209), ('survey', 0.206), ('information', 0.123), ('de', 0.121), ('per', 0.119), ('care', 0.113), ('part', 0.108)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('http', -0.197), ('fire', -0.174), ('movie', -0.157), ('point', -0.144), ('team', -0.138), ('image', -0.125), ('com', -0.123), ('race', -0.122), ('name', -0.116), ('child', -0.109), ('www', -0.107)]\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCL0obHUmgad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "fd12b875-1d5c-485a-ad09-8ad1fff3d22c"
      },
      "source": [
        "dt_df = pd.DataFrame(np.round(document_topics, 3), \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.T\n",
        "#2150 document"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2110</th>\n",
              "      <th>2111</th>\n",
              "      <th>2112</th>\n",
              "      <th>2113</th>\n",
              "      <th>2114</th>\n",
              "      <th>2115</th>\n",
              "      <th>2116</th>\n",
              "      <th>2117</th>\n",
              "      <th>2118</th>\n",
              "      <th>2119</th>\n",
              "      <th>2120</th>\n",
              "      <th>2121</th>\n",
              "      <th>2122</th>\n",
              "      <th>2123</th>\n",
              "      <th>2124</th>\n",
              "      <th>2125</th>\n",
              "      <th>2126</th>\n",
              "      <th>2127</th>\n",
              "      <th>2128</th>\n",
              "      <th>2129</th>\n",
              "      <th>2130</th>\n",
              "      <th>2131</th>\n",
              "      <th>2132</th>\n",
              "      <th>2133</th>\n",
              "      <th>2134</th>\n",
              "      <th>2135</th>\n",
              "      <th>2136</th>\n",
              "      <th>2137</th>\n",
              "      <th>2138</th>\n",
              "      <th>2139</th>\n",
              "      <th>2140</th>\n",
              "      <th>2141</th>\n",
              "      <th>2142</th>\n",
              "      <th>2143</th>\n",
              "      <th>2144</th>\n",
              "      <th>2145</th>\n",
              "      <th>2146</th>\n",
              "      <th>2147</th>\n",
              "      <th>2148</th>\n",
              "      <th>2149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1</th>\n",
              "      <td>0.439</td>\n",
              "      <td>1.150</td>\n",
              "      <td>1.030</td>\n",
              "      <td>5.407</td>\n",
              "      <td>0.529</td>\n",
              "      <td>1.605</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.400</td>\n",
              "      <td>1.012</td>\n",
              "      <td>0.928</td>\n",
              "      <td>1.346</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.616</td>\n",
              "      <td>0.399</td>\n",
              "      <td>0.279</td>\n",
              "      <td>1.509</td>\n",
              "      <td>0.987</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.466</td>\n",
              "      <td>0.423</td>\n",
              "      <td>1.048</td>\n",
              "      <td>1.742</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.381</td>\n",
              "      <td>1.913</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.638</td>\n",
              "      <td>11.345</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.264</td>\n",
              "      <td>2.582</td>\n",
              "      <td>0.249</td>\n",
              "      <td>1.898</td>\n",
              "      <td>...</td>\n",
              "      <td>1.148</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.345</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>3.561</td>\n",
              "      <td>11.376</td>\n",
              "      <td>7.611</td>\n",
              "      <td>11.192</td>\n",
              "      <td>4.449</td>\n",
              "      <td>10.668</td>\n",
              "      <td>1.292</td>\n",
              "      <td>1.104</td>\n",
              "      <td>4.714</td>\n",
              "      <td>4.545</td>\n",
              "      <td>1.973</td>\n",
              "      <td>5.762</td>\n",
              "      <td>5.910</td>\n",
              "      <td>8.704</td>\n",
              "      <td>2.449</td>\n",
              "      <td>3.017</td>\n",
              "      <td>3.270</td>\n",
              "      <td>2.161</td>\n",
              "      <td>8.132</td>\n",
              "      <td>7.626</td>\n",
              "      <td>1.769</td>\n",
              "      <td>3.959</td>\n",
              "      <td>3.672</td>\n",
              "      <td>11.006</td>\n",
              "      <td>12.280</td>\n",
              "      <td>0.496</td>\n",
              "      <td>8.143</td>\n",
              "      <td>3.122</td>\n",
              "      <td>17.566</td>\n",
              "      <td>5.982</td>\n",
              "      <td>7.000</td>\n",
              "      <td>5.311</td>\n",
              "      <td>2.433</td>\n",
              "      <td>3.395</td>\n",
              "      <td>1.386</td>\n",
              "      <td>1.674</td>\n",
              "      <td>2.162</td>\n",
              "      <td>4.401</td>\n",
              "      <td>2.170</td>\n",
              "      <td>5.936</td>\n",
              "      <td>...</td>\n",
              "      <td>2.439</td>\n",
              "      <td>0.816</td>\n",
              "      <td>1.022</td>\n",
              "      <td>1.204</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.930</td>\n",
              "      <td>2.082</td>\n",
              "      <td>3.908</td>\n",
              "      <td>4.460</td>\n",
              "      <td>2.820</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1.201</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1.524</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>1.625</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.077</td>\n",
              "      <td>4.460</td>\n",
              "      <td>1.625</td>\n",
              "      <td>0.939</td>\n",
              "      <td>1.449</td>\n",
              "      <td>1.825</td>\n",
              "      <td>0.323</td>\n",
              "      <td>2.731</td>\n",
              "      <td>2.829</td>\n",
              "      <td>1.221</td>\n",
              "      <td>1.950</td>\n",
              "      <td>2.301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>-2.190</td>\n",
              "      <td>5.920</td>\n",
              "      <td>-3.482</td>\n",
              "      <td>-5.392</td>\n",
              "      <td>-1.810</td>\n",
              "      <td>-4.841</td>\n",
              "      <td>-0.462</td>\n",
              "      <td>-0.678</td>\n",
              "      <td>-2.670</td>\n",
              "      <td>-1.829</td>\n",
              "      <td>-1.329</td>\n",
              "      <td>-2.146</td>\n",
              "      <td>-1.760</td>\n",
              "      <td>-8.425</td>\n",
              "      <td>-0.747</td>\n",
              "      <td>-0.887</td>\n",
              "      <td>-1.791</td>\n",
              "      <td>-1.486</td>\n",
              "      <td>-2.670</td>\n",
              "      <td>-3.975</td>\n",
              "      <td>-1.065</td>\n",
              "      <td>-1.029</td>\n",
              "      <td>-1.025</td>\n",
              "      <td>5.676</td>\n",
              "      <td>-4.683</td>\n",
              "      <td>-0.306</td>\n",
              "      <td>-5.266</td>\n",
              "      <td>-2.017</td>\n",
              "      <td>-6.781</td>\n",
              "      <td>-2.557</td>\n",
              "      <td>-4.512</td>\n",
              "      <td>-1.904</td>\n",
              "      <td>-0.969</td>\n",
              "      <td>-2.193</td>\n",
              "      <td>-0.813</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>-0.729</td>\n",
              "      <td>-2.218</td>\n",
              "      <td>-1.300</td>\n",
              "      <td>-3.851</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.676</td>\n",
              "      <td>-0.490</td>\n",
              "      <td>-0.643</td>\n",
              "      <td>-0.817</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.505</td>\n",
              "      <td>-1.589</td>\n",
              "      <td>-2.013</td>\n",
              "      <td>-1.890</td>\n",
              "      <td>-0.958</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.436</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.183</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.604</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.902</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.851</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-1.890</td>\n",
              "      <td>-0.880</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-0.740</td>\n",
              "      <td>-1.182</td>\n",
              "      <td>-0.234</td>\n",
              "      <td>-0.334</td>\n",
              "      <td>-1.312</td>\n",
              "      <td>-0.944</td>\n",
              "      <td>-0.465</td>\n",
              "      <td>-1.032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>-0.422</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.289</td>\n",
              "      <td>-1.468</td>\n",
              "      <td>-0.393</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>-0.542</td>\n",
              "      <td>-0.077</td>\n",
              "      <td>-0.352</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.724</td>\n",
              "      <td>-2.483</td>\n",
              "      <td>0.124</td>\n",
              "      <td>-0.335</td>\n",
              "      <td>-0.394</td>\n",
              "      <td>-0.361</td>\n",
              "      <td>0.380</td>\n",
              "      <td>1.019</td>\n",
              "      <td>-0.212</td>\n",
              "      <td>-0.431</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.806</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>-0.086</td>\n",
              "      <td>0.627</td>\n",
              "      <td>-0.407</td>\n",
              "      <td>-1.552</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>-0.090</td>\n",
              "      <td>-0.262</td>\n",
              "      <td>0.162</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>0.063</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>-0.117</td>\n",
              "      <td>-0.186</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.344</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>0.099</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.161</td>\n",
              "      <td>-0.096</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.130</td>\n",
              "      <td>-0.054</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.241</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>-0.174</td>\n",
              "      <td>-0.332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T5</th>\n",
              "      <td>-0.224</td>\n",
              "      <td>-0.217</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.908</td>\n",
              "      <td>0.146</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>-0.054</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>0.607</td>\n",
              "      <td>-1.114</td>\n",
              "      <td>0.960</td>\n",
              "      <td>-1.768</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>7.174</td>\n",
              "      <td>-2.690</td>\n",
              "      <td>-0.632</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.726</td>\n",
              "      <td>-3.546</td>\n",
              "      <td>-1.033</td>\n",
              "      <td>0.109</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>1.033</td>\n",
              "      <td>0.114</td>\n",
              "      <td>-5.975</td>\n",
              "      <td>-0.239</td>\n",
              "      <td>1.954</td>\n",
              "      <td>0.815</td>\n",
              "      <td>-2.189</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.186</td>\n",
              "      <td>-0.855</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>-1.846</td>\n",
              "      <td>0.195</td>\n",
              "      <td>-0.634</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-1.931</td>\n",
              "      <td>1.375</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.828</td>\n",
              "      <td>-0.644</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>0.173</td>\n",
              "      <td>-1.063</td>\n",
              "      <td>-0.959</td>\n",
              "      <td>-0.201</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.222</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>0.493</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-1.104</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.959</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>-0.253</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.132</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>-0.799</td>\n",
              "      <td>0.480</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T6</th>\n",
              "      <td>-0.853</td>\n",
              "      <td>-2.495</td>\n",
              "      <td>-1.340</td>\n",
              "      <td>-0.549</td>\n",
              "      <td>-2.863</td>\n",
              "      <td>-7.662</td>\n",
              "      <td>-0.389</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>-2.387</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>-0.269</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>-2.359</td>\n",
              "      <td>-16.858</td>\n",
              "      <td>1.752</td>\n",
              "      <td>0.096</td>\n",
              "      <td>-0.251</td>\n",
              "      <td>-1.317</td>\n",
              "      <td>0.540</td>\n",
              "      <td>-3.537</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>-2.767</td>\n",
              "      <td>0.974</td>\n",
              "      <td>-2.679</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-7.250</td>\n",
              "      <td>-1.684</td>\n",
              "      <td>6.712</td>\n",
              "      <td>-0.406</td>\n",
              "      <td>-7.606</td>\n",
              "      <td>-1.198</td>\n",
              "      <td>-0.866</td>\n",
              "      <td>-0.310</td>\n",
              "      <td>-0.399</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>-1.614</td>\n",
              "      <td>1.019</td>\n",
              "      <td>-2.381</td>\n",
              "      <td>...</td>\n",
              "      <td>0.728</td>\n",
              "      <td>0.146</td>\n",
              "      <td>-0.675</td>\n",
              "      <td>-0.852</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.344</td>\n",
              "      <td>-1.823</td>\n",
              "      <td>-2.471</td>\n",
              "      <td>-3.111</td>\n",
              "      <td>-0.057</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.655</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-3.111</td>\n",
              "      <td>-0.158</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.972</td>\n",
              "      <td>-1.186</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>-0.488</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.524</td>\n",
              "      <td>0.859</td>\n",
              "      <td>1.609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T7</th>\n",
              "      <td>1.287</td>\n",
              "      <td>1.772</td>\n",
              "      <td>1.604</td>\n",
              "      <td>3.006</td>\n",
              "      <td>-2.224</td>\n",
              "      <td>-5.621</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.262</td>\n",
              "      <td>-1.642</td>\n",
              "      <td>0.829</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>2.057</td>\n",
              "      <td>-2.406</td>\n",
              "      <td>76.337</td>\n",
              "      <td>-0.318</td>\n",
              "      <td>1.063</td>\n",
              "      <td>1.053</td>\n",
              "      <td>0.241</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.279</td>\n",
              "      <td>0.266</td>\n",
              "      <td>-0.658</td>\n",
              "      <td>0.505</td>\n",
              "      <td>1.254</td>\n",
              "      <td>1.660</td>\n",
              "      <td>0.173</td>\n",
              "      <td>-3.794</td>\n",
              "      <td>-2.289</td>\n",
              "      <td>-4.409</td>\n",
              "      <td>0.091</td>\n",
              "      <td>-7.400</td>\n",
              "      <td>2.531</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.694</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.446</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.039</td>\n",
              "      <td>9.500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.520</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.025</td>\n",
              "      <td>4.376</td>\n",
              "      <td>4.571</td>\n",
              "      <td>5.127</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.149</td>\n",
              "      <td>0.008</td>\n",
              "      <td>1.341</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.008</td>\n",
              "      <td>5.127</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.936</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.454</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.186</td>\n",
              "      <td>-0.330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T8</th>\n",
              "      <td>-2.009</td>\n",
              "      <td>-2.316</td>\n",
              "      <td>-2.412</td>\n",
              "      <td>-3.280</td>\n",
              "      <td>1.151</td>\n",
              "      <td>2.929</td>\n",
              "      <td>-0.605</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.741</td>\n",
              "      <td>-1.258</td>\n",
              "      <td>-0.161</td>\n",
              "      <td>-1.776</td>\n",
              "      <td>1.154</td>\n",
              "      <td>84.006</td>\n",
              "      <td>0.920</td>\n",
              "      <td>-1.174</td>\n",
              "      <td>-1.564</td>\n",
              "      <td>0.466</td>\n",
              "      <td>-1.334</td>\n",
              "      <td>-2.811</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.606</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>-2.137</td>\n",
              "      <td>-0.241</td>\n",
              "      <td>0.840</td>\n",
              "      <td>1.290</td>\n",
              "      <td>2.623</td>\n",
              "      <td>-0.731</td>\n",
              "      <td>4.561</td>\n",
              "      <td>-2.191</td>\n",
              "      <td>-1.114</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>0.236</td>\n",
              "      <td>-0.542</td>\n",
              "      <td>-0.220</td>\n",
              "      <td>-1.915</td>\n",
              "      <td>-0.037</td>\n",
              "      <td>11.925</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.341</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>-0.185</td>\n",
              "      <td>-0.777</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>1.936</td>\n",
              "      <td>-7.141</td>\n",
              "      <td>-7.947</td>\n",
              "      <td>-0.825</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.140</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.068</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.114</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-7.947</td>\n",
              "      <td>-0.273</td>\n",
              "      <td>-0.368</td>\n",
              "      <td>-0.818</td>\n",
              "      <td>-1.628</td>\n",
              "      <td>-0.075</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.823</td>\n",
              "      <td>-0.389</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T9</th>\n",
              "      <td>1.417</td>\n",
              "      <td>-1.116</td>\n",
              "      <td>-1.347</td>\n",
              "      <td>-5.409</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>-0.312</td>\n",
              "      <td>0.940</td>\n",
              "      <td>-0.559</td>\n",
              "      <td>-0.247</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.489</td>\n",
              "      <td>7.477</td>\n",
              "      <td>0.693</td>\n",
              "      <td>1.107</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.957</td>\n",
              "      <td>-1.406</td>\n",
              "      <td>1.611</td>\n",
              "      <td>-0.613</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>-0.338</td>\n",
              "      <td>0.860</td>\n",
              "      <td>-5.232</td>\n",
              "      <td>0.112</td>\n",
              "      <td>-1.259</td>\n",
              "      <td>0.869</td>\n",
              "      <td>2.224</td>\n",
              "      <td>-1.622</td>\n",
              "      <td>1.892</td>\n",
              "      <td>0.323</td>\n",
              "      <td>-0.590</td>\n",
              "      <td>-1.471</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-0.499</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>0.203</td>\n",
              "      <td>-0.398</td>\n",
              "      <td>1.109</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>-0.333</td>\n",
              "      <td>0.718</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.334</td>\n",
              "      <td>-0.356</td>\n",
              "      <td>14.440</td>\n",
              "      <td>15.671</td>\n",
              "      <td>-1.782</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.161</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.120</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.451</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>15.671</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>-0.109</td>\n",
              "      <td>0.290</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-0.085</td>\n",
              "      <td>-0.365</td>\n",
              "      <td>-1.428</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T10</th>\n",
              "      <td>0.952</td>\n",
              "      <td>-1.901</td>\n",
              "      <td>-1.649</td>\n",
              "      <td>-2.335</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-0.431</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.683</td>\n",
              "      <td>-0.087</td>\n",
              "      <td>1.213</td>\n",
              "      <td>1.587</td>\n",
              "      <td>6.266</td>\n",
              "      <td>-0.662</td>\n",
              "      <td>0.391</td>\n",
              "      <td>2.788</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-0.582</td>\n",
              "      <td>-0.081</td>\n",
              "      <td>-1.573</td>\n",
              "      <td>0.597</td>\n",
              "      <td>-0.349</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.164</td>\n",
              "      <td>-5.235</td>\n",
              "      <td>0.154</td>\n",
              "      <td>5.917</td>\n",
              "      <td>-0.232</td>\n",
              "      <td>-0.835</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>-0.564</td>\n",
              "      <td>-0.080</td>\n",
              "      <td>-0.073</td>\n",
              "      <td>-0.112</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.626</td>\n",
              "      <td>0.343</td>\n",
              "      <td>2.485</td>\n",
              "      <td>...</td>\n",
              "      <td>0.484</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.368</td>\n",
              "      <td>-0.232</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>-1.402</td>\n",
              "      <td>3.245</td>\n",
              "      <td>3.073</td>\n",
              "      <td>-0.696</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-0.507</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.064</td>\n",
              "      <td>3.073</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-0.171</td>\n",
              "      <td>-0.249</td>\n",
              "      <td>-0.163</td>\n",
              "      <td>-0.314</td>\n",
              "      <td>1.036</td>\n",
              "      <td>-0.293</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-0.348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T11</th>\n",
              "      <td>-0.392</td>\n",
              "      <td>-1.278</td>\n",
              "      <td>-0.776</td>\n",
              "      <td>-2.673</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.862</td>\n",
              "      <td>0.153</td>\n",
              "      <td>-0.127</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.072</td>\n",
              "      <td>1.070</td>\n",
              "      <td>-1.175</td>\n",
              "      <td>-2.936</td>\n",
              "      <td>-0.967</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>-0.360</td>\n",
              "      <td>-0.079</td>\n",
              "      <td>-1.437</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.220</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.119</td>\n",
              "      <td>-1.554</td>\n",
              "      <td>0.363</td>\n",
              "      <td>-1.373</td>\n",
              "      <td>0.467</td>\n",
              "      <td>5.140</td>\n",
              "      <td>2.375</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.168</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.227</td>\n",
              "      <td>-0.061</td>\n",
              "      <td>1.225</td>\n",
              "      <td>0.287</td>\n",
              "      <td>1.115</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-0.037</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.233</td>\n",
              "      <td>1.144</td>\n",
              "      <td>-0.123</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-1.993</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.448</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.048</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.151</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.487</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.537</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.305</td>\n",
              "      <td>0.943</td>\n",
              "      <td>0.670</td>\n",
              "      <td>-0.429</td>\n",
              "      <td>0.688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T12</th>\n",
              "      <td>0.524</td>\n",
              "      <td>1.756</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-1.287</td>\n",
              "      <td>-0.438</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>-1.194</td>\n",
              "      <td>0.556</td>\n",
              "      <td>-0.860</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>4.509</td>\n",
              "      <td>-0.357</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>-0.209</td>\n",
              "      <td>-0.941</td>\n",
              "      <td>0.066</td>\n",
              "      <td>-1.567</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>2.016</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>1.165</td>\n",
              "      <td>4.701</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-4.982</td>\n",
              "      <td>0.574</td>\n",
              "      <td>4.880</td>\n",
              "      <td>1.271</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>-0.554</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.364</td>\n",
              "      <td>1.022</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>-1.559</td>\n",
              "      <td>0.748</td>\n",
              "      <td>3.258</td>\n",
              "      <td>...</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.474</td>\n",
              "      <td>-0.547</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.173</td>\n",
              "      <td>-1.255</td>\n",
              "      <td>3.675</td>\n",
              "      <td>3.315</td>\n",
              "      <td>4.197</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.028</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.848</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.113</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>-0.445</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.017</td>\n",
              "      <td>3.315</td>\n",
              "      <td>-0.082</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>-0.234</td>\n",
              "      <td>-0.771</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>0.701</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-1.264</td>\n",
              "      <td>-0.878</td>\n",
              "      <td>0.185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T13</th>\n",
              "      <td>-0.735</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.076</td>\n",
              "      <td>5.924</td>\n",
              "      <td>1.529</td>\n",
              "      <td>3.140</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.537</td>\n",
              "      <td>-0.494</td>\n",
              "      <td>-0.160</td>\n",
              "      <td>-0.580</td>\n",
              "      <td>0.753</td>\n",
              "      <td>2.453</td>\n",
              "      <td>1.021</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.898</td>\n",
              "      <td>-1.458</td>\n",
              "      <td>-0.353</td>\n",
              "      <td>3.553</td>\n",
              "      <td>-1.504</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.171</td>\n",
              "      <td>0.091</td>\n",
              "      <td>-2.841</td>\n",
              "      <td>0.328</td>\n",
              "      <td>-7.504</td>\n",
              "      <td>-1.600</td>\n",
              "      <td>1.397</td>\n",
              "      <td>1.317</td>\n",
              "      <td>1.595</td>\n",
              "      <td>4.664</td>\n",
              "      <td>0.292</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>0.555</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>0.089</td>\n",
              "      <td>2.305</td>\n",
              "      <td>...</td>\n",
              "      <td>1.930</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.168</td>\n",
              "      <td>-0.058</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.133</td>\n",
              "      <td>-0.887</td>\n",
              "      <td>0.933</td>\n",
              "      <td>1.699</td>\n",
              "      <td>-3.060</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.626</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.022</td>\n",
              "      <td>1.699</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-0.244</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>-0.295</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>-0.489</td>\n",
              "      <td>-1.339</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>-0.336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T14</th>\n",
              "      <td>0.300</td>\n",
              "      <td>2.056</td>\n",
              "      <td>-1.496</td>\n",
              "      <td>-2.846</td>\n",
              "      <td>-0.185</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.621</td>\n",
              "      <td>1.636</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-0.426</td>\n",
              "      <td>-2.121</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.353</td>\n",
              "      <td>3.084</td>\n",
              "      <td>-1.213</td>\n",
              "      <td>0.354</td>\n",
              "      <td>8.538</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.272</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-6.633</td>\n",
              "      <td>0.084</td>\n",
              "      <td>2.933</td>\n",
              "      <td>0.086</td>\n",
              "      <td>-0.703</td>\n",
              "      <td>0.258</td>\n",
              "      <td>-0.044</td>\n",
              "      <td>-1.231</td>\n",
              "      <td>0.122</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.070</td>\n",
              "      <td>1.536</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.812</td>\n",
              "      <td>0.231</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>1.163</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.307</td>\n",
              "      <td>-0.819</td>\n",
              "      <td>-6.072</td>\n",
              "      <td>-4.852</td>\n",
              "      <td>-0.765</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.064</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.503</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.139</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>0.305</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-4.852</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.021</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>2.154</td>\n",
              "      <td>-0.052</td>\n",
              "      <td>0.224</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>-0.045</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T15</th>\n",
              "      <td>0.576</td>\n",
              "      <td>4.261</td>\n",
              "      <td>1.795</td>\n",
              "      <td>2.794</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.102</td>\n",
              "      <td>2.342</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.153</td>\n",
              "      <td>-1.389</td>\n",
              "      <td>4.350</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-0.051</td>\n",
              "      <td>0.698</td>\n",
              "      <td>1.263</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.332</td>\n",
              "      <td>-0.537</td>\n",
              "      <td>2.270</td>\n",
              "      <td>3.017</td>\n",
              "      <td>0.139</td>\n",
              "      <td>3.792</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>2.865</td>\n",
              "      <td>0.738</td>\n",
              "      <td>-1.763</td>\n",
              "      <td>-0.117</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.364</td>\n",
              "      <td>-0.029</td>\n",
              "      <td>0.053</td>\n",
              "      <td>-0.454</td>\n",
              "      <td>-1.879</td>\n",
              "      <td>0.107</td>\n",
              "      <td>3.954</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.780</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.140</td>\n",
              "      <td>-3.321</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.625</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.072</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.182</td>\n",
              "      <td>-0.009</td>\n",
              "      <td>0.893</td>\n",
              "      <td>0.196</td>\n",
              "      <td>-2.388</td>\n",
              "      <td>0.307</td>\n",
              "      <td>1.191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T16</th>\n",
              "      <td>-0.322</td>\n",
              "      <td>-4.400</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.857</td>\n",
              "      <td>-1.370</td>\n",
              "      <td>-0.713</td>\n",
              "      <td>-0.192</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.939</td>\n",
              "      <td>-1.061</td>\n",
              "      <td>1.355</td>\n",
              "      <td>-0.927</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>3.987</td>\n",
              "      <td>-0.376</td>\n",
              "      <td>-0.529</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.126</td>\n",
              "      <td>-2.667</td>\n",
              "      <td>2.834</td>\n",
              "      <td>0.278</td>\n",
              "      <td>-1.087</td>\n",
              "      <td>-0.575</td>\n",
              "      <td>-3.304</td>\n",
              "      <td>1.002</td>\n",
              "      <td>0.164</td>\n",
              "      <td>4.279</td>\n",
              "      <td>-0.535</td>\n",
              "      <td>-3.866</td>\n",
              "      <td>-1.789</td>\n",
              "      <td>2.146</td>\n",
              "      <td>-0.979</td>\n",
              "      <td>-1.316</td>\n",
              "      <td>1.467</td>\n",
              "      <td>-0.304</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.100</td>\n",
              "      <td>-1.206</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>...</td>\n",
              "      <td>1.276</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.916</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.290</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>0.766</td>\n",
              "      <td>1.576</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.142</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.272</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-1.265</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.016</td>\n",
              "      <td>1.576</td>\n",
              "      <td>0.118</td>\n",
              "      <td>-0.240</td>\n",
              "      <td>0.064</td>\n",
              "      <td>1.594</td>\n",
              "      <td>0.340</td>\n",
              "      <td>-0.982</td>\n",
              "      <td>1.383</td>\n",
              "      <td>-0.350</td>\n",
              "      <td>-0.856</td>\n",
              "      <td>-0.436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T17</th>\n",
              "      <td>-1.523</td>\n",
              "      <td>0.545</td>\n",
              "      <td>-0.639</td>\n",
              "      <td>0.343</td>\n",
              "      <td>-1.400</td>\n",
              "      <td>-2.430</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>-0.176</td>\n",
              "      <td>-0.586</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.248</td>\n",
              "      <td>1.206</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.861</td>\n",
              "      <td>0.533</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>0.388</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.184</td>\n",
              "      <td>-0.367</td>\n",
              "      <td>-0.111</td>\n",
              "      <td>0.670</td>\n",
              "      <td>-1.032</td>\n",
              "      <td>2.657</td>\n",
              "      <td>-3.749</td>\n",
              "      <td>-0.071</td>\n",
              "      <td>1.256</td>\n",
              "      <td>-0.665</td>\n",
              "      <td>-5.139</td>\n",
              "      <td>1.949</td>\n",
              "      <td>0.636</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>-1.082</td>\n",
              "      <td>0.692</td>\n",
              "      <td>-0.437</td>\n",
              "      <td>-0.674</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-0.737</td>\n",
              "      <td>0.480</td>\n",
              "      <td>-2.303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.231</td>\n",
              "      <td>0.911</td>\n",
              "      <td>1.353</td>\n",
              "      <td>1.789</td>\n",
              "      <td>-1.583</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.222</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>-0.949</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.038</td>\n",
              "      <td>1.789</td>\n",
              "      <td>-0.158</td>\n",
              "      <td>-0.362</td>\n",
              "      <td>-0.344</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>-0.055</td>\n",
              "      <td>0.641</td>\n",
              "      <td>0.368</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T18</th>\n",
              "      <td>1.402</td>\n",
              "      <td>4.280</td>\n",
              "      <td>-1.806</td>\n",
              "      <td>-2.204</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>-1.791</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>0.339</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>1.135</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>-0.913</td>\n",
              "      <td>-0.334</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-0.092</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>-1.889</td>\n",
              "      <td>-1.834</td>\n",
              "      <td>0.504</td>\n",
              "      <td>1.331</td>\n",
              "      <td>-0.072</td>\n",
              "      <td>1.496</td>\n",
              "      <td>1.670</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-3.837</td>\n",
              "      <td>-0.560</td>\n",
              "      <td>-2.452</td>\n",
              "      <td>-0.088</td>\n",
              "      <td>1.217</td>\n",
              "      <td>-0.742</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>-0.418</td>\n",
              "      <td>-0.123</td>\n",
              "      <td>0.239</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-1.915</td>\n",
              "      <td>0.279</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.467</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>0.490</td>\n",
              "      <td>-0.052</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-1.980</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.142</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.623</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.384</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.683</td>\n",
              "      <td>-0.386</td>\n",
              "      <td>0.418</td>\n",
              "      <td>0.634</td>\n",
              "      <td>0.288</td>\n",
              "      <td>0.616</td>\n",
              "      <td>0.289</td>\n",
              "      <td>-0.387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T19</th>\n",
              "      <td>-0.638</td>\n",
              "      <td>-5.427</td>\n",
              "      <td>-0.719</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.650</td>\n",
              "      <td>-1.104</td>\n",
              "      <td>-0.195</td>\n",
              "      <td>-0.807</td>\n",
              "      <td>-0.680</td>\n",
              "      <td>-0.422</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>-0.355</td>\n",
              "      <td>1.386</td>\n",
              "      <td>0.316</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-3.481</td>\n",
              "      <td>0.988</td>\n",
              "      <td>-0.314</td>\n",
              "      <td>-1.160</td>\n",
              "      <td>-1.242</td>\n",
              "      <td>-2.326</td>\n",
              "      <td>-8.990</td>\n",
              "      <td>-0.391</td>\n",
              "      <td>0.967</td>\n",
              "      <td>-1.288</td>\n",
              "      <td>0.931</td>\n",
              "      <td>0.916</td>\n",
              "      <td>-0.496</td>\n",
              "      <td>0.299</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>-1.626</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.484</td>\n",
              "      <td>0.475</td>\n",
              "      <td>-1.616</td>\n",
              "      <td>-1.058</td>\n",
              "      <td>-1.589</td>\n",
              "      <td>...</td>\n",
              "      <td>0.947</td>\n",
              "      <td>-0.151</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>0.146</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.066</td>\n",
              "      <td>0.786</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>3.189</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.088</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.135</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.470</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>-0.572</td>\n",
              "      <td>-0.111</td>\n",
              "      <td>-0.519</td>\n",
              "      <td>0.632</td>\n",
              "      <td>-0.170</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>-1.696</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T20</th>\n",
              "      <td>0.888</td>\n",
              "      <td>-1.644</td>\n",
              "      <td>0.085</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>3.320</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.392</td>\n",
              "      <td>0.103</td>\n",
              "      <td>-0.166</td>\n",
              "      <td>0.126</td>\n",
              "      <td>-3.480</td>\n",
              "      <td>1.518</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.523</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>4.024</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>-0.262</td>\n",
              "      <td>-0.984</td>\n",
              "      <td>0.791</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>-3.338</td>\n",
              "      <td>-0.160</td>\n",
              "      <td>-1.934</td>\n",
              "      <td>-0.141</td>\n",
              "      <td>3.136</td>\n",
              "      <td>1.128</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.400</td>\n",
              "      <td>2.003</td>\n",
              "      <td>8.028</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.604</td>\n",
              "      <td>0.313</td>\n",
              "      <td>6.076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.242</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.350</td>\n",
              "      <td>-0.660</td>\n",
              "      <td>-0.215</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>1.476</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>-0.157</td>\n",
              "      <td>0.072</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.072</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.611</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>-0.358</td>\n",
              "      <td>-0.225</td>\n",
              "      <td>-0.035</td>\n",
              "      <td>-0.722</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>0.495</td>\n",
              "      <td>1.023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 2150 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0       1      2       3      4     ...   2145   2146   2147   2148   2149\n",
              "T1   0.439   1.150  1.030   5.407  0.529  ...  0.300  0.345  0.137  0.231  0.313\n",
              "T2   3.561  11.376  7.611  11.192  4.449  ...  2.731  2.829  1.221  1.950  2.301\n",
              "T3  -2.190   5.920 -3.482  -5.392 -1.810  ... -0.334 -1.312 -0.944 -0.465 -1.032\n",
              "T4  -0.422   0.607  0.289  -1.468 -0.393  ... -0.320 -0.241 -0.178 -0.174 -0.332\n",
              "T5  -0.224  -0.217  0.305   0.908  0.146  ... -0.100 -0.799  0.480 -0.933 -1.552\n",
              "T6  -0.853  -2.495 -1.340  -0.549 -2.863  ... -0.488 -0.482 -0.524  0.859  1.609\n",
              "T7   1.287   1.772  1.604   3.006 -2.224  ...  0.405  0.454  0.418 -0.186 -0.330\n",
              "T8  -2.009  -2.316 -2.412  -3.280  1.151  ... -0.757 -0.823 -0.389  0.250  0.357\n",
              "T9   1.417  -1.116 -1.347  -5.409  0.423  ... -0.365 -1.428 -0.299  0.310  0.065\n",
              "T10  0.952  -1.901 -1.649  -2.335  0.378  ... -0.314  1.036 -0.293  0.392 -0.348\n",
              "T11 -0.392  -1.278 -0.776  -2.673  0.592  ... -0.305  0.943  0.670 -0.429  0.688\n",
              "T12  0.524   1.756 -2.287  -0.008  0.467  ...  0.701  0.467 -1.264 -0.878  0.185\n",
              "T13 -0.735  -0.162  0.076   5.924  1.529  ... -0.479 -0.489 -1.339 -0.374 -0.336\n",
              "T14  0.300   2.056 -1.496  -2.846 -0.185  ...  0.224 -0.268 -0.045  0.093  0.238\n",
              "T15  0.576   4.261  1.795   2.794 -0.280  ...  0.893  0.196 -2.388  0.307  1.191\n",
              "T16 -0.322  -4.400 -0.283  -0.857 -1.370  ... -0.982  1.383 -0.350 -0.856 -0.436\n",
              "T17 -1.523   0.545 -0.639   0.343 -1.400  ... -0.055  0.641  0.368 -0.464  0.581\n",
              "T18  1.402   4.280 -1.806  -2.204 -0.955  ...  0.634  0.288  0.616  0.289 -0.387\n",
              "T19 -0.638  -5.427 -0.719   1.938 -0.650  ... -0.134 -1.696  0.295  0.403  0.623\n",
              "T20  0.888  -1.644  0.085  -0.307 -0.074  ... -0.035 -0.722 -0.191  0.495  1.023\n",
              "\n",
              "[20 rows x 2150 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlsNkErbmgYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "d5e61d6b-228b-4037-8818-66437fb8c037"
      },
      "source": [
        "document_numbers = [1, 4, 10]\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(dt_df.columns[np.argsort(-np.absolute(dt_df.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print('Topic model '+top_topics[0][1:]+':',lsi_bow.show_topic(int(top_topics[0][1:]), topn=20))\n",
        "    print('Topic model '+top_topics[1][1:]+':',lsi_bow.show_topic(int(top_topics[1][1:]), topn=20))\n",
        "    print()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document #1:\n",
            "Dominant Topics (top 3): ['T2', 'T3', 'T19']\n",
            "Paper Summary:\n",
            "The ultimate Soccer database for data analysis and machine learning\n",
            "What you get:\n",
            "+25,000 matches\n",
            "+10,000 players\n",
            "11 European Countries with their lead championship\n",
            "Seasons 2008 to 2016\n",
            "Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates\n",
            "Team line up with squad formation (X, Y coordinates)\n",
            "Betting odds from up to 10 providers\n",
            "Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
            "*16th Oct 201\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "Topic model 3: [('year', 0.6066476425324451), ('date', -0.2521122438137626), ('file', -0.2301105179088531), ('total', 0.18790975575918087), ('csv', -0.15496077804136865), ('one', -0.15379966195785308), ('registration', -0.14983257418264562), ('zero', -0.14495064736824229), ('start', -0.1417260644472022), ('application', -0.13396177541110163), ('time', -0.13204114910014622), ('given', 0.12287379909988017), ('position', -0.11903978091156583), ('numeric', -0.11866454317357562), ('containing', -0.11546119074012208), ('energy', 0.10716218091695022), ('text', -0.10357211852273053), ('state', 0.0950605060488379), ('element', -0.09412418384076568), ('child', 0.08843306485651935)]\n",
            "\n",
            "Document #4:\n",
            "Dominant Topics (top 3): ['T2', 'T6', 'T7']\n",
            "Paper Summary:\n",
            "Context\n",
            "Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus \"chained\" together, \n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "Topic model 6: [('numeric', 0.7737103058328608), ('text', 0.4993476682927095), ('csv', -0.27244942185952215), ('integer', -0.11130023051204573), ('word', 0.06894308219253854), ('date', -0.06198646452424762), ('file', -0.05364781371846869), ('open', 0.051067712551731204), ('time', -0.04455174645642765), ('food', 0.042626416640394786), ('movie', -0.042405335415046826), ('language', 0.03900505452131897), ('product', 0.03899985454687244), ('student', 0.036781790937351205), ('number', -0.03647304030058715), ('one', -0.035450279410746126), ('registration', -0.035042266957650656), ('database', 0.03453239442243826), ('zero', -0.0341555466525183), ('use', 0.03384596864927049)]\n",
            "\n",
            "Document #10:\n",
            "Dominant Topics (top 3): ['T2', 'T16', 'T3']\n",
            "Paper Summary:\n",
            "These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of a\n",
            "Topic model 2: [('player', -0.677578910958172), ('team', -0.3940464304148368), ('goal', -0.23656264928185738), ('zone', -0.15124711306019448), ('file', 0.1352717752315316), ('game', -0.13359756647119775), ('allowed', -0.12940669656171266), ('csv', 0.11046645363578558), ('year', 0.10156555786775204), ('one', 0.09087393933412283), ('date', 0.08701565699597512), ('information', 0.0804350089995299), ('percentage', -0.07627993509599855), ('name', 0.07459330788052514), ('value', 0.07169754845021309), ('individual', -0.06766828687151855), ('taken', -0.06506021662159814), ('scored', -0.06459885393138377), ('relative', -0.05942473486472541), ('time', 0.05763992924033394)]\n",
            "Topic model 16: [('race', -0.22872344366458364), ('fire', -0.2231388391823038), ('user', -0.2222971221159055), ('station', 0.1980886197468324), ('word', -0.19015901320077047), ('point', -0.17797854363230017), ('image', -0.17617319805694523), ('name', -0.15545399504070395), ('child', -0.1534954586092869), ('feature', 0.15259011089725077), ('game', 0.1520167765260759), ('taken', -0.13944439777972517), ('time', -0.12973383610412498), ('team', -0.125417406509159), ('unit', -0.1142553246125783), ('match', 0.10845715681357088), ('language', -0.10729404559005745), ('file', -0.10607955244760199), ('state', -0.1056088880063532), ('player', 0.10239704493571669)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jLQoGXWozo_",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nACQzCYmgXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "892eb9d4-ed14-43d9-829d-85070e995a64"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50,\n",
        "                                      learning_method='online', batch_size=1740, learning_offset=50., \n",
        "                                      random_state=42, n_jobs=16)\n",
        "document_topics = lda_model.fit_transform(cv_features)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 8.83 s, total: 1min 10s\n",
            "Wall time: 7min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsZwaPMImgQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "947eda87-715e-4ed5-8580-13e6cf0293a0"
      },
      "source": [
        "topic_terms = lda_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms] #20\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>image, file, one, time, wa, contains, acknowledgement, inspiration, day, like, using, would, feature, used, set, see, available, column, label, different</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>image, wa, time, text, survey, csv, acknowledgement, using, file, statistic, open, ha, numeric, set, value, price, http, learning, database, paper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>player, csv, team, game, file, wa, http, user, movie, time, match, com, number, ha, www, information, http www, set, id, rating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>context data, love, claim, basic, stats, inspiration, filed, al, east, acknowledgement, modification, john, lost, player, sometimes, file, contains, number, ha, year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>wa, state, year, information, acknowledgement, time, number, country, ha, available, inspiration, national, source, public, date, record, report, survey, includes, world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>text, word, language, file, corpus, id, http, use, wa, title, contains, article, license, question, english, page, name, org, author, collection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>set, data set, value, column, activity, wa, mean, energy, model, sample, feature, using, use, used, research, sensor, attribute, system, temperature, specie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>com, http, tweet, github, github com, twitter, http github, vector, http www, www, word, nltk, model, kaggle, acknowledgement, found, zip, code, tree, used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>year, age, type, month, animal, rate, sex, male, female, outcome, bank, birth, name, credit, number, payment, set, amount, usd, data set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>model, trained, de, pre, pre trained, trained model, feature, network, ha, depth, layer, deep, architecture, time, image, learned, large, en, model pre, learned feature</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>csv, name, code, map, city, open, latitude, longitude, number, street, district, india, open data, country, coordinate, crime, http, contains, license, acknowledgement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>variable, integer, customer, attribute, categorical, datasets, park, original, file, bird, credit, german, http, continuous, one, good, two, form, version, ml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>back, lower, air, sound, may, low, flight, mile, cause, distance, body, problem, go, normal, taking, smaller, month, large, might, probability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>others, data science, community, world, science, inspiration, inspiration data, acknowledgement, question, thanks, answered, research, want, want see, largest, world largest, science community, past research, see, past</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>space, solar, earth, sun, node, nasa, near, total, occur, center, flight, possible, cover, completely, experience, one, two, acknowledgement, type, location</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>health, google, search, interest, care, condition, related, top, tech, highest, increase, towards, based, worldwide, volume, predictor, pressure, ongoing, cover, similarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>new, york, new york, school, city, numeric, student, uci, york city, job, education, learning, http, edu, machine learning, machine, ml, grade, archive, http archive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>price, description, review, doe, yet, dataset doe, doe description, description yet, company, product, sale, stock, property, car, market, restaurant, date, city, total, inspiration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>university, state, college, california, north, institute, university california, san, technology, new, francisco, south, john, washington, st, andrew, international, county, science, community</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>txt, instance, cell, group, cancer, number, medical, attribute, hospital, method, application, expression, missing, size, pp, call, body, removed, set, flow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                    Terms per Topic\n",
              "Topic1   image, file, one, time, wa, contains, acknowledgement, inspiration, day, like, using, would, feature, used, set, see, available, column, label, different                                                                 \n",
              "Topic2   image, wa, time, text, survey, csv, acknowledgement, using, file, statistic, open, ha, numeric, set, value, price, http, learning, database, paper                                                                        \n",
              "Topic3   player, csv, team, game, file, wa, http, user, movie, time, match, com, number, ha, www, information, http www, set, id, rating                                                                                           \n",
              "Topic4   context data, love, claim, basic, stats, inspiration, filed, al, east, acknowledgement, modification, john, lost, player, sometimes, file, contains, number, ha, year                                                     \n",
              "Topic5   wa, state, year, information, acknowledgement, time, number, country, ha, available, inspiration, national, source, public, date, record, report, survey, includes, world                                                 \n",
              "Topic6   text, word, language, file, corpus, id, http, use, wa, title, contains, article, license, question, english, page, name, org, author, collection                                                                          \n",
              "Topic7   set, data set, value, column, activity, wa, mean, energy, model, sample, feature, using, use, used, research, sensor, attribute, system, temperature, specie                                                              \n",
              "Topic8   com, http, tweet, github, github com, twitter, http github, vector, http www, www, word, nltk, model, kaggle, acknowledgement, found, zip, code, tree, used                                                               \n",
              "Topic9   year, age, type, month, animal, rate, sex, male, female, outcome, bank, birth, name, credit, number, payment, set, amount, usd, data set                                                                                  \n",
              "Topic10  model, trained, de, pre, pre trained, trained model, feature, network, ha, depth, layer, deep, architecture, time, image, learned, large, en, model pre, learned feature                                                  \n",
              "Topic11  csv, name, code, map, city, open, latitude, longitude, number, street, district, india, open data, country, coordinate, crime, http, contains, license, acknowledgement                                                   \n",
              "Topic12  variable, integer, customer, attribute, categorical, datasets, park, original, file, bird, credit, german, http, continuous, one, good, two, form, version, ml                                                            \n",
              "Topic13  back, lower, air, sound, may, low, flight, mile, cause, distance, body, problem, go, normal, taking, smaller, month, large, might, probability                                                                            \n",
              "Topic14  others, data science, community, world, science, inspiration, inspiration data, acknowledgement, question, thanks, answered, research, want, want see, largest, world largest, science community, past research, see, past\n",
              "Topic15  space, solar, earth, sun, node, nasa, near, total, occur, center, flight, possible, cover, completely, experience, one, two, acknowledgement, type, location                                                              \n",
              "Topic16  health, google, search, interest, care, condition, related, top, tech, highest, increase, towards, based, worldwide, volume, predictor, pressure, ongoing, cover, similarity                                              \n",
              "Topic17  new, york, new york, school, city, numeric, student, uci, york city, job, education, learning, http, edu, machine learning, machine, ml, grade, archive, http archive                                                     \n",
              "Topic18  price, description, review, doe, yet, dataset doe, doe description, description yet, company, product, sale, stock, property, car, market, restaurant, date, city, total, inspiration                                     \n",
              "Topic19  university, state, college, california, north, institute, university california, san, technology, new, francisco, south, john, washington, st, andrew, international, county, science, community                          \n",
              "Topic20  txt, instance, cell, group, cancer, number, medical, attribute, hospital, method, application, expression, missing, size, pp, call, body, removed, set, flow                                                              "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8BnOkDImgOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "021e8cf1-0436-4439-88c5-66967845b246"
      },
      "source": [
        "pd.options.display.float_format = '{:,.3f}'.format\n",
        "dt_df = pd.DataFrame(document_topics, \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.T"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2110</th>\n",
              "      <th>2111</th>\n",
              "      <th>2112</th>\n",
              "      <th>2113</th>\n",
              "      <th>2114</th>\n",
              "      <th>2115</th>\n",
              "      <th>2116</th>\n",
              "      <th>2117</th>\n",
              "      <th>2118</th>\n",
              "      <th>2119</th>\n",
              "      <th>2120</th>\n",
              "      <th>2121</th>\n",
              "      <th>2122</th>\n",
              "      <th>2123</th>\n",
              "      <th>2124</th>\n",
              "      <th>2125</th>\n",
              "      <th>2126</th>\n",
              "      <th>2127</th>\n",
              "      <th>2128</th>\n",
              "      <th>2129</th>\n",
              "      <th>2130</th>\n",
              "      <th>2131</th>\n",
              "      <th>2132</th>\n",
              "      <th>2133</th>\n",
              "      <th>2134</th>\n",
              "      <th>2135</th>\n",
              "      <th>2136</th>\n",
              "      <th>2137</th>\n",
              "      <th>2138</th>\n",
              "      <th>2139</th>\n",
              "      <th>2140</th>\n",
              "      <th>2141</th>\n",
              "      <th>2142</th>\n",
              "      <th>2143</th>\n",
              "      <th>2144</th>\n",
              "      <th>2145</th>\n",
              "      <th>2146</th>\n",
              "      <th>2147</th>\n",
              "      <th>2148</th>\n",
              "      <th>2149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1</th>\n",
              "      <td>0.416</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.401</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.368</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.978</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.908</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.874</td>\n",
              "      <td>0.905</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.851</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.934</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.345</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.587</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T6</th>\n",
              "      <td>0.072</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.989</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.415</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T7</th>\n",
              "      <td>0.324</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.523</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T8</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T9</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.034</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T10</th>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T11</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T12</th>\n",
              "      <td>0.088</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.842</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T13</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.988</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T14</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.990</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T15</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T16</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.025</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T17</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.579</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T18</th>\n",
              "      <td>0.069</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.573</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.564</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T19</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T20</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 2150 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2     3     4     5  ...  2144  2145  2146  2147  2148  2149\n",
              "T1  0.416 0.214 0.413 0.067 0.226 0.109  ... 0.008 0.001 0.764 0.002 0.720 0.001\n",
              "T2  0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T3  0.000 0.775 0.291 0.070 0.267 0.361  ... 0.008 0.851 0.001 0.002 0.001 0.001\n",
              "T4  0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.030 0.001\n",
              "T5  0.000 0.000 0.268 0.764 0.210 0.321  ... 0.008 0.001 0.001 0.002 0.001 0.383\n",
              "T6  0.072 0.000 0.000 0.080 0.000 0.070  ... 0.008 0.001 0.001 0.966 0.001 0.001\n",
              "T7  0.324 0.000 0.000 0.000 0.000 0.022  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T8  0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.047 0.002 0.001 0.001\n",
              "T9  0.000 0.000 0.025 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.226 0.575\n",
              "T10 0.024 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T11 0.000 0.000 0.000 0.000 0.000 0.010  ... 0.008 0.132 0.118 0.002 0.001 0.001\n",
              "T12 0.088 0.000 0.000 0.000 0.000 0.000  ... 0.842 0.001 0.059 0.002 0.001 0.001\n",
              "T13 0.000 0.008 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T14 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T15 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T16 0.000 0.000 0.000 0.003 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.023\n",
              "T17 0.000 0.000 0.000 0.000 0.000 0.025  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T18 0.069 0.000 0.000 0.000 0.291 0.067  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T19 0.000 0.000 0.000 0.014 0.000 0.000  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "T20 0.000 0.000 0.000 0.000 0.000 0.013  ... 0.008 0.001 0.001 0.002 0.001 0.001\n",
              "\n",
              "[20 rows x 2150 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1zeUDjgo7DH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63ab1e98-a336-42ea-a193-5be950a89f22"
      },
      "source": [
        "pd.options.display.float_format = '{:,.5f}'.format\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "max_contrib_topics = dt_df.max(axis=0)\n",
        "dominant_topics = max_contrib_topics.index\n",
        "contrib_perc = max_contrib_topics.values\n",
        "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0]\n",
        "                       for t in dominant_topics]\n",
        "documents = [papers[i] for i in document_numbers]\n",
        "\n",
        "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
        "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
        "                          'Paper Name': documents})\n",
        "results_df"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Paper Num</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Paper Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>T1</td>\n",
              "      <td>0.99435</td>\n",
              "      <td>1977</td>\n",
              "      <td>image, file, one, time, wa, contains, acknowledgement, inspiration, day, like, using, would, feature, used, set, see, available, column, label, different</td>\n",
              "      <td>Context\\nWhile studying neural networks in machine learning, I found an ingenious 2-D scatter pattern at the cn231 course by Andrej Karpathy. Decision boundaries for the three classes of points ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>T2</td>\n",
              "      <td>0.05000</td>\n",
              "      <td>589</td>\n",
              "      <td>image, wa, time, text, survey, csv, acknowledgement, using, file, statistic, open, ha, numeric, set, value, price, http, learning, database, paper</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>T3</td>\n",
              "      <td>0.99554</td>\n",
              "      <td>1881</td>\n",
              "      <td>player, csv, team, game, file, wa, http, user, movie, time, match, com, number, ha, www, information, http www, set, id, rating</td>\n",
              "      <td>Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>T4</td>\n",
              "      <td>0.91364</td>\n",
              "      <td>2037</td>\n",
              "      <td>context data, love, claim, basic, stats, inspiration, filed, al, east, acknowledgement, modification, john, lost, player, sometimes, file, contains, number, ha, year</td>\n",
              "      <td>Context\\nData of hitters in MLB's AL East\\nContent\\nBasic fundamental data on AL East hitters sorted descending by Plate Appearances\\nAcknowledgements\\nInspiration\\nI love baseball and stats.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>T5</td>\n",
              "      <td>0.99195</td>\n",
              "      <td>1811</td>\n",
              "      <td>wa, state, year, information, acknowledgement, time, number, country, ha, available, inspiration, national, source, public, date, record, report, survey, includes, world</td>\n",
              "      <td>Context:\\nMapping the Klan is a rough timeline of the rise of the second Ku Klux Klan between 1915 and 1940. Each red dot shows a local unit or \"Klavern.\" The official numbers for each Klavern ind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>T6</td>\n",
              "      <td>0.99379</td>\n",
              "      <td>1912</td>\n",
              "      <td>text, word, language, file, corpus, id, http, use, wa, title, contains, article, license, question, english, page, name, org, author, collection</td>\n",
              "      <td>Context:\\nSome words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>T7</td>\n",
              "      <td>0.99525</td>\n",
              "      <td>1986</td>\n",
              "      <td>set, data set, value, column, activity, wa, mean, energy, model, sample, feature, using, use, used, research, sensor, attribute, system, temperature, specie</td>\n",
              "      <td>This research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from France between Jan-2016 and Aug-2016 were interviewed in ord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>T8</td>\n",
              "      <td>0.98827</td>\n",
              "      <td>1870</td>\n",
              "      <td>com, http, tweet, github, github com, twitter, http github, vector, http www, www, word, nltk, model, kaggle, acknowledgement, found, zip, code, tree, used</td>\n",
              "      <td>Context\\nThe maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus (perhaps ACE ACE 2004 Multilingual Training Corpus?)\\nIt will load an nltk.chunk.name...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>T9</td>\n",
              "      <td>0.99472</td>\n",
              "      <td>1841</td>\n",
              "      <td>year, age, type, month, animal, rate, sex, male, female, outcome, bank, birth, name, credit, number, payment, set, amount, usd, data set</td>\n",
              "      <td>Context\\nThe Austin Animal Center is the largest no-kill animal shelter in the United States that provides care and shelter to over 18,000 animals each year and is involved in a range of county, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>T10</td>\n",
              "      <td>0.99625</td>\n",
              "      <td>1355</td>\n",
              "      <td>model, trained, de, pre, pre trained, trained model, feature, network, ha, depth, layer, deep, architecture, time, image, learned, large, en, model pre, learned feature</td>\n",
              "      <td>InceptionV3\\nRethinking the Inception Architecture for Computer Vision\\nConvolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>T11</td>\n",
              "      <td>0.99208</td>\n",
              "      <td>1276</td>\n",
              "      <td>csv, name, code, map, city, open, latitude, longitude, number, street, district, india, open data, country, coordinate, crime, http, contains, license, acknowledgement</td>\n",
              "      <td>Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>T12</td>\n",
              "      <td>0.99326</td>\n",
              "      <td>1838</td>\n",
              "      <td>variable, integer, customer, attribute, categorical, datasets, park, original, file, bird, credit, german, http, continuous, one, good, two, form, version, ml</td>\n",
              "      <td>First of all, this dataset is not mine!\\nI just want to use this dataset to approve my machine learning skills.\\nSo I upload this one! :-)\\nHope you like it!\\n=====================================...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>T13</td>\n",
              "      <td>0.98827</td>\n",
              "      <td>106</td>\n",
              "      <td>back, lower, air, sound, may, low, flight, mile, cause, distance, body, problem, go, normal, taking, smaller, month, large, might, probability</td>\n",
              "      <td>310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>T14</td>\n",
              "      <td>0.98989</td>\n",
              "      <td>2134</td>\n",
              "      <td>others, data science, community, world, science, inspiration, inspiration data, acknowledgement, question, thanks, answered, research, want, want see, largest, world largest, science community, pa...</td>\n",
              "      <td>Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>T15</td>\n",
              "      <td>0.98516</td>\n",
              "      <td>272</td>\n",
              "      <td>space, solar, earth, sun, node, nasa, near, total, occur, center, flight, possible, cover, completely, experience, one, two, acknowledgement, type, location</td>\n",
              "      <td>Context\\nEclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral, umbral, or antumbral shadows...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>T16</td>\n",
              "      <td>0.93214</td>\n",
              "      <td>2015</td>\n",
              "      <td>health, google, search, interest, care, condition, related, top, tech, highest, increase, towards, based, worldwide, volume, predictor, pressure, ongoing, cover, similarity</td>\n",
              "      <td>The top \"how to\" related searches on Google from 2004 to 2017 worldwide. Top searches are searches with the highest search interest based on volume.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>T17</td>\n",
              "      <td>0.98797</td>\n",
              "      <td>1797</td>\n",
              "      <td>new, york, new york, school, city, numeric, student, uci, york city, job, education, learning, http, edu, machine learning, machine, ml, grade, archive, http archive</td>\n",
              "      <td>Context:\\nThe City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed, e.g. “gut renovated”) buildings in New York City. These documents assert that the cit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>T18</td>\n",
              "      <td>0.98920</td>\n",
              "      <td>593</td>\n",
              "      <td>price, description, review, doe, yet, dataset doe, doe description, description yet, company, product, sale, stock, property, car, market, restaurant, date, city, total, inspiration</td>\n",
              "      <td>About This Data\\nThis is a list of over 18,000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address, city, state, busi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>T19</td>\n",
              "      <td>0.99823</td>\n",
              "      <td>2071</td>\n",
              "      <td>university, state, college, california, north, institute, university california, san, technology, new, francisco, south, john, washington, st, andrew, international, county, science, community</td>\n",
              "      <td>Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>T20</td>\n",
              "      <td>0.99749</td>\n",
              "      <td>2011</td>\n",
              "      <td>txt, instance, cell, group, cancer, number, medical, attribute, hospital, method, application, expression, missing, size, pp, call, body, removed, set, flow</td>\n",
              "      <td>Context\\nThis breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.\\nContent\\nPast Usage:\\nAttributes 2 through 10 have been used to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topic  ...                                                                                                                                                                                               Paper Name\n",
              "Topic1              T1  ...  Context\\nWhile studying neural networks in machine learning, I found an ingenious 2-D scatter pattern at the cn231 course by Andrej Karpathy. Decision boundaries for the three classes of points ca...\n",
              "Topic2              T2  ...                                                                                                                                                                                                      nan\n",
              "Topic3              T3  ...  Context\\nThis dataset was built as a supplementary to \"[European Soccer Database][1]\". It includes data dictionary, extraction of detailed match information previously contains in XML columns.\\nCo...\n",
              "Topic4              T4  ...          Context\\nData of hitters in MLB's AL East\\nContent\\nBasic fundamental data on AL East hitters sorted descending by Plate Appearances\\nAcknowledgements\\nInspiration\\nI love baseball and stats.\n",
              "Topic5              T5  ...  Context:\\nMapping the Klan is a rough timeline of the rise of the second Ku Klux Klan between 1915 and 1940. Each red dot shows a local unit or \"Klavern.\" The official numbers for each Klavern ind...\n",
              "Topic6              T6  ...  Context:\\nSome words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. Th...\n",
              "Topic7              T7  ...  This research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from France between Jan-2016 and Aug-2016 were interviewed in ord...\n",
              "Topic8              T8  ...  Context\\nThe maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus (perhaps ACE ACE 2004 Multilingual Training Corpus?)\\nIt will load an nltk.chunk.name...\n",
              "Topic9              T9  ...  Context\\nThe Austin Animal Center is the largest no-kill animal shelter in the United States that provides care and shelter to over 18,000 animals each year and is involved in a range of county, c...\n",
              "Topic10            T10  ...  InceptionV3\\nRethinking the Inception Architecture for Computer Vision\\nConvolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since...\n",
              "Topic11            T11  ...  Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one dat...\n",
              "Topic12            T12  ...  First of all, this dataset is not mine!\\nI just want to use this dataset to approve my machine learning skills.\\nSo I upload this one! :-)\\nHope you like it!\\n=====================================...\n",
              "Topic13            T13  ...  310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconn...\n",
              "Topic14            T14  ...  Context\\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair...\n",
              "Topic15            T15  ...  Context\\nEclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral, umbral, or antumbral shadows...\n",
              "Topic16            T16  ...                                                     The top \"how to\" related searches on Google from 2004 to 2017 worldwide. Top searches are searches with the highest search interest based on volume.\n",
              "Topic17            T17  ...  Context:\\nThe City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed, e.g. “gut renovated”) buildings in New York City. These documents assert that the cit...\n",
              "Topic18            T18  ...  About This Data\\nThis is a list of over 18,000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address, city, state, busi...\n",
              "Topic19            T19  ...  Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...\n",
              "Topic20            T20  ...  Context\\nThis breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.\\nContent\\nPast Usage:\\nAttributes 2 through 10 have been used to...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEiZSkCvo9Zf",
        "colab_type": "text"
      },
      "source": [
        "## Topic Models with Non-Negative Matrix Factorization (NMF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bzFpRIAo7U0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3caa5013-ced9-454a-e388-a167c8be87a1"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500,\n",
        "                random_state=42, alpha=.1, l1_ratio=.85)\n",
        "document_topics = nmf_model.fit_transform(cv_features)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.56 s, sys: 220 ms, total: 4.78 s\n",
            "Wall time: 4.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3df-gbo7a1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa8cc522-1991-4956-992f-8143a1c9cb2c"
      },
      "source": [
        "topic_terms = nmf_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                  Terms per Topic\n",
              "Topic1   university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago \n",
              "Topic2   http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match                         \n",
              "Topic3   player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit                             \n",
              "Topic4   integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time                              \n",
              "Topic5   date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status                             \n",
              "Topic6   year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people                        \n",
              "Topic7   numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class                                   \n",
              "Topic8   csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included                                \n",
              "Topic9   model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using\n",
              "Topic10  value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time                                        \n",
              "Topic11  child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last                                       \n",
              "Topic12  de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value                                                    \n",
              "Topic13  file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org                                                  \n",
              "Topic14  image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease                             \n",
              "Topic15  word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also                      \n",
              "Topic16  fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record                          \n",
              "Topic17  race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run                                        \n",
              "Topic18  station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date                                    \n",
              "Topic19  police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago                                   \n",
              "Topic20  health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type                     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66q9hgKeo7Y1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "55fcacd1-ec78-405a-a4b9-49e6b7fef279"
      },
      "source": [
        "pd.options.display.float_format = '{:,.3f}'.format\n",
        "dt_df = pd.DataFrame(document_topics, \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.head(10)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "      <th>T11</th>\n",
              "      <th>T12</th>\n",
              "      <th>T13</th>\n",
              "      <th>T14</th>\n",
              "      <th>T15</th>\n",
              "      <th>T16</th>\n",
              "      <th>T17</th>\n",
              "      <th>T18</th>\n",
              "      <th>T19</th>\n",
              "      <th>T20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.184</td>\n",
              "      <td>0.636</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.402</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     T1    T2    T3    T4    T5    T6  ...   T15   T16   T17   T18   T19   T20\n",
              "0 0.000 0.195 0.000 0.000 0.019 0.000  ... 0.000 0.000 0.063 0.255 0.000 0.026\n",
              "1 0.000 0.665 0.709 0.047 0.000 0.000  ... 0.000 0.000 0.032 0.025 0.000 0.000\n",
              "2 0.001 0.476 0.013 0.058 0.089 0.023  ... 0.003 0.002 0.042 0.000 0.000 0.001\n",
              "3 0.184 0.636 0.000 0.000 0.218 0.000  ... 0.000 0.548 0.210 0.000 0.047 0.178\n",
              "4 0.000 0.214 0.023 0.000 0.031 0.000  ... 0.000 0.016 0.128 0.013 0.006 0.000\n",
              "5 0.010 0.357 0.067 0.038 0.031 0.035  ... 0.000 0.070 0.143 0.187 0.000 0.375\n",
              "6 0.000 0.088 0.008 0.000 0.002 0.000  ... 0.026 0.000 0.000 0.005 0.000 0.000\n",
              "7 0.000 0.101 0.000 0.000 0.013 0.008  ... 0.007 0.000 0.000 0.011 0.000 0.033\n",
              "8 0.000 0.193 0.000 0.000 0.110 0.059  ... 0.001 0.000 0.000 0.108 0.000 0.000\n",
              "9 0.000 0.217 0.001 0.017 0.000 0.011  ... 0.000 0.004 0.128 0.000 0.025 0.167\n",
              "\n",
              "[10 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1vJRNwipBNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b017e6ae-5ed9-4dd7-c55c-492456c0420f"
      },
      "source": [
        "pd.options.display.float_format = '{:,.5f}'.format\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "max_score_topics = dt_df.max(axis=0)\n",
        "dominant_topics = max_score_topics.index\n",
        "term_score = max_score_topics.values\n",
        "document_numbers = [dt_df[dt_df[t] == max_score_topics.loc[t]].index[0]\n",
        "                       for t in dominant_topics]\n",
        "documents = [papers[i] for i in document_numbers]\n",
        "\n",
        "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Max Score': term_score,\n",
        "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
        "                          'Paper Name': documents})\n",
        "results_df"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Max Score</th>\n",
              "      <th>Paper Num</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Paper Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>T1</td>\n",
              "      <td>16.32296</td>\n",
              "      <td>2071</td>\n",
              "      <td>university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago</td>\n",
              "      <td>Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>T2</td>\n",
              "      <td>1.63533</td>\n",
              "      <td>585</td>\n",
              "      <td>http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match</td>\n",
              "      <td>About the Missing Migrants Data\\nThis data is sourced from the International Organization for Migration. The data is part of a specific project called the Missing Migrants Project which tracks dea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>T3</td>\n",
              "      <td>14.46719</td>\n",
              "      <td>627</td>\n",
              "      <td>player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit</td>\n",
              "      <td>Context &amp; Content\\nThis dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) population...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>T4</td>\n",
              "      <td>13.65851</td>\n",
              "      <td>41</td>\n",
              "      <td>integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time</td>\n",
              "      <td>Introduction\\nIn 2013, students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.\\nThe data file (responses.csv) consists of 1010 rows and 150 co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>T5</td>\n",
              "      <td>12.93043</td>\n",
              "      <td>1469</td>\n",
              "      <td>date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status</td>\n",
              "      <td>Context:\\nThis dataset contains pending and registered trademark text data (no drawings/images) to include word mark, serial number, registration number, filing date, registration date, goods and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>T6</td>\n",
              "      <td>7.93463</td>\n",
              "      <td>238</td>\n",
              "      <td>year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people</td>\n",
              "      <td>The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>T7</td>\n",
              "      <td>8.89455</td>\n",
              "      <td>13</td>\n",
              "      <td>numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class</td>\n",
              "      <td>A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>T8</td>\n",
              "      <td>6.12363</td>\n",
              "      <td>507</td>\n",
              "      <td>csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included</td>\n",
              "      <td>Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nMinistry of Human Resource Development (MHRD), Govt of India has initiated an All India Survey on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>T9</td>\n",
              "      <td>2.10993</td>\n",
              "      <td>1357</td>\n",
              "      <td>model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using</td>\n",
              "      <td>DenseNet-201\\nDensely Connected Convolutional Networks\\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>T10</td>\n",
              "      <td>7.23471</td>\n",
              "      <td>416</td>\n",
              "      <td>value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time</td>\n",
              "      <td>Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>T11</td>\n",
              "      <td>7.73633</td>\n",
              "      <td>1181</td>\n",
              "      <td>child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last</td>\n",
              "      <td>Context\\nAbraham Lincoln's election produced Southern secession, war, and abolition. This dataset was used to study connections between news and slave prices for the period 1856-1861. By August 18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>T12</td>\n",
              "      <td>4.62960</td>\n",
              "      <td>812</td>\n",
              "      <td>de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value</td>\n",
              "      <td>«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>T13</td>\n",
              "      <td>4.97141</td>\n",
              "      <td>1963</td>\n",
              "      <td>file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org</td>\n",
              "      <td>Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>T14</td>\n",
              "      <td>3.33996</td>\n",
              "      <td>183</td>\n",
              "      <td>image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease</td>\n",
              "      <td>NIH Chest X-ray Dataset\\nNational Institutes of Health Chest X-Ray Dataset\\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>T15</td>\n",
              "      <td>2.91703</td>\n",
              "      <td>1927</td>\n",
              "      <td>word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also</td>\n",
              "      <td>Context\\nHuman communication abilities have greatly evolved with time. Speech/Text/Images/Videos are the channels we often use to communicate, store/share information.\"Text\" is one of the primary ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>T16</td>\n",
              "      <td>7.54752</td>\n",
              "      <td>120</td>\n",
              "      <td>fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record</td>\n",
              "      <td>Context:\\nThis data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>T17</td>\n",
              "      <td>7.23678</td>\n",
              "      <td>601</td>\n",
              "      <td>race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run</td>\n",
              "      <td>Can you beat the market?\\nHorse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. Inspired by the pioneers of computer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>T18</td>\n",
              "      <td>9.11955</td>\n",
              "      <td>160</td>\n",
              "      <td>station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date</td>\n",
              "      <td>Version 5 Description\\nTLDR\\nThe directory 1-1-16_5-31-17_Weather contains 1663 files (one for each of the 1663 stations in Japan)\\nAs its name implies, the data is from the same date window as th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>T19</td>\n",
              "      <td>8.26405</td>\n",
              "      <td>257</td>\n",
              "      <td>police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago</td>\n",
              "      <td>Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis dataset contains complete information about various aspects of crimes happened in India from...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>T20</td>\n",
              "      <td>8.32362</td>\n",
              "      <td>571</td>\n",
              "      <td>health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type</td>\n",
              "      <td>Context\\nHealth care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. 58% of US community hospit...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topic  ...                                                                                                                                                                                               Paper Name\n",
              "Topic1              T1  ...  Context\\nData was grabbed from US-News: https://www.usnews.com\\nThe following data points are included in this data set:\\nRanking\\nAcceptance-Rate\\nAct-Avg\\nSat-Avg\\nPhoto\\nCost after Financial Ai...\n",
              "Topic2              T2  ...  About the Missing Migrants Data\\nThis data is sourced from the International Organization for Migration. The data is part of a specific project called the Missing Migrants Project which tracks dea...\n",
              "Topic3              T3  ...  Context & Content\\nThis dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) population...\n",
              "Topic4              T4  ...  Introduction\\nIn 2013, students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.\\nThe data file (responses.csv) consists of 1010 rows and 150 co...\n",
              "Topic5              T5  ...  Context:\\nThis dataset contains pending and registered trademark text data (no drawings/images) to include word mark, serial number, registration number, filing date, registration date, goods and ...\n",
              "Topic6              T6  ...  The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists o...\n",
              "Topic7              T7  ...  A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of informa...\n",
              "Topic8              T8  ...  Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nMinistry of Human Resource Development (MHRD), Govt of India has initiated an All India Survey on...\n",
              "Topic9              T9  ...  DenseNet-201\\nDensely Connected Convolutional Networks\\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter...\n",
              "Topic10            T10  ...  Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQ...\n",
              "Topic11            T11  ...  Context\\nAbraham Lincoln's election produced Southern secession, war, and abolition. This dataset was used to study connections between news and slave prices for the period 1856-1861. By August 18...\n",
              "Topic12            T12  ...  «Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents mov...\n",
              "Topic13            T13  ...  Summary\\nThis dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications acros...\n",
              "Topic14            T14  ...  NIH Chest X-ray Dataset\\nNational Institutes of Health Chest X-Ray Dataset\\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clini...\n",
              "Topic15            T15  ...  Context\\nHuman communication abilities have greatly evolved with time. Speech/Text/Images/Videos are the channels we often use to communicate, store/share information.\"Text\" is one of the primary ...\n",
              "Topic16            T16  ...  Context:\\nThis data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to suppo...\n",
              "Topic17            T17  ...  Can you beat the market?\\nHorse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. Inspired by the pioneers of computer...\n",
              "Topic18            T18  ...  Version 5 Description\\nTLDR\\nThe directory 1-1-16_5-31-17_Weather contains 1663 files (one for each of the 1663 stations in Japan)\\nAs its name implies, the data is from the same date window as th...\n",
              "Topic19            T19  ...  Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis dataset contains complete information about various aspects of crimes happened in India from...\n",
              "Topic20            T20  ...  Context\\nHealth care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. 58% of US community hospit...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQoLkjjpJB7",
        "colab_type": "text"
      },
      "source": [
        "# Persisting Model and Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W1M0q23pL38",
        "colab_type": "text"
      },
      "source": [
        "This is just for visualizing the topics in the other notebook (since PyLDAViz expands the notebook size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-ljpUCpBVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dill\n",
        "\n",
        "with open('nmf_model.pkl', 'wb') as f:\n",
        "    dill.dump(nmf_model, f)\n",
        "with open('cv_features.pkl', 'wb') as f:\n",
        "    dill.dump(cv_features, f)\n",
        "with open('cv.pkl', 'wb') as f:\n",
        "    dill.dump(cv, f)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZX5ZUZZqkZt",
        "colab_type": "text"
      },
      "source": [
        "# Ch06d - Visualizing Topic Models with pyLDAvis.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF2v03rErLtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "299637a6-2615-4e86-fc63-acf97df58176"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 17.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (49.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=b920c9aa31ce1fd04349b206e8fd3a5714a8245fdcca3f603a8a2ff700953c69\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=45858e232b86e35ade290994127177ddb0b1f039494b9eb416abaf7cd8bf9561\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On8UrEeOpBTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import dill\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyedglvPpBRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('nmf_model.pkl', 'rb') as f:\n",
        "    nmf_model = dill.load(f)\n",
        "with open('cv_features.pkl', 'rb') as f:\n",
        "    cv_features = dill.load(f)\n",
        "with open('cv.pkl', 'rb') as f:\n",
        "    cv = dill.load(f)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efPPNcboqqP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1923d813-6af7-4102-8e14-dff45eb7338f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "top_terms = 20\n",
        "TOTAL_TOPICS = 20\n",
        "vocabulary = np.array(cv.get_feature_names())\n",
        "topic_terms = nmf_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                  Terms per Topic\n",
              "Topic1   university, state, college, california, university california, institute, technology, north, new, san, st, south, international, washington, john, school, west, tech, science, chicago \n",
              "Topic2   http, wa, information, com, set, www, acknowledgement, column, available, source, use, http www, also, ha, inspiration, one, contains, country, data set, match                         \n",
              "Topic3   player, team, wa, goal, attempt, taken, weighted, zone, game, allowed, minute, per, average, individual, percentage, scored, relative, point, expected, hit                             \n",
              "Topic4   integer, interested, enjoy, much, movie, categorical, people, always, music, interest, preference, lot, item, money, point, thing, life, often, make, time                              \n",
              "Topic5   date, element, one, registration, number, zero, time, tag, end, start, application, file, containing, position, code, mark, version, field, section, status                             \n",
              "Topic6   year, total, given, population, energy, percent, million, state, consumption, net, average, usd, billion, old, year year, rate, age, expenditure, period, people                        \n",
              "Topic7   numeric, text, reading, real, reference, open, food, product, student, sensor, database, fact, binary, left, label, school, grade, right, turn, class                                   \n",
              "Topic8   csv, file, csv file, higher, india, contains, education, row, statistic, ha, name, match, source, one, street, score, like, license, following, included                                \n",
              "Topic9   model, trained, pre, pre trained, feature, trained model, network, layer, ha, time, learned, model pre, transferable, learned feature, deep, architecture, depth, imagenet, large, using\n",
              "Topic10  value, mean, name, hour, event, monitor, air, site, sample, parameter, number, code, measured, monitoring, summary, day, max, wa, standard, time                                        \n",
              "Topic11  child, number, name, age, sold, sale, price, month, listed, year, rate, blank, sex, credit, first, transaction, gender, total number, total, last                                       \n",
              "Topic12  de, en, per, com, le, la, http, number, www, http www, kaggle, kaggle com, ca, ha, datasets, document, index, www kaggle, net, value                                                    \n",
              "Topic13  file, user, movie, id, tag, rating, set, ha, line, data set, use, contains, information, following, tweet, text, title, link, one, org                                                  \n",
              "Topic14  image, label, class, file, contains, zip, sample, pixel, wa, learning, training, one, original, available, set, datasets, used, different, machine, disease                             \n",
              "Topic15  word, language, corpus, text, vector, english, speech, wa, use, frequency, contains, using, file, used, sentence, article, one, acknowledgement, information, also                      \n",
              "Topic16  fire, state, code, department, unit, name, wa, report, national, area, service, center, united, united state, incident, forest, agency, county, bureau, record                          \n",
              "Topic17  race, time, section, point, taken, position, number, team, score, attack, end, length, behind, reach, ranking, result, request, paid, total, run                                        \n",
              "Topic18  station, feature, file, value, weather, null, non, store, id, ratio, latitude, longitude, csv file, name, day, degree, see, location, contains, date                                    \n",
              "Topic19  police, crime, woman, section, total, death, case, court, victim, property, district, child, person, act, age, vehicle, actual, ha, incident, chicago                                   \n",
              "Topic20  health, drug, plan, survey, rating, care, disease, information, part, description, star, product, food, national, name, contains, datasets, measure, variable, type                     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIOQM0EqqaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "95e1098c-b43f-48b9-880d-54ee9fc064cd"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(nmf_model, cv_features, cv, mds='mmds')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-af909b6397f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mmds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA5Y0iGUqqY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "927cf40e-38af-44ed-bb73-f0c048dd27f4"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(nmf_model, cv_features, cv, mds='tsne')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-93fd4556ecc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoFQVIEgqqXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.sklearn.prepare(nmf_model, cv_features, cv, mds='pcoa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTAs-TqbqqTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}