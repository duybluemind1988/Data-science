{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Ch06c - Topic Modeling with scikit-learn.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BnPqvtfFryt",
        "colab_type": "text"
      },
      "source": [
        "# Load and Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9KeLZ85FwDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e0339de9-1cb1-439f-b503-397373817d70"
      },
      "source": [
        "!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
        "!tar -xzf nips12raw_str602.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-22 07:35:02--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
            "Resolving cs.nyu.edu (cs.nyu.edu)... 128.122.49.30\n",
            "Connecting to cs.nyu.edu (cs.nyu.edu)|128.122.49.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12851423 (12M) [application/x-gzip]\n",
            "Saving to: ‘nips12raw_str602.tgz’\n",
            "\n",
            "nips12raw_str602.tg 100%[===================>]  12.26M  14.6MB/s    in 0.8s    \n",
            "\n",
            "2020-08-22 07:35:03 (14.6 MB/s) - ‘nips12raw_str602.tgz’ saved [12851423/12851423]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APZjEge5Fryu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d4ec4eac-6f9a-41a8-82c0-a0b4ab989951"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = 'nipstxt/'\n",
        "print(os.listdir(DATA_PATH))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nips08', 'idx', 'nips09', 'MATLAB_NOTES', 'nips04', 'nips07', 'nips11', 'nips03', 'nips05', 'orig', 'nips06', 'nips02', 'README_yann', 'nips00', 'nips01', 'RAW_DATA_NOTES', 'nips12', 'nips10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eiaTpGJFryz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18a87d3c-7b37-4ffa-9db3-23cb3006a07c"
      },
      "source": [
        "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
        "# Read all texts into a list.\n",
        "papers = []\n",
        "for folder in folders:\n",
        "    file_names = os.listdir(DATA_PATH + folder)\n",
        "    for file_name in file_names:\n",
        "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
        "            data = f.read()\n",
        "        papers.append(data)\n",
        "len(papers)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1740"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja99cSsLFry2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9ac1f6fe-d70d-4f69-c015-33ea2c89cd35"
      },
      "source": [
        "print(papers[0][:1000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "573 \n",
            "BIT - SERIAL NEURAL NETWORKS \n",
            "Alan F. Murray, Anthony V. W. Smith and Zoe F. Buffer. \n",
            "Department of Electrical Engineering, University of Edinburgh, \n",
            "The King's Buildings, Mayfield Road, Edinburgh, \n",
            "Scoff and, EH9 3JL. \n",
            "ABSTRACT \n",
            "A bit - serial VLSI neural network is described from an initial architecture for a \n",
            "synapse array through to silicon layout and board design. The issues surrounding bit \n",
            "- serial computation, and analog/digital arithmetic are discussed and the parallel \n",
            "development of a hybrid analog/digital neural network is outlined. Learning and \n",
            "recall capabilities are reported for the bit - serial network along with a projected \n",
            "specification for a 64 - neuron, bit - serial board operating at 20 MHz. This tech- \n",
            "nique is extended to a 256 (2562 synapses) network with an update time of 3ms, \n",
            "using a \"paging\" technique to time - multiplex calculations through the synapse \n",
            "array. \n",
            "1. INTRODUCTION \n",
            "The functions a synthetic neural network may aspire to mimic are the abil\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skzsyfp4Fry5",
        "colab_type": "text"
      },
      "source": [
        "## Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHm4rGQ2Fry6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "62e5adfa-6420-4c8e-cea4-1cbee1eee167"
      },
      "source": [
        "%%time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(papers):\n",
        "    norm_papers = []\n",
        "    for paper in papers:\n",
        "        paper = paper.lower()\n",
        "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "        paper_tokens = list(filter(None, paper_tokens))\n",
        "        if paper_tokens:\n",
        "            norm_papers.append(paper_tokens)\n",
        "            \n",
        "    return norm_papers\n",
        "    \n",
        "norm_papers = normalize_corpus(papers)\n",
        "print(len(norm_papers))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "1740\n",
            "CPU times: user 28.7 s, sys: 341 ms, total: 29.1 s\n",
            "Wall time: 29.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8tzNEtLzUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06e4a310-c919-43cd-9be5-941cec25fae3"
      },
      "source": [
        "print(norm_papers[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bit', 'serial', 'neural', 'network', 'alan', 'murray', 'anthony', 'smith', 'zoe', 'buffer', 'department', 'electrical', 'engineering', 'university', 'edinburgh', 'king', 'building', 'mayfield', 'road', 'edinburgh', 'scoff', 'eh9', '3jl', 'abstract', 'bit', 'serial', 'vlsi', 'neural', 'network', 'described', 'initial', 'architecture', 'synapse', 'array', 'silicon', 'layout', 'board', 'design', 'issue', 'surrounding', 'bit', 'serial', 'computation', 'analog', 'digital', 'arithmetic', 'discussed', 'parallel', 'development', 'hybrid', 'analog', 'digital', 'neural', 'network', 'outlined', 'learning', 'recall', 'capability', 'reported', 'bit', 'serial', 'network', 'along', 'projected', 'specification', 'neuron', 'bit', 'serial', 'board', 'operating', 'mhz', 'tech', 'nique', 'extended', 'synapsis', 'network', 'update', 'time', '3ms', 'using', 'paging', 'technique', 'time', 'multiplex', 'calculation', 'synapse', 'array', 'introduction', 'function', 'synthetic', 'neural', 'network', 'may', 'aspire', 'mimic', 'ability', 'con', 'sider', 'many', 'solution', 'simultaneously', 'ability', 'work', 'corrupted', 'data', 'natural', 'fault', 'tolerance', 'arises', 'parallelism', 'distributed', 'knowledge', 'representation', 'give', 'rise', 'gentle', 'degradation', 'fault', 'appear', 'func', 'tions', 'attractive', 'implementation', 'vlsi', 'wsi', 'example', 'natural', 'fault', 'tolerance', 'could', 'useful', 'silicon', 'wafer', 'imperfect', 'yield', 'network', 'degradation', 'approximately', 'proportional', 'non', 'functioning', 'silicon', 'area', 'cast', 'neural', 'network', 'engineering', 'language', 'neuron', 'state', 'machine', 'either', 'general', 'assumes', 'intermediate', 'state', 'switch', 'smoothly', 'extremum', 'synapsis', 'weighting', 'signal', 'transmitting', 'neuron', 'le', 'excitatory', 'inhibitory', 'receiv', 'ing', 'neuron', 'set', 'synaptic', 'weight', 'determines', 'stable', 'state', 'represents', 'learned', 'information', 'system', 'neural', 'state', 'vi', 'related', 'total', 'neural', 'activity', 'stimulated', 'input', 'neuron', 'activation', 'function', 'neural', 'activity', 'level', 'excita', 'tion', 'neuron', 'activation', 'way', 'reacts', 'response', 'change', 'activation', 'neural', 'output', 'state', 'time', 'related', 'activation', 'function', 'squashing', 'function', 'ensuring', 'say', 'vi', 'xi', 'large', 'xi', 'small', 'neural', 'update', 'function', 'therefore', 'straight', 'forward', 'represents', 'rate', 'change', 'neural', 'activity', 'tq', 'synaptic', 'weight', 'number', 'term', 'giving', 'neuron', 'array', 'although', 'neural', 'function', 'simple', 'enough', 'totally', 'interconnected', 'neu', 'ron', 'network', 'synapsis', 'requiring', 'multiplication', 'summation', 'american', 'institute', 'physic', 'large', 'number', 'interconnects', 'challenge', 'vlsi', 'therefore', 'design', 'sim', 'ple', 'compact', 'synapse', 'repeated', 'build', 'vlsi', 'neural', 'network', 'manageable', 'interconnect', 'network', 'fixed', 'functionality', 'relatively', 'straightforward', 'network', 'able', 'learn', 'however', 'synaptic', 'weight', 'must', 'programmable', 'therefore', 'complicated', 'designing', 'neural', 'network', 'vlsi', 'fundamentally', 'two', 'approach', 'implementing', 'function', 'silicon', 'digital', 'analog', 'technique', 'ha', 'advantage', 'disadvantage', 'listed', 'along', 'merit', 'demerit', 'bit', 'serial', 'architecture', 'digital', 'synchronous', 'system', 'digital', 'analog', 'primary', 'advantage', 'digital', 'design', 'synapse', 'array', 'digital', 'memory', 'well', 'understood', 'incorporated', 'easily', 'learning', 'network', 'therefore', 'possible', 'without', 'recourse', 'unusual', 'technique', 'technolo', 'gy', 'strength', 'digital', 'approach', 'design', 'technique', 'advanced', 'automated', 'well', 'understood', 'noise', 'immunity', 'computational', 'speed', 'high', 'unattractive', 'feature', 'digital', 'circuit', 'complexity', 'need', 'synchronous', 'state', 'activity', 'quantised', 'real', 'neural', 'network', 'asynchronous', 'unquantised', 'furthermore', 'digital', 'multiplier', 'occupy', 'large', 'silicon', 'area', 'giving', 'low', 'synapse', 'count', 'single', 'chip', 'advantage', 'analog', 'circuitry', 'asynchronous', 'behaviour', 'smooth', 'neural', 'activation', 'automatic', 'circuit', 'element', 'small', 'noise', 'immunity', 'relatively', 'low', 'arbitrarily', 'high', 'precision', 'possible', 'importantly', 'reliable', 'analog', 'non', 'volatile', 'memory', 'technology', 'yet', 'readily', 'available', 'reason', 'learning', 'network', 'lend', 'naturally', 'digital', 'design', 'implementation', 'several', 'group', 'developing', 'neural', 'chip', 'board', 'following', 'listing', 'doe', 'pretend', 'exhaustive', 'included', 'rather', 'indicate', 'spread', 'activity', 'field', 'analog', 'technique', 'used', 'build', 'resistor', 'opera', 'tional', 'amplifier', 'network', 'similar', 'proposed', 'hopfield', 'tank', 'large', 'group', 'caltech', 'developing', 'network', 'implementing', 'early', 'vision', 'auditory', 'processing', 'function', 'using', 'intrinsic', 'nonlinearities', 'mo', 'transistor', 'subthreshold', 'regime', 'problem', 'implementing', 'analog', 'network', 'electrically', 'programmable', 'synapsis', 'ha', 'addressed', 'using', 'ccd', 'mnos', 'technol', 'ogy', 'finally', 'garth', 'developing', 'digital', 'neural', 'accelerator', 'board', 'let', 'sire', 'effectively', 'fast', 'simd', 'processor', 'supporting', 'memory', 'com', 'munications', 'chip', 'bit', 'serial', 'bit', 'parallel', 'bit', 'serial', 'arithmetic', 'communication', 'efficient', 'computational', 'process', 'allowing', 'good', 'communication', 'within', 'vlsi', 'chip', 'tightly', 'pipelined', 'arithmetic', 'structure', 'ideal', 'neural', 'net', 'work', 'minimises', 'interconnect', 'requirement', 'eliminating', 'multi', 'wire', 'bus', 'although', 'bit', 'parallel', 'design', 'would', 'free', 'computational', 'latency', 'delay', 'input', 'output', 'pipelining', 'make', 'optimal', 'use', 'high', 'bit', 'rate', 'possible', 'serial', 'systernx', 'make', 'efficient', 'circuit', 'usage', 'asynchronous', 'pulse', 'stream', 'vlsi', 'neural', 'network', 'addition', 'digital', 'system', 'form', 'substance', 'paper', 'developing', 'hybrid', 'analog', 'digital', 'network', 'family', 'work', 'outlined', 'ha', 'reported', 'greater', 'detail', 'elsewhere', 'generic', 'logical', 'layout', 'architecture', 'single', 'network', 'totally', 'interconnected', 'neuron', 'shown', 'schematically', 'figure', 'neuron', 'represented', 'circle', 'signal', 'state', 'vi', 'upward', 'matrix', 'synaptic', 'operator', 'state', 'signal', 'con', 'neeted', 'bit', 'horizontal', 'bus', 'running', 'synaptic', 'array', 'con', 'nection', 'synaptic', 'operator', 'every', 'column', 'column', 'operator', 'denoted', 'square', 'operator', 'add', 'synaptic', 'contribution', 'vj', 'running', 'total', 'activity', 'neuron', 'foot', 'column', 'synaptic', 'function', 'therefore', 'multiply', 'signalling', 'neuron', 'state', 'vi', 'synaptic', 'weight', 't0', 'add', 'product', 'running', 'total', 'architecture', 'com', 'mon', 'bit', 'serial', 'pulse', 'stream', 'network', '_1', 'synapse', 'state', 'vj', 'neuron', 'figure', 'generic', 'architecture', 'network', 'totally', 'interconnected', 'neuron', 'type', 'architecture', 'ha', 'many', 'attraction', 'implementation', 'dimensional', 'silicon', 'summation', 'distributed', 'space', 'interconnect', 'requirement', 'input', 'neuron', 'therefore', 'distributed', 'column', 'reducing', 'need', 'long', 'range', 'wiring', 'architecture', 'modular', 'regular', 'easily', 'expanded', 'hybrid', 'analog', 'digital', 'system', 'circuitry', 'us', 'pulse', 'stream', 'signalling', 'method', 'similar', 'natural', 'neural', 'system', 'neuron', 'indicate', 'state', 'presence', 'absence', 'pulse', 'output', 'synapfic', 'weighting', 'achieved', 'time', 'chopping', 'presynaptic', 'pulse', 'stream', 'prior', 'adding', 'postsynaptic', 'activity', 'summation', 'therefore', 'asynchronous', 'imposes', 'fun', 'damental', 'limitation', 'activation', 'neural', 'state', 'figure', 'show', 'pulse', 'stream', 'mechanism', 'detail', 'synaptic', 'weight', 'stored', 'digital', 'memory', 'local', 'operator', 'synaptic', 'operator', 'ha', 'excitatory', 'inhibitory', 'pulse', 'stream', 'input', 'output', 'resultant', 'product', 'synaptic', 'operation', 'added', 'running', 'total', 'propagating', 'either', 'excitatory', 'inhibitory', 'channel', 'one', 'binary', 'bit', 'msbit', 'stored', 'determines', 'whether', 'con', 'tribution', 'excitatory', 'inhibitory', 'incoming', 'excitatory', 'inhibitory', 'pulse', 'stream', 'input', 'neuron', 'integrated', 'give', 'neural', 'activation', 'potential', 'varies', 'smoothly', 'potential', 'control', 'feedback', 'loop', 'odd', 'number', 'logic', 'inversion', 'exe', 'znh', 'exe', 'yi', 'figure', 'pulse', 'stream', 'arithmetic', 'neuron', 'denoted', 'synaptic', 'operator', 'bye', 'thus', 'form', 'switched', 'ring', 'oscillator', 'inhibitory', 'input', 'dominates', 'feed', 'back', 'loop', 'broken', 'excitatory', 'spike', 'subsequently', 'dominate', 'input', 'neural', 'activity', 'rise', '5v', 'feedback', 'loop', 'oscillates', 'period', 'determined', 'delay', 'around', 'loop', 'resultant', 'periodic', 'waveform', 'converted', 'series', 'voltage', 'spike', 'whose', 'pulse', 'rate', 'represents', 'neural', 'state', 'vi', 'interest', 'ingly', 'dissimilar', 'technique', 'reported', 'elsewhere', 'volume', 'although', 'synapse', 'function', 'executed', 'differently', 'state', 'bit', 'serial', 'neural', 'network', 'overall', 'architecture', 'state', 'bit', 'serial', 'neural', 'network', 'identical', 'pulse', 'stream', 'network', 'array', 'interconnected', 'synchronous', 'synaptic', 'operator', 'whereas', 'pulse', 'stream', 'method', 'allowed', 'vi', 'assume', 'value', 'state', 'network', 'vj', 'constrained', 'resultant', 'activation', 'function', 'shown', 'figure', 'full', 'digital', 'multiplica', 'tion', 'costly', 'silicon', 'area', 'multiplication', 'vj', 'merely', 'requires', 'synaptic', 'weight', 'right', 'shifted', 'bit', 'similarly', 'multiplication', 'involves', 'right', 'shift', 'multiplication', 'trivially', 'easy', 'vj', 'problematic', 'switchable', 'adder', 'subtractor', 'much', 'complex', 'adder', 'five', 'neural', 'state', 'therefore', 'feasible', 'circuitry', 'slightly', 'complex', 'simple', 'serial', 'adder', 'neural', 'state', 'expands', 'bit', 'bit', 'state', 'representation', 'bit', 'represent', 'add', 'subtract', 'shift', 'multiply', 'figure', 'show', 'part', 'synaptic', 'array', 'synaptic', 'operator', 'includes', 'bit', 'shift', 'register', 'memory', 'block', 'holding', 'synaptic', 'weight', 'bit', 'bus', 'neural', 'state', 'run', 'horizontally', 'synaptic', 'row', 'single', 'phase', 'dynamic', 'cmos', 'ha', 'used', 'clock', 'frequency', 'excess', 'mhz', 'detail', 'synaptic', 'operator', 'shown', 'figure', 'synaptic', 'weight', 'cycle', 'around', 'shift', 'register', 'neural', 'state', 'vj', 'present', 'state', 'bus', 'first', 'clock', 'cycle', 'synaptic', 'weight', 'multiplied', 'neural', 'state', 'second', 'significant', 'bit', 'msbit', 'resultant', 'vj', 'sign', 'extended', 'threshold', 'sme', 'aclivry', 'state', 'vj', 'harper', 'state', 'sigm', 'activity', 'xj', 'figure', 'hard', 'threshold', 'state', 'sigmoid', 'activation', 'function', 'itsjv', 'figure', 'section', 'synaptic', 'array', 'state', 'activation', 'function', 'neural', 'net', 'work', 'bit', 'allow', 'word', 'growth', 'running', 'summation', 'least', 'significant', 'bit', 'lsbit', 'signal', 'running', 'synaptic', 'column', 'indicates', 'arrival', 'lsbit', 'xi', 'running', 'total', 'neural', 'state', 'synaptic', 'weight', 'right', 'shined', 'bit', 'added', 'subtracted', 'running', 'total', 'multipli', 'cation', 'add', 'subtracts', 'weight', 'total', 'multiplication', 'add', 'subtract', 'add', 'subtract', 'itiiv', 'figure', 'synaptic', 'operator', 'state', 'activation', 'function', 'doe', 'alter', 'running', 'summation', 'final', 'summation', 'foot', 'column', 'thresholded', 'externally', 'according', 'state', 'activation', 'function', 'figure', 'neuron', 'activity', 'xj', 'increase', 'threshold', 'value', 'ideal', 'sigmoidal', 'activation', 'represents', 'smooth', 'switch', 'neural', 'state', 'state', 'staircase', 'function', 'give', 'superficially', 'much', 'better', 'approximation', 'sigrnoid', 'ment', 'threshold', 'function', 'sharpness', 'tune', 'neural', 'dynamic', 'learning', 'referred', 'temperature', 'analogy', 'form', 'high', 'temperature', 'give', 'smoother', 'form', 'much', 'simpler', 'imple', 'transition', 'controlled', 'computation', 'control', 'parameter', 'statistical', 'function', 'sigmoidal', 'staircase', 'sigrnoid', 'tempera', 'ture', 'reduces', 'iopfield', 'like', 'threshold', 'function', 'effect', 'temperature', 'learning', 'recall', 'threshold', 'state', 'activation', 'option', 'discussed', 'section', 'learning', 'recall', 'vlsi', 'constraint', 'implementing', 'reduced', 'arithmetic', 'network', 'vlsi', 'simulation', 'experi', 'ments', 'conducted', 'verify', 'state', 'model', 'represented', 'worthwhile', 'enhancement', 'simple', 'threshold', 'activation', 'qaenchmark', 'problem', 'wa', 'chosen', 'ubiquitousness', 'rather', 'intrinsic', 'value', 'implication', 'learning', 'recall', 'state', 'model', 'threshold', 'state', 'model', 'smooth', 'sigrnoidal', 'activation', 'state', 'compared', 'varying', 'temperature', 'restricted', 'dynamic', 'range', 'weight', 'simulation', 'totally', 'interconnected', 'node', 'network', 'attempted', 'learn', 'random', 'pattern', 'using', 'delta', 'rule', 'learning', 'algorithm', 'see', 'example', 'pattern', 'wa', 'cor', 'rupted', 'noise', 'recall', 'attempted', 'probe', 'content', 'addressable', 'memory', 'property', 'three', 'different', 'activation', 'option', 'learning', 'individual', 'weight', 'become', 'large', 'positive', 'negative', 'weight', 'driven', 'beyond', 'maximum', 'value', 'hardware', 'implementation', 'determined', 'size', 'synaptic', 'weight', 'block', 'limiting', 'mechanism', 'must', 'introduced', 'example', 'eight', 'bit', 'weight', 'register', 'limitation', 'tii', 'integer', 'weight', 'seen', 'prob', 'lem', 'dynamic', 'range', 'relafonship', 'smallest', 'possible', 'weight', 'largest', 'issue', 'result', 'fig', 'show', 'example', 'result', 'obtained', 'studying', 'learning', 'using', 'state', 'activation', 'different', 'temperature', 'recall', 'using', 'state', 'thres', 'hold', 'activation', 'temperature', 'state', 'threshold', 'model', 'degenerate', 'result', 'identical', 'increasing', 'smoothness', 'activation', 'tempera', 'ture', 'learning', 'improves', 'quality', 'learning', 'regardless', 'activation', 'function', 'used', 'recall', 'pattern', 'recognised', 'succeasfully', 'using', 'state', 'activation', 'recall', 'effective', 'simple', 'threshold', 'activation', 'effect', 'dynamic', 'range', 'restriction', 'assessed', 'horizontal', 'axis', 'shown', 'result', 'many', 'experiment', 'may', 'summarised', 'follows', 'stste', 'activation', 'threshold', 'learning', 'state', 'activation', 'wa', 'protracted', 'threshold', 'activation', 'binary', 'pattern', 'learnt', 'inclusion', 'intermediate', 'value', 'added', 'extra', 'degree', 'freedom', 'weight', 'set', 'learnt', 'using', 'state', 'activation', 'function', 'etter', 'learnt', 'via', 'threshold', 'activation', 'recall', 'property', 'state', 'threshold', 'network', 'using', 'weight', 'set', 'robust', 'noise', 'full', 'sigmoidal', 'activation', 'wa', 'better', 'state', 'enhancement', 'wa', 'le', 'significant', 'incurred', 'moving', 'threshold', 'state', 'suggests', 'law', 'diminishing', 'return', 'applies', 'addition', 'level', 'neural', 'state', 'vj', 'issue', 'ha', 'studied', 'mathematically', 'result', 'agree', 'qualitatively', 'weight', 'saturation', 'three', 'method', 'tried', 'deal', 'weight', 'saturation', 'firstly', 'inclusion', 'decay', 'forgetting', 'term', 'wa', 'included', 'learning', 'cycle', 'view', 'technique', 'produce', 'desired', 'weight', 'limiting', 'property', 'time', 'available', 'experiment', 'unable', 'tune', 'rate', 'decay', 'sufficiently', 'well', 'confirm', 'renormalisafion', 'weight', 'division', 'bring', 'large', 'weight', 'back', 'dynamic', 'range', 'wa', 'unsuccessful', 'suggesting', 'information', 'distributed', 'throughout', 'numerically', 'small', 'weight', 'wa', 'destroyed', 'finally', 'weight', 'allowed', 'clip', 'ie', 'weight', 'outside', 'dynamic', 'range', 'wa', 'set', 'maximum', 'allowed', 'value', 'method', 'proved', 'successful', 'learn', 'ing', 'algorithm', 'adjusted', 'weight', 'still', 'control', 'compensate', 'saturation', 'effect', 'interesting', 'note', 'experiment', 'indicated', 'hopfield', 'net', 'forget', 'different', 'way', 'different', 'learning', 'control', 'giving', 'preference', 'recently', 'acquired', 'memory', 'result', 'satura', 'tion', 'experiment', 'pattern', 'node', 'problem', 'integer', 'weight', 'dynamic', 'range', 'greater', 'necessary', 'give', 'enough', 'storage', 'capability', 'weight', 'maximum', 'value', 'ti', 'clipping', 'occurs', 'net', 'work', 'performance', 'seriously', 'degraded', 'unrestricted', 'weight', 'set', 'limit', 'state', 'activation', 'function', 'recall', 'limit', 'hopfield', 'activation', 'function', 'recall', 'figure', 'recall', 'pattern', 'learned', 'state', 'activation', 'function', 'subse', 'quently', 'restored', 'using', 'state', 'hard', 'threshold', 'activation', 'function', 'temperature', 'smoothness', 'activation', 'function', 'limit', 'value', 'result', 'showed', 'state', 'model', 'wa', 'worthy', 'implementation', 'vlsi', 'neural', 'board', 'suggested', 'bit', 'weight', 'sufficient', 'projected', 'specification', 'hardware', 'neural', 'board', 'specification', 'neuron', 'board', 'given', 'using', 'state', 'bit', 'serial', 'synapse', 'array', 'derated', 'clock', 'speed', 'mhz', 'synaptic', 'weight', 'bit', 'word', 'word', 'length', 'running', 'summation', 'bit', 'allow', 'growth', 'synapse', 'column', 'ha', 'computational', 'latency', 'clock', 'cycle', 'bit', 'giving', 'update', 'time', 'network', 'time', 'load', 'weight', 'array', 'limited', 'supporting', 'ram', 'access', 'time', '120ns', 'load', 'update', 'time', 'mean', 'network', 'executing', 'operation', 'second', 'one', 'operation', 'jvj', 'much', 'faster', 'natural', 'neural', 'network', 'much', 'faster', 'necessary', 'hardware', 'accelera', 'tor', 'therefore', 'developed', 'paging', 'architecture', 'effectively', 'trade', 'excessive', 'speed', 'increased', 'network', 'size', 'moving', 'patch', 'neural', 'board', 'array', 'state', 'synapsis', 'currently', 'fabricated', 'vlsi', 'integrated', 'circuit', 'shift', 'register', 'adder', 'subtractor', 'synapse', 'occupy', 'disappointingly', 'large', 'silicon', 'area', 'allow', 'ing', 'synaptic', 'array', 'achieve', 'suitable', 'size', 'neural', 'network', 'array', 'several', 'chip', 'need', 'included', 'board', 'memory', 'control', 'circu', 'itry', 'moving', 'patch', 'concept', 'shown', 'figure', 'small', 'array', 'synapsis', 'passed', 'much', 'larger', 'synaptic', 'array', 'time', 'array', 'moved', 'represent', 'another', 'set', 'synapsis', 'new', 'weight', 'must', 'loaded', 'example', 'first', 'set', 'weight', 'tu', 't2j', 'second', 'set', 'tj', 'etc', 'final', 'weight', 'loaded', 'oooo', 'neuron', 'nxn', 'synaptic', 'array', 'small', 'patch', 'move', 'array', 'figure', 'moving', 'patch', 'concept', 'passing', 'small', 'synaptic', 'oatch', 'larger', 'nxn', 'synapse', 'array', 'static', 'shelf', 'ram', 'used', 'store', 'weight', 'whole', 'opera', 'tion', 'pipelined', 'maximum', 'efficiency', 'figure', 'show', 'board', 'level', 'design', 'network', 'synaptic', 'accelerator', 'chip', 'host', 'figure', 'moving', 'patch', 'neural', 'network', 'board', 'small', 'patch', 'move', 'around', 'array', 'give', 'neuron', 'comprises', 'vlsi', 'synaptic', 'accelerator', 'chip', 'give', 'synaptic', 'array', 'number', 'neuron', 'simulated', 'weight', 'stored', 'mb', 'ram', 'load', 'time', '8ms', 'patch', 'movement', 'partial', 'runninz', 'summatin', 'calculated', 'column', 'stored', 'separate', 'ram', 'required', 'added', 'next', 'appropriate', 'summation', 'update', 'time', 'board', '3ms', 'giving', 'operation', 'second', 'slower', 'neuron', 'specification', 'network', 'time', 'larger', 'arithmetic', 'element', 'used', 'efficiently', 'achieve', 'network', 'greater', 'neuron', 'ram', 'required', 'store', 'weight', 'network', 'slower', 'unless', 'larger', 'number', 'accelerator', 'chip', 'used', 'give', 'larger', 'moving', 'patch', 'conclusion', 'strategy', 'design', 'method', 'ha', 'given', 'construction', 'bit', 'serial', 'vlsi', 'neural', 'network', 'chip', 'circuit', 'board', 'bit', 'serial', 'arithmetic', 'coupled', 'reduced', 'arithmetic', 'style', 'enhances', 'level', 'integration', 'possible', 'beyond', 'conventional', 'digital', 'bit', 'parallel', 'scheme', 'restriction', 'imposed', 'synal', 'tic', 'weight', 'size', 'arithmetic', 'precision', 'vlsi', 'constraint', 'examined', 'shown', 'tolerable', 'using', 'associative', 'memory', 'problem', 'test', 'believe', 'digital', 'approach', 'represent', 'good', 'compromise', 'arithmetic', 'accuracy', 'circuit', 'complexity', 'acknowledge', 'level', 'integration', 'disappoinfingly', 'low', 'belief', 'digital', 'approach', 'may', 'interesting', 'useful', 'medium', 'term', 'essentially', 'hardware', 'accelera', 'tor', 'neural', 'simulation', 'analog', 'technique', 'represent', 'best', 'ultimate', 'option', 'dimensional', 'silicon', 'end', 'currently', 'pursuing', 'technique', 'analog', 'pseudo', 'static', 'memory', 'using', 'standard', 'cmos', 'technology', 'event', 'full', 'development', 'nonvolatile', 'analog', 'memory', 'technology', 'mnos', 'tech', 'nique', 'key', 'long', 'term', 'future', 'vlsi', 'neural', 'net', 'learn', 'acknowledgement', 'author', 'acknowledge', 'support', 'science', 'engineering', 'research', 'council', 'uk', 'execution', 'work', 'reference', 'grossberg', 'physiological', 'biochemical', 'consequence', 'psycho', 'logical', 'postulate', 'proc', 'natl', 'acad', 'sci', 'usa', 'vol', 'pp', 'graf', 'jackel', 'howard', 'straughn', 'denker', 'hubbard', 'tennant', 'schwartz', 'vlsi', 'implementation', 'neural', 'network', 'memory', 'several', 'hundred', 'neuron', 'proc', 'alp', 'conference', 'neural', 'network', 'computing', 'snowbird', 'pp', 'mackie', 'graf', 'denker', 'microelectronic', 'implementa', 'tion', 'connectionist', 'neural', 'network', 'model', 'ieee', 'conference', 'neural', 'information', 'processing', 'system', 'denver', 'hopfield', 'tank', 'neural', 'computation', 'decision', 'optim', 'isation', 'problem', 'biol', 'cybern', 'vol', 'pp', 'sivilotti', 'mahowald', 'mead', 'real', 'time', 'visual', 'com', 'putations', 'using', 'analog', 'cmos', 'processing', 'array', 'published', 'mead', 'network', 'real', 'time', 'sensory', 'processing', 'ieee', 'confer', 'ence', 'neural', 'information', 'processing', 'system', 'denver', 'sage', 'thompson', 'withers', 'artificial', 'neural', 'network', 'integrated', 'circuit', 'based', 'mnos', 'ccd', 'principle', 'proc', 'aip', 'conference', 'neural', 'network', 'computing', 'snowbird', 'pp', 'garth', 'chipset', 'high', 'speed', 'simulation', 'neural', 'network', 'sys', 'tems', 'ieee', 'conference', 'neural', 'network', 'san', 'diego', 'murray', 'smith', 'novel', 'computational', 'signalling', 'method', 'vlsi', 'neural', 'network', 'european', 'solid', 'state', 'circuit', 'conference', 'murray', 'smith', 'asynchronous', 'arithmetic', 'vlsi', 'neural', 'system', 'electronics', 'letter', 'vol', 'june', 'murray', 'smith', 'asynchronous', 'vlsi', 'neural', 'network', 'using', 'pulse', 'stream', 'arithmetic', 'ieee', 'journal', 'solid', 'state', 'circuit', 'sys', 'tems', 'published', 'gaspar', 'pulsed', 'neural', 'network', 'hardware', 'software', 'hop', 'field', 'afd', 'converter', 'example', 'ieee', 'conference', 'neural', 'information', 'pro', 'cessing', 'system', 'denver', 'mcgregor', 'denyet', 'murray', 'single', 'phase', 'clock', 'ing', 'scheme', 'cmos', 'vlsi', 'advanced', 'research', 'vlsi', 'proceeding', 'stanford', 'conference', 'rumelhart', 'hinton', 'williams', 'learning', 'internal', 'representation', 'error', 'propagation', 'parallel', 'distributed', 'processing', 'exploration', 'microstructure', 'cognition', 'vol', 'pp', 'fleisher', 'levin', 'hopfiled', 'model', 'multilevel', 'neuron', 'model', 'ieee', 'conference', 'neural', 'information', 'processing', 'system', 'denver', 'parisi', 'memory', 'forgets', 'phys', 'math', 'gert', 'vol', 'pp', 'l617', 'l620']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmEz_b6pFry9",
        "colab_type": "text"
      },
      "source": [
        "# Text Representation with Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6td1VGxTFry9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6bb19ae-06b7-4bb9-b96b-70a0774ce44f"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2),\n",
        "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
        "                     preprocessor=lambda doc: doc)\n",
        "cv_features = cv.fit_transform(norm_papers)\n",
        "cv_features.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1740, 14408)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZjGMzMLFrzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd29f6bf-d030-4e7a-cd9a-5dae0fccca7d"
      },
      "source": [
        "vocabulary = np.array(cv.get_feature_names())\n",
        "print('Total Vocabulary Size:', len(vocabulary))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Vocabulary Size: 14408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPWr0J2JNf17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a1c7b5a-dae2-4c74-c480-6156482adf7b"
      },
      "source": [
        "vocabulary"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0i', '0j', '0o', ..., 'zt', 'zx', 'zz'], dtype='<U28')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5DC2FqHFrzD",
        "colab_type": "text"
      },
      "source": [
        "# Topic Models with Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS4mjJEiFrzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "278cca80-25ad-458d-9cd9-d24a04ef3bd9"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "TOTAL_TOPICS = 20\n",
        "\n",
        "lsi_model = TruncatedSVD(n_components=TOTAL_TOPICS, n_iter=500, random_state=42)\n",
        "document_topics = lsi_model.fit_transform(cv_features)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 27s, sys: 1min 3s, total: 2min 30s\n",
            "Wall time: 1min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNO8-strQHNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6b50867-9b62-423a-ef02-a62a279783f7"
      },
      "source": [
        "document_topics.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1740, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUE0Q1SvFrzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ae6e1f3-3007-4862-f812-dcabd27e19d2"
      },
      "source": [
        "topic_terms = lsi_model.components_\n",
        "topic_terms.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 14408)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI67jj21PBa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "87643291-a40c-42a4-e0c7-5eee2ec156b0"
      },
      "source": [
        "topic_terms"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.12927439e-03,  6.09288836e-04,  1.51008884e-03, ...,\n",
              "         4.34334087e-03,  1.01937083e-03,  5.07621376e-04],\n",
              "       [-3.00632887e-04, -4.76607711e-04,  1.12743502e-03, ...,\n",
              "        -6.32378629e-03, -1.05511388e-03, -2.69528891e-04],\n",
              "       [-1.74476092e-04, -8.63521835e-05,  7.45203760e-04, ...,\n",
              "         4.88534437e-03, -2.09449111e-04, -6.79315314e-05],\n",
              "       ...,\n",
              "       [-1.00841993e-03,  2.29490496e-04,  2.33954453e-04, ...,\n",
              "         3.84500572e-03, -4.62599365e-04,  1.14622887e-03],\n",
              "       [ 5.36057499e-05,  6.68163037e-04,  9.11247662e-04, ...,\n",
              "        -3.91911452e-03,  2.77760569e-04, -7.74824840e-04],\n",
              "       [-1.80776507e-03,  5.75130384e-04,  1.94317046e-04, ...,\n",
              "        -1.05594410e-03, -6.62353527e-05, -4.32904953e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hHdmZxJiFrzJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "794d93a9-ca4f-4162-a74e-3726d235c8c5"
      },
      "source": [
        "top_terms = 20\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterm_weights = np.array([topic_terms[row, columns] \n",
        "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
        "for n in range(TOTAL_TOPICS):\n",
        "    print('Topic #'+str(n+1)+':')\n",
        "    print('='*50)\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    terms, weights = topic_keyterms_weights[n]\n",
        "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
        "                          key=lambda row: -abs(row[1]))\n",
        "    for term, wt in term_weights:\n",
        "        if wt >= 0:\n",
        "            d1.append((term, round(wt, 3)))\n",
        "        else:\n",
        "            d2.append((term, round(wt, 3)))\n",
        "\n",
        "    print('Direction 1:', d1)\n",
        "    print('-'*50)\n",
        "    print('Direction 2:', d2)\n",
        "    print('-'*50)\n",
        "    print()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1:\n",
            "==================================================\n",
            "Direction 1: [('state', 0.221), ('neuron', 0.169), ('image', 0.138), ('cell', 0.13), ('layer', 0.13), ('feature', 0.127), ('probability', 0.121), ('hidden', 0.114), ('distribution', 0.105), ('rate', 0.098), ('signal', 0.095), ('task', 0.093), ('class', 0.092), ('noise', 0.09), ('net', 0.089), ('recognition', 0.089), ('representation', 0.088), ('field', 0.082), ('rule', 0.082), ('step', 0.08)]\n",
            "--------------------------------------------------\n",
            "Direction 2: []\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #2:\n",
            "==================================================\n",
            "Direction 1: [('cell', 0.417), ('neuron', 0.39), ('response', 0.175), ('stimulus', 0.155), ('visual', 0.131), ('spike', 0.13), ('firing', 0.117), ('synaptic', 0.11), ('activity', 0.104), ('cortex', 0.097), ('field', 0.085), ('frequency', 0.085), ('direction', 0.082), ('circuit', 0.082), ('motion', 0.082)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('state', -0.289), ('probability', -0.109), ('hidden', -0.098), ('class', -0.091), ('policy', -0.081)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #3:\n",
            "==================================================\n",
            "Direction 1: [('state', 0.574), ('neuron', 0.212), ('action', 0.187), ('policy', 0.149), ('control', 0.12), ('dynamic', 0.1), ('cell', 0.083), ('reinforcement', 0.081), ('optimal', 0.075), ('reinforcement learning', 0.068)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -0.364), ('feature', -0.223), ('object', -0.144), ('recognition', -0.143), ('classifier', -0.111), ('class', -0.106), ('layer', -0.092), ('classification', -0.085), ('face', -0.073), ('test', -0.069)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #4:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.425), ('state', 0.326), ('object', 0.215), ('feature', 0.159), ('action', 0.147), ('visual', 0.143), ('control', 0.126), ('task', 0.111), ('policy', 0.103), ('recognition', 0.103), ('face', 0.092), ('representation', 0.086), ('motion', 0.086)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('neuron', -0.216), ('distribution', -0.166), ('class', -0.112), ('bound', -0.109), ('probability', -0.108), ('spike', -0.104), ('variable', -0.087)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #5:\n",
            "==================================================\n",
            "Direction 1: [('layer', 0.261), ('net', 0.225), ('hidden', 0.222), ('neuron', 0.216), ('word', 0.206), ('recognition', 0.17), ('speech', 0.152), ('hidden unit', 0.11), ('architecture', 0.102), ('task', 0.094), ('activation', 0.092), ('memory', 0.091)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('cell', -0.227), ('distribution', -0.222), ('image', -0.175), ('gaussian', -0.125), ('variable', -0.112), ('density', -0.108), ('probability', -0.099), ('approximation', -0.091)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #6:\n",
            "==================================================\n",
            "Direction 1: [('cell', 0.548), ('layer', 0.139), ('word', 0.124), ('hidden', 0.111), ('classifier', 0.097), ('direction', 0.09), ('head', 0.078), ('rule', 0.073), ('rat', 0.073), ('speech', 0.071)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('neuron', -0.416), ('image', -0.336), ('circuit', -0.126), ('noise', -0.124), ('chip', -0.121), ('analog', -0.099), ('object', -0.09), ('spike', -0.075), ('signal', -0.071), ('voltage', -0.069)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #7:\n",
            "==================================================\n",
            "Direction 1: [('word', 0.294), ('recognition', 0.252), ('speech', 0.213), ('probability', 0.194), ('classifier', 0.181), ('spike', 0.179), ('state', 0.162), ('class', 0.14), ('neuron', 0.136), ('rate', 0.123), ('hmm', 0.119), ('feature', 0.112), ('classification', 0.097), ('speaker', 0.093), ('cell', 0.091)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('hidden', -0.207), ('layer', -0.179), ('hidden unit', -0.16), ('net', -0.136), ('field', -0.117)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #8:\n",
            "==================================================\n",
            "Direction 1: [('signal', 0.278), ('noise', 0.208), ('speech', 0.197), ('word', 0.165), ('hidden', 0.123), ('control', 0.117), ('motion', 0.116), ('filter', 0.108), ('frequency', 0.102)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('classifier', -0.225), ('node', -0.21), ('class', -0.197), ('feature', -0.186), ('neuron', -0.177), ('tree', -0.162), ('cell', -0.133), ('image', -0.119), ('rule', -0.115), ('object', -0.106), ('decision', -0.103)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #9:\n",
            "==================================================\n",
            "Direction 1: [('circuit', 0.244), ('control', 0.242), ('classifier', 0.229), ('chip', 0.167), ('node', 0.137), ('current', 0.132), ('analog', 0.13), ('voltage', 0.129), ('signal', 0.118), ('controller', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('hidden', -0.27), ('neuron', -0.247), ('state', -0.175), ('distribution', -0.158), ('hidden unit', -0.143), ('layer', -0.125), ('object', -0.115), ('probability', -0.108), ('image', -0.1), ('representation', -0.098)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #10:\n",
            "==================================================\n",
            "Direction 1: [('circuit', 0.245), ('cell', 0.225), ('node', 0.211), ('state', 0.183), ('image', 0.166), ('chip', 0.163), ('analog', 0.147), ('layer', 0.144), ('net', 0.12), ('voltage', 0.115)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('task', -0.201), ('rule', -0.193), ('spike', -0.166), ('feature', -0.165), ('control', -0.157), ('neuron', -0.144), ('rate', -0.134), ('stimulus', -0.116), ('classifier', -0.116), ('action', -0.112)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #11:\n",
            "==================================================\n",
            "Direction 1: [('image', 0.315), ('cell', 0.225), ('hidden', 0.205), ('spike', 0.192), ('noise', 0.163), ('rate', 0.141), ('hidden unit', 0.141), ('rule', 0.138), ('signal', 0.119), ('net', 0.111)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('field', -0.203), ('object', -0.2), ('word', -0.184), ('node', -0.161), ('motion', -0.136), ('visual', -0.134), ('neuron', -0.128), ('structure', -0.121), ('tree', -0.119), ('map', -0.107)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #12:\n",
            "==================================================\n",
            "Direction 1: [('rule', 0.581), ('representation', 0.156), ('word', 0.146), ('memory', 0.137), ('structure', 0.125), ('matrix', 0.108), ('cell', 0.086)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('classifier', -0.293), ('layer', -0.17), ('hidden', -0.16), ('motion', -0.129), ('neuron', -0.129), ('field', -0.12), ('class', -0.109), ('visual', -0.101), ('net', -0.092), ('state', -0.085), ('region', -0.084), ('hidden unit', -0.076), ('stimulus', -0.076)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #13:\n",
            "==================================================\n",
            "Direction 1: [('node', 0.396), ('tree', 0.262), ('spike', 0.226), ('stimulus', 0.208), ('signal', 0.169), ('representation', 0.147), ('motion', 0.142), ('response', 0.138), ('frequency', 0.109), ('visual', 0.1), ('rate', 0.097)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('cell', -0.231), ('feature', -0.157), ('neuron', -0.147), ('control', -0.13), ('matrix', -0.119), ('word', -0.114), ('recognition', -0.113), ('distance', -0.104), ('equation', -0.098)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #14:\n",
            "==================================================\n",
            "Direction 1: [('feature', 0.506), ('noise', 0.196), ('map', 0.171), ('signal', 0.133), ('classifier', 0.129), ('state', 0.124), ('memory', 0.122), ('orientation', 0.109), ('component', 0.103)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('image', -0.254), ('control', -0.187), ('word', -0.162), ('recognition', -0.132), ('neuron', -0.131), ('object', -0.113), ('rate', -0.105), ('character', -0.099), ('probability', -0.096), ('bound', -0.089), ('rule', -0.086)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #15:\n",
            "==================================================\n",
            "Direction 1: [('rule', 0.365), ('classifier', 0.365), ('mixture', 0.171), ('node', 0.156), ('gaussian', 0.148), ('layer', 0.128), ('neuron', 0.114), ('field', 0.109), ('control', 0.108), ('image', 0.104), ('component', 0.098)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('bound', -0.189), ('word', -0.156), ('feature', -0.136), ('threshold', -0.135), ('object', -0.125), ('representation', -0.118), ('size', -0.117), ('task', -0.098), ('theorem', -0.097)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #16:\n",
            "==================================================\n",
            "Direction 1: [('object', 0.291), ('control', 0.206), ('mixture', 0.178), ('feature', 0.158), ('task', 0.132), ('cell', 0.13), ('variable', 0.125), ('expert', 0.117), ('current', 0.117), ('circuit', 0.115), ('tree', 0.101), ('distribution', 0.098)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('word', -0.21), ('field', -0.172), ('rule', -0.138), ('rate', -0.121), ('motion', -0.116), ('character', -0.108), ('orientation', -0.107), ('image', -0.104)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #17:\n",
            "==================================================\n",
            "Direction 1: [('rule', 0.372), ('motion', 0.325), ('circuit', 0.193), ('direction', 0.175), ('neuron', 0.153), ('chip', 0.127), ('task', 0.123), ('visual', 0.113), ('velocity', 0.092), ('action', 0.091)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('memory', -0.231), ('node', -0.215), ('control', -0.182), ('dynamic', -0.148), ('spike', -0.128), ('rate', -0.116), ('matrix', -0.108), ('noise', -0.103), ('fig', -0.097), ('cell', -0.093)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #18:\n",
            "==================================================\n",
            "Direction 1: [('object', 0.419), ('signal', 0.26), ('layer', 0.258), ('rule', 0.209), ('feature', 0.164), ('view', 0.162), ('net', 0.113), ('noise', 0.112), ('bound', 0.105), ('speech', 0.1)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('memory', -0.18), ('task', -0.161), ('representation', -0.14), ('hidden', -0.137), ('image', -0.135), ('hidden unit', -0.121), ('tree', -0.117), ('structure', -0.094), ('test', -0.093), ('word', -0.092)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #19:\n",
            "==================================================\n",
            "Direction 1: [('class', 0.287), ('memory', 0.275), ('classifier', 0.144), ('response', 0.139), ('sequence', 0.112), ('component', 0.11), ('stimulus', 0.101), ('region', 0.092), ('bound', 0.088)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('node', -0.292), ('feature', -0.244), ('field', -0.202), ('rate', -0.152), ('word', -0.146), ('spike', -0.139), ('map', -0.132), ('character', -0.127), ('policy', -0.108), ('tree', -0.092), ('noise', -0.088)]\n",
            "--------------------------------------------------\n",
            "\n",
            "Topic #20:\n",
            "==================================================\n",
            "Direction 1: [('map', 0.222), ('control', 0.2), ('region', 0.181), ('ii', 0.145), ('feature', 0.132), ('image', 0.122), ('bound', 0.11), ('orientation', 0.109), ('rule', 0.109), ('threshold', 0.094), ('class', 0.092)]\n",
            "--------------------------------------------------\n",
            "Direction 2: [('object', -0.31), ('motion', -0.252), ('direction', -0.229), ('memory', -0.223), ('classifier', -0.193), ('view', -0.136), ('matrix', -0.13), ('rate', -0.121), ('distance', -0.11)]\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tevdjJruQBVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c88876e0-b033-40a3-e54e-26601638e127"
      },
      "source": [
        "document_topics.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1740, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK3NBfUUFrzM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "03c3abee-5317-4688-bf08-4e9b63c215f9"
      },
      "source": [
        "dt_df = pd.DataFrame(np.round(document_topics, 3), \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.T"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1700</th>\n",
              "      <th>1701</th>\n",
              "      <th>1702</th>\n",
              "      <th>1703</th>\n",
              "      <th>1704</th>\n",
              "      <th>1705</th>\n",
              "      <th>1706</th>\n",
              "      <th>1707</th>\n",
              "      <th>1708</th>\n",
              "      <th>1709</th>\n",
              "      <th>1710</th>\n",
              "      <th>1711</th>\n",
              "      <th>1712</th>\n",
              "      <th>1713</th>\n",
              "      <th>1714</th>\n",
              "      <th>1715</th>\n",
              "      <th>1716</th>\n",
              "      <th>1717</th>\n",
              "      <th>1718</th>\n",
              "      <th>1719</th>\n",
              "      <th>1720</th>\n",
              "      <th>1721</th>\n",
              "      <th>1722</th>\n",
              "      <th>1723</th>\n",
              "      <th>1724</th>\n",
              "      <th>1725</th>\n",
              "      <th>1726</th>\n",
              "      <th>1727</th>\n",
              "      <th>1728</th>\n",
              "      <th>1729</th>\n",
              "      <th>1730</th>\n",
              "      <th>1731</th>\n",
              "      <th>1732</th>\n",
              "      <th>1733</th>\n",
              "      <th>1734</th>\n",
              "      <th>1735</th>\n",
              "      <th>1736</th>\n",
              "      <th>1737</th>\n",
              "      <th>1738</th>\n",
              "      <th>1739</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1</th>\n",
              "      <td>49.207</td>\n",
              "      <td>19.875</td>\n",
              "      <td>32.480</td>\n",
              "      <td>41.207</td>\n",
              "      <td>39.148</td>\n",
              "      <td>14.773</td>\n",
              "      <td>17.331</td>\n",
              "      <td>39.717</td>\n",
              "      <td>38.008</td>\n",
              "      <td>30.219</td>\n",
              "      <td>22.803</td>\n",
              "      <td>40.100</td>\n",
              "      <td>28.272</td>\n",
              "      <td>44.592</td>\n",
              "      <td>28.231</td>\n",
              "      <td>62.958</td>\n",
              "      <td>56.191</td>\n",
              "      <td>20.943</td>\n",
              "      <td>22.532</td>\n",
              "      <td>38.178</td>\n",
              "      <td>46.168</td>\n",
              "      <td>26.435</td>\n",
              "      <td>25.845</td>\n",
              "      <td>30.974</td>\n",
              "      <td>24.209</td>\n",
              "      <td>16.437</td>\n",
              "      <td>51.298</td>\n",
              "      <td>21.560</td>\n",
              "      <td>57.723</td>\n",
              "      <td>39.235</td>\n",
              "      <td>42.714</td>\n",
              "      <td>29.331</td>\n",
              "      <td>33.197</td>\n",
              "      <td>32.520</td>\n",
              "      <td>25.488</td>\n",
              "      <td>37.202</td>\n",
              "      <td>21.368</td>\n",
              "      <td>44.822</td>\n",
              "      <td>26.408</td>\n",
              "      <td>46.477</td>\n",
              "      <td>...</td>\n",
              "      <td>34.768</td>\n",
              "      <td>28.628</td>\n",
              "      <td>40.438</td>\n",
              "      <td>26.530</td>\n",
              "      <td>34.684</td>\n",
              "      <td>26.262</td>\n",
              "      <td>27.079</td>\n",
              "      <td>48.741</td>\n",
              "      <td>24.224</td>\n",
              "      <td>22.219</td>\n",
              "      <td>25.199</td>\n",
              "      <td>34.131</td>\n",
              "      <td>25.437</td>\n",
              "      <td>25.679</td>\n",
              "      <td>26.435</td>\n",
              "      <td>25.253</td>\n",
              "      <td>31.160</td>\n",
              "      <td>28.026</td>\n",
              "      <td>27.053</td>\n",
              "      <td>23.603</td>\n",
              "      <td>37.321</td>\n",
              "      <td>26.683</td>\n",
              "      <td>37.965</td>\n",
              "      <td>29.300</td>\n",
              "      <td>36.184</td>\n",
              "      <td>35.729</td>\n",
              "      <td>34.049</td>\n",
              "      <td>25.148</td>\n",
              "      <td>28.648</td>\n",
              "      <td>29.186</td>\n",
              "      <td>24.986</td>\n",
              "      <td>32.216</td>\n",
              "      <td>37.454</td>\n",
              "      <td>42.738</td>\n",
              "      <td>48.313</td>\n",
              "      <td>39.854</td>\n",
              "      <td>46.058</td>\n",
              "      <td>25.953</td>\n",
              "      <td>43.850</td>\n",
              "      <td>26.103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>5.732</td>\n",
              "      <td>6.621</td>\n",
              "      <td>-3.491</td>\n",
              "      <td>28.411</td>\n",
              "      <td>-7.606</td>\n",
              "      <td>1.322</td>\n",
              "      <td>-6.047</td>\n",
              "      <td>15.803</td>\n",
              "      <td>0.648</td>\n",
              "      <td>30.913</td>\n",
              "      <td>-1.964</td>\n",
              "      <td>16.541</td>\n",
              "      <td>24.387</td>\n",
              "      <td>9.328</td>\n",
              "      <td>-0.721</td>\n",
              "      <td>-14.629</td>\n",
              "      <td>-19.533</td>\n",
              "      <td>-5.107</td>\n",
              "      <td>3.671</td>\n",
              "      <td>12.680</td>\n",
              "      <td>-5.992</td>\n",
              "      <td>25.820</td>\n",
              "      <td>12.870</td>\n",
              "      <td>1.370</td>\n",
              "      <td>16.700</td>\n",
              "      <td>2.358</td>\n",
              "      <td>5.869</td>\n",
              "      <td>6.716</td>\n",
              "      <td>-0.892</td>\n",
              "      <td>-10.072</td>\n",
              "      <td>17.770</td>\n",
              "      <td>-2.075</td>\n",
              "      <td>8.359</td>\n",
              "      <td>30.041</td>\n",
              "      <td>-5.832</td>\n",
              "      <td>-2.052</td>\n",
              "      <td>2.552</td>\n",
              "      <td>6.649</td>\n",
              "      <td>-3.343</td>\n",
              "      <td>-8.457</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.836</td>\n",
              "      <td>-10.822</td>\n",
              "      <td>-21.470</td>\n",
              "      <td>-13.144</td>\n",
              "      <td>1.289</td>\n",
              "      <td>-6.212</td>\n",
              "      <td>-6.517</td>\n",
              "      <td>-2.790</td>\n",
              "      <td>7.120</td>\n",
              "      <td>-5.562</td>\n",
              "      <td>-11.998</td>\n",
              "      <td>-11.416</td>\n",
              "      <td>-13.793</td>\n",
              "      <td>-7.744</td>\n",
              "      <td>-16.619</td>\n",
              "      <td>-14.658</td>\n",
              "      <td>-17.637</td>\n",
              "      <td>-10.626</td>\n",
              "      <td>-13.162</td>\n",
              "      <td>-9.169</td>\n",
              "      <td>39.284</td>\n",
              "      <td>3.296</td>\n",
              "      <td>-14.914</td>\n",
              "      <td>-16.829</td>\n",
              "      <td>-21.410</td>\n",
              "      <td>-4.670</td>\n",
              "      <td>-8.576</td>\n",
              "      <td>-4.536</td>\n",
              "      <td>-12.314</td>\n",
              "      <td>-9.614</td>\n",
              "      <td>16.211</td>\n",
              "      <td>-13.841</td>\n",
              "      <td>38.621</td>\n",
              "      <td>41.314</td>\n",
              "      <td>36.324</td>\n",
              "      <td>-25.001</td>\n",
              "      <td>-11.195</td>\n",
              "      <td>1.214</td>\n",
              "      <td>-31.794</td>\n",
              "      <td>-6.085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>42.819</td>\n",
              "      <td>6.901</td>\n",
              "      <td>-12.635</td>\n",
              "      <td>7.661</td>\n",
              "      <td>-13.323</td>\n",
              "      <td>5.769</td>\n",
              "      <td>-5.044</td>\n",
              "      <td>4.083</td>\n",
              "      <td>0.832</td>\n",
              "      <td>4.965</td>\n",
              "      <td>3.363</td>\n",
              "      <td>15.912</td>\n",
              "      <td>11.841</td>\n",
              "      <td>10.170</td>\n",
              "      <td>16.053</td>\n",
              "      <td>-37.653</td>\n",
              "      <td>14.272</td>\n",
              "      <td>-4.495</td>\n",
              "      <td>2.896</td>\n",
              "      <td>-14.701</td>\n",
              "      <td>-16.382</td>\n",
              "      <td>5.257</td>\n",
              "      <td>15.013</td>\n",
              "      <td>-15.393</td>\n",
              "      <td>11.138</td>\n",
              "      <td>5.168</td>\n",
              "      <td>24.591</td>\n",
              "      <td>7.300</td>\n",
              "      <td>-5.349</td>\n",
              "      <td>18.533</td>\n",
              "      <td>8.193</td>\n",
              "      <td>0.064</td>\n",
              "      <td>-6.200</td>\n",
              "      <td>15.089</td>\n",
              "      <td>3.284</td>\n",
              "      <td>-2.749</td>\n",
              "      <td>6.702</td>\n",
              "      <td>18.312</td>\n",
              "      <td>-10.793</td>\n",
              "      <td>12.215</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.838</td>\n",
              "      <td>3.483</td>\n",
              "      <td>33.258</td>\n",
              "      <td>-8.551</td>\n",
              "      <td>-9.955</td>\n",
              "      <td>-18.248</td>\n",
              "      <td>-9.752</td>\n",
              "      <td>6.439</td>\n",
              "      <td>-7.312</td>\n",
              "      <td>-5.486</td>\n",
              "      <td>-2.793</td>\n",
              "      <td>-18.425</td>\n",
              "      <td>-9.863</td>\n",
              "      <td>16.584</td>\n",
              "      <td>-4.860</td>\n",
              "      <td>-6.139</td>\n",
              "      <td>17.812</td>\n",
              "      <td>-8.474</td>\n",
              "      <td>5.110</td>\n",
              "      <td>0.062</td>\n",
              "      <td>22.271</td>\n",
              "      <td>20.439</td>\n",
              "      <td>-11.746</td>\n",
              "      <td>-13.059</td>\n",
              "      <td>34.469</td>\n",
              "      <td>-24.388</td>\n",
              "      <td>-23.708</td>\n",
              "      <td>-0.678</td>\n",
              "      <td>-8.209</td>\n",
              "      <td>-18.183</td>\n",
              "      <td>20.085</td>\n",
              "      <td>-14.064</td>\n",
              "      <td>13.244</td>\n",
              "      <td>-5.136</td>\n",
              "      <td>24.935</td>\n",
              "      <td>-18.122</td>\n",
              "      <td>-45.472</td>\n",
              "      <td>-3.542</td>\n",
              "      <td>28.409</td>\n",
              "      <td>4.660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>5.160</td>\n",
              "      <td>-12.437</td>\n",
              "      <td>8.834</td>\n",
              "      <td>-28.256</td>\n",
              "      <td>-16.420</td>\n",
              "      <td>-4.901</td>\n",
              "      <td>-6.236</td>\n",
              "      <td>-13.368</td>\n",
              "      <td>-1.920</td>\n",
              "      <td>-2.253</td>\n",
              "      <td>-8.412</td>\n",
              "      <td>-11.555</td>\n",
              "      <td>-1.929</td>\n",
              "      <td>-0.979</td>\n",
              "      <td>15.456</td>\n",
              "      <td>-17.904</td>\n",
              "      <td>0.086</td>\n",
              "      <td>-9.058</td>\n",
              "      <td>-3.360</td>\n",
              "      <td>25.229</td>\n",
              "      <td>5.795</td>\n",
              "      <td>-4.165</td>\n",
              "      <td>-10.029</td>\n",
              "      <td>12.518</td>\n",
              "      <td>-6.440</td>\n",
              "      <td>-5.587</td>\n",
              "      <td>-13.522</td>\n",
              "      <td>1.774</td>\n",
              "      <td>-20.905</td>\n",
              "      <td>-5.095</td>\n",
              "      <td>-9.513</td>\n",
              "      <td>-7.140</td>\n",
              "      <td>0.360</td>\n",
              "      <td>12.833</td>\n",
              "      <td>-6.972</td>\n",
              "      <td>-2.120</td>\n",
              "      <td>-3.574</td>\n",
              "      <td>-13.303</td>\n",
              "      <td>-2.240</td>\n",
              "      <td>-1.709</td>\n",
              "      <td>...</td>\n",
              "      <td>-15.593</td>\n",
              "      <td>-8.200</td>\n",
              "      <td>22.846</td>\n",
              "      <td>-9.464</td>\n",
              "      <td>5.659</td>\n",
              "      <td>1.509</td>\n",
              "      <td>-3.635</td>\n",
              "      <td>-9.064</td>\n",
              "      <td>5.330</td>\n",
              "      <td>-2.055</td>\n",
              "      <td>-12.988</td>\n",
              "      <td>-5.781</td>\n",
              "      <td>-14.134</td>\n",
              "      <td>8.284</td>\n",
              "      <td>-20.882</td>\n",
              "      <td>-21.136</td>\n",
              "      <td>-4.458</td>\n",
              "      <td>-12.279</td>\n",
              "      <td>-8.108</td>\n",
              "      <td>-12.926</td>\n",
              "      <td>-19.320</td>\n",
              "      <td>-5.588</td>\n",
              "      <td>-16.644</td>\n",
              "      <td>-16.424</td>\n",
              "      <td>16.593</td>\n",
              "      <td>7.786</td>\n",
              "      <td>2.464</td>\n",
              "      <td>-9.012</td>\n",
              "      <td>-16.533</td>\n",
              "      <td>-1.926</td>\n",
              "      <td>-5.853</td>\n",
              "      <td>-13.535</td>\n",
              "      <td>-20.935</td>\n",
              "      <td>11.011</td>\n",
              "      <td>-3.209</td>\n",
              "      <td>-27.714</td>\n",
              "      <td>23.541</td>\n",
              "      <td>-5.380</td>\n",
              "      <td>2.059</td>\n",
              "      <td>-8.330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T5</th>\n",
              "      <td>22.560</td>\n",
              "      <td>0.486</td>\n",
              "      <td>-2.550</td>\n",
              "      <td>7.780</td>\n",
              "      <td>8.055</td>\n",
              "      <td>0.574</td>\n",
              "      <td>7.831</td>\n",
              "      <td>7.242</td>\n",
              "      <td>13.015</td>\n",
              "      <td>4.269</td>\n",
              "      <td>-6.386</td>\n",
              "      <td>14.478</td>\n",
              "      <td>-13.875</td>\n",
              "      <td>-7.441</td>\n",
              "      <td>-4.269</td>\n",
              "      <td>49.664</td>\n",
              "      <td>50.193</td>\n",
              "      <td>7.615</td>\n",
              "      <td>-3.325</td>\n",
              "      <td>-2.389</td>\n",
              "      <td>15.193</td>\n",
              "      <td>-7.910</td>\n",
              "      <td>6.769</td>\n",
              "      <td>-12.691</td>\n",
              "      <td>11.726</td>\n",
              "      <td>-1.485</td>\n",
              "      <td>25.487</td>\n",
              "      <td>7.323</td>\n",
              "      <td>40.715</td>\n",
              "      <td>-0.443</td>\n",
              "      <td>22.028</td>\n",
              "      <td>4.695</td>\n",
              "      <td>17.794</td>\n",
              "      <td>-8.494</td>\n",
              "      <td>6.618</td>\n",
              "      <td>15.400</td>\n",
              "      <td>1.330</td>\n",
              "      <td>8.986</td>\n",
              "      <td>22.351</td>\n",
              "      <td>-10.586</td>\n",
              "      <td>...</td>\n",
              "      <td>-15.640</td>\n",
              "      <td>-12.866</td>\n",
              "      <td>-15.625</td>\n",
              "      <td>-5.500</td>\n",
              "      <td>5.527</td>\n",
              "      <td>-0.972</td>\n",
              "      <td>-14.202</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-4.470</td>\n",
              "      <td>-1.934</td>\n",
              "      <td>-7.743</td>\n",
              "      <td>-12.738</td>\n",
              "      <td>-12.728</td>\n",
              "      <td>-1.335</td>\n",
              "      <td>-24.384</td>\n",
              "      <td>-20.838</td>\n",
              "      <td>-8.551</td>\n",
              "      <td>-19.305</td>\n",
              "      <td>-15.171</td>\n",
              "      <td>-10.492</td>\n",
              "      <td>7.840</td>\n",
              "      <td>0.768</td>\n",
              "      <td>-8.073</td>\n",
              "      <td>-16.380</td>\n",
              "      <td>-1.060</td>\n",
              "      <td>-7.153</td>\n",
              "      <td>-28.593</td>\n",
              "      <td>-6.433</td>\n",
              "      <td>-7.078</td>\n",
              "      <td>-7.298</td>\n",
              "      <td>9.197</td>\n",
              "      <td>-11.809</td>\n",
              "      <td>11.182</td>\n",
              "      <td>-20.949</td>\n",
              "      <td>11.896</td>\n",
              "      <td>2.009</td>\n",
              "      <td>-20.470</td>\n",
              "      <td>-7.368</td>\n",
              "      <td>-24.891</td>\n",
              "      <td>-4.438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T6</th>\n",
              "      <td>-25.837</td>\n",
              "      <td>-12.345</td>\n",
              "      <td>-5.989</td>\n",
              "      <td>-24.211</td>\n",
              "      <td>-2.699</td>\n",
              "      <td>-10.863</td>\n",
              "      <td>4.683</td>\n",
              "      <td>-20.493</td>\n",
              "      <td>-5.612</td>\n",
              "      <td>27.100</td>\n",
              "      <td>-7.550</td>\n",
              "      <td>-28.463</td>\n",
              "      <td>28.047</td>\n",
              "      <td>20.870</td>\n",
              "      <td>3.513</td>\n",
              "      <td>35.128</td>\n",
              "      <td>12.312</td>\n",
              "      <td>3.771</td>\n",
              "      <td>-0.050</td>\n",
              "      <td>-35.468</td>\n",
              "      <td>8.457</td>\n",
              "      <td>18.177</td>\n",
              "      <td>-20.484</td>\n",
              "      <td>-13.965</td>\n",
              "      <td>-14.895</td>\n",
              "      <td>-5.476</td>\n",
              "      <td>-22.503</td>\n",
              "      <td>7.397</td>\n",
              "      <td>0.160</td>\n",
              "      <td>-12.373</td>\n",
              "      <td>-16.731</td>\n",
              "      <td>-8.585</td>\n",
              "      <td>-14.401</td>\n",
              "      <td>25.439</td>\n",
              "      <td>1.415</td>\n",
              "      <td>4.820</td>\n",
              "      <td>-8.309</td>\n",
              "      <td>-20.833</td>\n",
              "      <td>6.130</td>\n",
              "      <td>-8.040</td>\n",
              "      <td>...</td>\n",
              "      <td>4.404</td>\n",
              "      <td>-3.721</td>\n",
              "      <td>-0.151</td>\n",
              "      <td>3.976</td>\n",
              "      <td>8.203</td>\n",
              "      <td>1.392</td>\n",
              "      <td>-7.294</td>\n",
              "      <td>-8.859</td>\n",
              "      <td>-3.545</td>\n",
              "      <td>-2.638</td>\n",
              "      <td>0.812</td>\n",
              "      <td>-4.954</td>\n",
              "      <td>0.285</td>\n",
              "      <td>-2.257</td>\n",
              "      <td>-2.801</td>\n",
              "      <td>-0.415</td>\n",
              "      <td>-5.297</td>\n",
              "      <td>-2.089</td>\n",
              "      <td>-2.584</td>\n",
              "      <td>-4.244</td>\n",
              "      <td>-18.001</td>\n",
              "      <td>-15.709</td>\n",
              "      <td>9.262</td>\n",
              "      <td>3.473</td>\n",
              "      <td>6.211</td>\n",
              "      <td>5.102</td>\n",
              "      <td>-15.132</td>\n",
              "      <td>-4.033</td>\n",
              "      <td>1.994</td>\n",
              "      <td>0.723</td>\n",
              "      <td>-21.229</td>\n",
              "      <td>-1.106</td>\n",
              "      <td>-12.885</td>\n",
              "      <td>16.011</td>\n",
              "      <td>-13.509</td>\n",
              "      <td>13.687</td>\n",
              "      <td>-19.396</td>\n",
              "      <td>0.502</td>\n",
              "      <td>-0.492</td>\n",
              "      <td>-1.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T7</th>\n",
              "      <td>5.353</td>\n",
              "      <td>2.338</td>\n",
              "      <td>1.101</td>\n",
              "      <td>32.674</td>\n",
              "      <td>7.759</td>\n",
              "      <td>-2.904</td>\n",
              "      <td>-6.652</td>\n",
              "      <td>6.068</td>\n",
              "      <td>-18.856</td>\n",
              "      <td>-4.608</td>\n",
              "      <td>-6.982</td>\n",
              "      <td>-7.744</td>\n",
              "      <td>7.428</td>\n",
              "      <td>-1.963</td>\n",
              "      <td>1.173</td>\n",
              "      <td>-6.262</td>\n",
              "      <td>-28.829</td>\n",
              "      <td>-8.912</td>\n",
              "      <td>-4.967</td>\n",
              "      <td>2.072</td>\n",
              "      <td>-15.607</td>\n",
              "      <td>6.338</td>\n",
              "      <td>-0.847</td>\n",
              "      <td>-5.678</td>\n",
              "      <td>-1.289</td>\n",
              "      <td>-1.637</td>\n",
              "      <td>-4.643</td>\n",
              "      <td>0.041</td>\n",
              "      <td>-33.546</td>\n",
              "      <td>-14.848</td>\n",
              "      <td>-13.660</td>\n",
              "      <td>-5.385</td>\n",
              "      <td>0.655</td>\n",
              "      <td>-1.665</td>\n",
              "      <td>-13.522</td>\n",
              "      <td>5.655</td>\n",
              "      <td>-4.756</td>\n",
              "      <td>2.279</td>\n",
              "      <td>11.448</td>\n",
              "      <td>-12.092</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.088</td>\n",
              "      <td>2.631</td>\n",
              "      <td>0.971</td>\n",
              "      <td>12.962</td>\n",
              "      <td>-12.879</td>\n",
              "      <td>2.477</td>\n",
              "      <td>0.048</td>\n",
              "      <td>-2.994</td>\n",
              "      <td>3.806</td>\n",
              "      <td>1.308</td>\n",
              "      <td>-2.004</td>\n",
              "      <td>4.965</td>\n",
              "      <td>6.087</td>\n",
              "      <td>-6.449</td>\n",
              "      <td>4.637</td>\n",
              "      <td>-1.775</td>\n",
              "      <td>8.229</td>\n",
              "      <td>-3.489</td>\n",
              "      <td>1.311</td>\n",
              "      <td>-8.116</td>\n",
              "      <td>10.088</td>\n",
              "      <td>8.288</td>\n",
              "      <td>-0.301</td>\n",
              "      <td>15.720</td>\n",
              "      <td>2.442</td>\n",
              "      <td>20.456</td>\n",
              "      <td>7.893</td>\n",
              "      <td>-3.427</td>\n",
              "      <td>0.013</td>\n",
              "      <td>12.992</td>\n",
              "      <td>5.444</td>\n",
              "      <td>7.125</td>\n",
              "      <td>1.488</td>\n",
              "      <td>-0.636</td>\n",
              "      <td>10.004</td>\n",
              "      <td>9.551</td>\n",
              "      <td>15.697</td>\n",
              "      <td>0.390</td>\n",
              "      <td>6.574</td>\n",
              "      <td>-9.650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T8</th>\n",
              "      <td>-11.635</td>\n",
              "      <td>-9.726</td>\n",
              "      <td>-5.436</td>\n",
              "      <td>-6.190</td>\n",
              "      <td>-22.946</td>\n",
              "      <td>-0.080</td>\n",
              "      <td>-8.796</td>\n",
              "      <td>8.278</td>\n",
              "      <td>-7.554</td>\n",
              "      <td>-8.910</td>\n",
              "      <td>-4.243</td>\n",
              "      <td>-7.879</td>\n",
              "      <td>-9.524</td>\n",
              "      <td>-19.290</td>\n",
              "      <td>1.066</td>\n",
              "      <td>-53.335</td>\n",
              "      <td>2.247</td>\n",
              "      <td>-3.682</td>\n",
              "      <td>2.300</td>\n",
              "      <td>-6.602</td>\n",
              "      <td>0.191</td>\n",
              "      <td>-2.577</td>\n",
              "      <td>-8.675</td>\n",
              "      <td>3.065</td>\n",
              "      <td>-6.677</td>\n",
              "      <td>0.022</td>\n",
              "      <td>-10.776</td>\n",
              "      <td>0.758</td>\n",
              "      <td>1.034</td>\n",
              "      <td>3.379</td>\n",
              "      <td>-18.427</td>\n",
              "      <td>-1.433</td>\n",
              "      <td>-10.861</td>\n",
              "      <td>4.501</td>\n",
              "      <td>2.267</td>\n",
              "      <td>11.190</td>\n",
              "      <td>-4.402</td>\n",
              "      <td>-9.052</td>\n",
              "      <td>5.588</td>\n",
              "      <td>4.081</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.322</td>\n",
              "      <td>12.862</td>\n",
              "      <td>-1.173</td>\n",
              "      <td>-1.531</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-11.032</td>\n",
              "      <td>9.829</td>\n",
              "      <td>3.015</td>\n",
              "      <td>20.049</td>\n",
              "      <td>5.222</td>\n",
              "      <td>-8.105</td>\n",
              "      <td>-8.353</td>\n",
              "      <td>-2.722</td>\n",
              "      <td>8.033</td>\n",
              "      <td>4.473</td>\n",
              "      <td>8.488</td>\n",
              "      <td>-2.626</td>\n",
              "      <td>3.616</td>\n",
              "      <td>6.710</td>\n",
              "      <td>-1.275</td>\n",
              "      <td>-12.096</td>\n",
              "      <td>3.797</td>\n",
              "      <td>-13.161</td>\n",
              "      <td>-4.670</td>\n",
              "      <td>0.823</td>\n",
              "      <td>-14.250</td>\n",
              "      <td>5.101</td>\n",
              "      <td>10.635</td>\n",
              "      <td>-2.377</td>\n",
              "      <td>-7.423</td>\n",
              "      <td>-5.078</td>\n",
              "      <td>-7.722</td>\n",
              "      <td>-21.138</td>\n",
              "      <td>-17.673</td>\n",
              "      <td>2.019</td>\n",
              "      <td>-46.964</td>\n",
              "      <td>-16.684</td>\n",
              "      <td>0.343</td>\n",
              "      <td>-4.835</td>\n",
              "      <td>6.520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T9</th>\n",
              "      <td>3.665</td>\n",
              "      <td>-4.675</td>\n",
              "      <td>7.998</td>\n",
              "      <td>-6.820</td>\n",
              "      <td>9.314</td>\n",
              "      <td>0.540</td>\n",
              "      <td>1.673</td>\n",
              "      <td>12.494</td>\n",
              "      <td>13.340</td>\n",
              "      <td>-3.410</td>\n",
              "      <td>2.459</td>\n",
              "      <td>-3.752</td>\n",
              "      <td>-2.255</td>\n",
              "      <td>-2.187</td>\n",
              "      <td>-5.479</td>\n",
              "      <td>21.302</td>\n",
              "      <td>-18.139</td>\n",
              "      <td>-1.345</td>\n",
              "      <td>1.787</td>\n",
              "      <td>4.216</td>\n",
              "      <td>0.149</td>\n",
              "      <td>1.979</td>\n",
              "      <td>-7.986</td>\n",
              "      <td>2.538</td>\n",
              "      <td>-3.979</td>\n",
              "      <td>2.997</td>\n",
              "      <td>-24.903</td>\n",
              "      <td>-4.313</td>\n",
              "      <td>-18.733</td>\n",
              "      <td>-2.200</td>\n",
              "      <td>1.199</td>\n",
              "      <td>6.733</td>\n",
              "      <td>25.506</td>\n",
              "      <td>19.309</td>\n",
              "      <td>-3.886</td>\n",
              "      <td>12.227</td>\n",
              "      <td>1.095</td>\n",
              "      <td>-5.040</td>\n",
              "      <td>1.319</td>\n",
              "      <td>6.762</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.320</td>\n",
              "      <td>-4.695</td>\n",
              "      <td>0.036</td>\n",
              "      <td>1.861</td>\n",
              "      <td>-3.797</td>\n",
              "      <td>-1.993</td>\n",
              "      <td>-3.041</td>\n",
              "      <td>-36.797</td>\n",
              "      <td>5.211</td>\n",
              "      <td>2.863</td>\n",
              "      <td>12.749</td>\n",
              "      <td>4.534</td>\n",
              "      <td>3.244</td>\n",
              "      <td>15.654</td>\n",
              "      <td>-6.723</td>\n",
              "      <td>-6.717</td>\n",
              "      <td>-5.246</td>\n",
              "      <td>5.495</td>\n",
              "      <td>-7.811</td>\n",
              "      <td>6.190</td>\n",
              "      <td>-18.549</td>\n",
              "      <td>-8.670</td>\n",
              "      <td>-3.197</td>\n",
              "      <td>-3.732</td>\n",
              "      <td>-6.278</td>\n",
              "      <td>-2.541</td>\n",
              "      <td>-12.837</td>\n",
              "      <td>8.427</td>\n",
              "      <td>8.174</td>\n",
              "      <td>-4.812</td>\n",
              "      <td>2.686</td>\n",
              "      <td>8.063</td>\n",
              "      <td>-14.983</td>\n",
              "      <td>-5.274</td>\n",
              "      <td>-14.095</td>\n",
              "      <td>28.005</td>\n",
              "      <td>-20.269</td>\n",
              "      <td>5.660</td>\n",
              "      <td>-8.082</td>\n",
              "      <td>1.863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T10</th>\n",
              "      <td>20.994</td>\n",
              "      <td>-0.788</td>\n",
              "      <td>0.395</td>\n",
              "      <td>-28.475</td>\n",
              "      <td>-3.678</td>\n",
              "      <td>4.218</td>\n",
              "      <td>4.589</td>\n",
              "      <td>-4.172</td>\n",
              "      <td>8.100</td>\n",
              "      <td>12.502</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>1.614</td>\n",
              "      <td>8.033</td>\n",
              "      <td>27.037</td>\n",
              "      <td>-0.377</td>\n",
              "      <td>7.630</td>\n",
              "      <td>18.847</td>\n",
              "      <td>6.169</td>\n",
              "      <td>6.055</td>\n",
              "      <td>12.835</td>\n",
              "      <td>-14.281</td>\n",
              "      <td>3.387</td>\n",
              "      <td>-1.169</td>\n",
              "      <td>4.474</td>\n",
              "      <td>-4.839</td>\n",
              "      <td>-6.458</td>\n",
              "      <td>6.013</td>\n",
              "      <td>2.454</td>\n",
              "      <td>9.663</td>\n",
              "      <td>1.798</td>\n",
              "      <td>5.723</td>\n",
              "      <td>11.722</td>\n",
              "      <td>20.090</td>\n",
              "      <td>-0.569</td>\n",
              "      <td>-1.268</td>\n",
              "      <td>5.995</td>\n",
              "      <td>0.573</td>\n",
              "      <td>7.673</td>\n",
              "      <td>0.274</td>\n",
              "      <td>2.857</td>\n",
              "      <td>...</td>\n",
              "      <td>19.941</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.008</td>\n",
              "      <td>-2.462</td>\n",
              "      <td>-1.399</td>\n",
              "      <td>-15.318</td>\n",
              "      <td>2.555</td>\n",
              "      <td>6.862</td>\n",
              "      <td>-0.506</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>-7.511</td>\n",
              "      <td>-10.342</td>\n",
              "      <td>-7.186</td>\n",
              "      <td>-6.867</td>\n",
              "      <td>7.517</td>\n",
              "      <td>-5.568</td>\n",
              "      <td>12.569</td>\n",
              "      <td>-7.532</td>\n",
              "      <td>2.295</td>\n",
              "      <td>-5.251</td>\n",
              "      <td>-10.455</td>\n",
              "      <td>-1.788</td>\n",
              "      <td>16.048</td>\n",
              "      <td>-3.061</td>\n",
              "      <td>3.458</td>\n",
              "      <td>5.946</td>\n",
              "      <td>14.441</td>\n",
              "      <td>-2.919</td>\n",
              "      <td>-9.138</td>\n",
              "      <td>-11.090</td>\n",
              "      <td>5.685</td>\n",
              "      <td>-7.892</td>\n",
              "      <td>-5.628</td>\n",
              "      <td>7.271</td>\n",
              "      <td>-20.156</td>\n",
              "      <td>5.385</td>\n",
              "      <td>8.319</td>\n",
              "      <td>8.089</td>\n",
              "      <td>0.428</td>\n",
              "      <td>-6.606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T11</th>\n",
              "      <td>-3.092</td>\n",
              "      <td>-5.121</td>\n",
              "      <td>-1.234</td>\n",
              "      <td>18.031</td>\n",
              "      <td>-4.519</td>\n",
              "      <td>-2.921</td>\n",
              "      <td>-1.326</td>\n",
              "      <td>11.744</td>\n",
              "      <td>-11.559</td>\n",
              "      <td>10.019</td>\n",
              "      <td>-7.372</td>\n",
              "      <td>-12.862</td>\n",
              "      <td>11.698</td>\n",
              "      <td>-0.026</td>\n",
              "      <td>-10.233</td>\n",
              "      <td>5.430</td>\n",
              "      <td>21.438</td>\n",
              "      <td>-1.671</td>\n",
              "      <td>0.449</td>\n",
              "      <td>16.913</td>\n",
              "      <td>4.229</td>\n",
              "      <td>7.637</td>\n",
              "      <td>-7.111</td>\n",
              "      <td>9.864</td>\n",
              "      <td>-8.159</td>\n",
              "      <td>-0.386</td>\n",
              "      <td>3.772</td>\n",
              "      <td>0.144</td>\n",
              "      <td>16.688</td>\n",
              "      <td>-4.245</td>\n",
              "      <td>-18.611</td>\n",
              "      <td>-4.893</td>\n",
              "      <td>-4.937</td>\n",
              "      <td>-1.454</td>\n",
              "      <td>8.558</td>\n",
              "      <td>-3.275</td>\n",
              "      <td>-6.954</td>\n",
              "      <td>-13.316</td>\n",
              "      <td>-3.576</td>\n",
              "      <td>-7.474</td>\n",
              "      <td>...</td>\n",
              "      <td>-20.697</td>\n",
              "      <td>0.200</td>\n",
              "      <td>-7.470</td>\n",
              "      <td>-8.341</td>\n",
              "      <td>-4.654</td>\n",
              "      <td>0.127</td>\n",
              "      <td>2.344</td>\n",
              "      <td>2.367</td>\n",
              "      <td>-1.500</td>\n",
              "      <td>-5.705</td>\n",
              "      <td>3.663</td>\n",
              "      <td>1.207</td>\n",
              "      <td>0.462</td>\n",
              "      <td>-2.987</td>\n",
              "      <td>-8.890</td>\n",
              "      <td>1.227</td>\n",
              "      <td>-3.588</td>\n",
              "      <td>-0.462</td>\n",
              "      <td>-2.854</td>\n",
              "      <td>1.519</td>\n",
              "      <td>-4.787</td>\n",
              "      <td>1.494</td>\n",
              "      <td>-22.425</td>\n",
              "      <td>-5.390</td>\n",
              "      <td>6.075</td>\n",
              "      <td>1.749</td>\n",
              "      <td>6.596</td>\n",
              "      <td>6.378</td>\n",
              "      <td>4.132</td>\n",
              "      <td>-5.765</td>\n",
              "      <td>-4.136</td>\n",
              "      <td>-2.661</td>\n",
              "      <td>-0.068</td>\n",
              "      <td>4.525</td>\n",
              "      <td>-9.346</td>\n",
              "      <td>-10.643</td>\n",
              "      <td>11.858</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>2.691</td>\n",
              "      <td>3.622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T12</th>\n",
              "      <td>4.517</td>\n",
              "      <td>-1.583</td>\n",
              "      <td>8.024</td>\n",
              "      <td>-8.204</td>\n",
              "      <td>-5.983</td>\n",
              "      <td>3.939</td>\n",
              "      <td>-0.534</td>\n",
              "      <td>0.762</td>\n",
              "      <td>15.363</td>\n",
              "      <td>1.598</td>\n",
              "      <td>4.322</td>\n",
              "      <td>14.832</td>\n",
              "      <td>12.281</td>\n",
              "      <td>3.860</td>\n",
              "      <td>-6.761</td>\n",
              "      <td>-63.445</td>\n",
              "      <td>-5.920</td>\n",
              "      <td>-2.373</td>\n",
              "      <td>5.579</td>\n",
              "      <td>3.197</td>\n",
              "      <td>3.058</td>\n",
              "      <td>3.711</td>\n",
              "      <td>-2.007</td>\n",
              "      <td>0.930</td>\n",
              "      <td>-0.532</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>-15.688</td>\n",
              "      <td>2.893</td>\n",
              "      <td>-18.779</td>\n",
              "      <td>8.577</td>\n",
              "      <td>6.124</td>\n",
              "      <td>15.245</td>\n",
              "      <td>11.937</td>\n",
              "      <td>3.359</td>\n",
              "      <td>7.388</td>\n",
              "      <td>4.253</td>\n",
              "      <td>2.265</td>\n",
              "      <td>1.045</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>6.483</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.837</td>\n",
              "      <td>-0.179</td>\n",
              "      <td>-6.411</td>\n",
              "      <td>7.018</td>\n",
              "      <td>6.320</td>\n",
              "      <td>2.523</td>\n",
              "      <td>1.510</td>\n",
              "      <td>-8.950</td>\n",
              "      <td>-1.040</td>\n",
              "      <td>13.028</td>\n",
              "      <td>-5.771</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-7.489</td>\n",
              "      <td>-2.913</td>\n",
              "      <td>-1.731</td>\n",
              "      <td>-3.044</td>\n",
              "      <td>2.334</td>\n",
              "      <td>-1.673</td>\n",
              "      <td>-1.954</td>\n",
              "      <td>1.058</td>\n",
              "      <td>3.731</td>\n",
              "      <td>-3.351</td>\n",
              "      <td>8.143</td>\n",
              "      <td>-6.984</td>\n",
              "      <td>-7.917</td>\n",
              "      <td>2.844</td>\n",
              "      <td>1.553</td>\n",
              "      <td>6.342</td>\n",
              "      <td>-2.787</td>\n",
              "      <td>4.507</td>\n",
              "      <td>-2.339</td>\n",
              "      <td>-2.302</td>\n",
              "      <td>-2.118</td>\n",
              "      <td>-13.837</td>\n",
              "      <td>-8.885</td>\n",
              "      <td>-20.279</td>\n",
              "      <td>-1.885</td>\n",
              "      <td>6.153</td>\n",
              "      <td>-4.379</td>\n",
              "      <td>5.275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T13</th>\n",
              "      <td>-6.655</td>\n",
              "      <td>-2.442</td>\n",
              "      <td>4.533</td>\n",
              "      <td>14.893</td>\n",
              "      <td>-9.161</td>\n",
              "      <td>-9.885</td>\n",
              "      <td>11.293</td>\n",
              "      <td>5.102</td>\n",
              "      <td>1.440</td>\n",
              "      <td>-12.064</td>\n",
              "      <td>-8.435</td>\n",
              "      <td>-16.277</td>\n",
              "      <td>-4.884</td>\n",
              "      <td>9.025</td>\n",
              "      <td>5.199</td>\n",
              "      <td>24.315</td>\n",
              "      <td>1.957</td>\n",
              "      <td>6.468</td>\n",
              "      <td>-4.010</td>\n",
              "      <td>-1.004</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>-7.413</td>\n",
              "      <td>-12.869</td>\n",
              "      <td>-0.960</td>\n",
              "      <td>-5.257</td>\n",
              "      <td>-3.341</td>\n",
              "      <td>-14.408</td>\n",
              "      <td>-1.278</td>\n",
              "      <td>-8.442</td>\n",
              "      <td>-18.105</td>\n",
              "      <td>-10.872</td>\n",
              "      <td>0.047</td>\n",
              "      <td>-17.095</td>\n",
              "      <td>-14.176</td>\n",
              "      <td>-2.242</td>\n",
              "      <td>24.394</td>\n",
              "      <td>-7.175</td>\n",
              "      <td>-0.721</td>\n",
              "      <td>-2.485</td>\n",
              "      <td>-11.260</td>\n",
              "      <td>...</td>\n",
              "      <td>20.239</td>\n",
              "      <td>-0.538</td>\n",
              "      <td>4.821</td>\n",
              "      <td>-5.106</td>\n",
              "      <td>14.847</td>\n",
              "      <td>-9.501</td>\n",
              "      <td>-0.575</td>\n",
              "      <td>5.894</td>\n",
              "      <td>13.627</td>\n",
              "      <td>-5.253</td>\n",
              "      <td>-3.428</td>\n",
              "      <td>-18.467</td>\n",
              "      <td>-5.858</td>\n",
              "      <td>-12.596</td>\n",
              "      <td>-1.164</td>\n",
              "      <td>-3.222</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-14.245</td>\n",
              "      <td>-4.760</td>\n",
              "      <td>-5.472</td>\n",
              "      <td>-12.652</td>\n",
              "      <td>0.984</td>\n",
              "      <td>32.949</td>\n",
              "      <td>-3.003</td>\n",
              "      <td>3.068</td>\n",
              "      <td>-8.178</td>\n",
              "      <td>6.575</td>\n",
              "      <td>-2.394</td>\n",
              "      <td>-2.753</td>\n",
              "      <td>-12.753</td>\n",
              "      <td>-7.025</td>\n",
              "      <td>-14.540</td>\n",
              "      <td>-17.929</td>\n",
              "      <td>-8.875</td>\n",
              "      <td>15.422</td>\n",
              "      <td>27.466</td>\n",
              "      <td>3.144</td>\n",
              "      <td>-9.127</td>\n",
              "      <td>3.959</td>\n",
              "      <td>-2.501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T14</th>\n",
              "      <td>10.184</td>\n",
              "      <td>-6.719</td>\n",
              "      <td>-12.230</td>\n",
              "      <td>-8.543</td>\n",
              "      <td>16.270</td>\n",
              "      <td>3.494</td>\n",
              "      <td>-0.185</td>\n",
              "      <td>9.359</td>\n",
              "      <td>4.386</td>\n",
              "      <td>-3.749</td>\n",
              "      <td>-2.103</td>\n",
              "      <td>7.291</td>\n",
              "      <td>-2.806</td>\n",
              "      <td>-3.632</td>\n",
              "      <td>13.804</td>\n",
              "      <td>27.908</td>\n",
              "      <td>-7.513</td>\n",
              "      <td>-3.909</td>\n",
              "      <td>3.871</td>\n",
              "      <td>-13.113</td>\n",
              "      <td>11.029</td>\n",
              "      <td>1.571</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>-5.271</td>\n",
              "      <td>-3.593</td>\n",
              "      <td>-4.833</td>\n",
              "      <td>-7.015</td>\n",
              "      <td>-0.597</td>\n",
              "      <td>-5.799</td>\n",
              "      <td>4.042</td>\n",
              "      <td>1.479</td>\n",
              "      <td>11.675</td>\n",
              "      <td>28.208</td>\n",
              "      <td>-9.968</td>\n",
              "      <td>-4.347</td>\n",
              "      <td>-0.290</td>\n",
              "      <td>0.172</td>\n",
              "      <td>1.525</td>\n",
              "      <td>-3.222</td>\n",
              "      <td>5.605</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.212</td>\n",
              "      <td>7.097</td>\n",
              "      <td>-0.035</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>5.054</td>\n",
              "      <td>31.555</td>\n",
              "      <td>7.336</td>\n",
              "      <td>-9.550</td>\n",
              "      <td>10.483</td>\n",
              "      <td>2.790</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>22.569</td>\n",
              "      <td>8.468</td>\n",
              "      <td>-7.510</td>\n",
              "      <td>-1.072</td>\n",
              "      <td>-4.670</td>\n",
              "      <td>0.918</td>\n",
              "      <td>5.287</td>\n",
              "      <td>-1.883</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-7.959</td>\n",
              "      <td>2.889</td>\n",
              "      <td>-8.365</td>\n",
              "      <td>5.173</td>\n",
              "      <td>-0.251</td>\n",
              "      <td>-8.223</td>\n",
              "      <td>-8.851</td>\n",
              "      <td>-1.505</td>\n",
              "      <td>-2.905</td>\n",
              "      <td>24.113</td>\n",
              "      <td>-0.135</td>\n",
              "      <td>10.933</td>\n",
              "      <td>-8.467</td>\n",
              "      <td>-0.169</td>\n",
              "      <td>4.933</td>\n",
              "      <td>-5.087</td>\n",
              "      <td>-5.362</td>\n",
              "      <td>5.956</td>\n",
              "      <td>1.637</td>\n",
              "      <td>-0.414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T15</th>\n",
              "      <td>0.827</td>\n",
              "      <td>-5.620</td>\n",
              "      <td>-1.186</td>\n",
              "      <td>-3.123</td>\n",
              "      <td>9.317</td>\n",
              "      <td>4.057</td>\n",
              "      <td>3.634</td>\n",
              "      <td>3.703</td>\n",
              "      <td>-3.773</td>\n",
              "      <td>7.752</td>\n",
              "      <td>-5.277</td>\n",
              "      <td>6.939</td>\n",
              "      <td>-1.187</td>\n",
              "      <td>4.443</td>\n",
              "      <td>3.395</td>\n",
              "      <td>64.936</td>\n",
              "      <td>-4.865</td>\n",
              "      <td>4.341</td>\n",
              "      <td>0.245</td>\n",
              "      <td>7.712</td>\n",
              "      <td>-10.372</td>\n",
              "      <td>0.236</td>\n",
              "      <td>5.720</td>\n",
              "      <td>2.809</td>\n",
              "      <td>5.545</td>\n",
              "      <td>-1.937</td>\n",
              "      <td>11.314</td>\n",
              "      <td>-2.376</td>\n",
              "      <td>4.342</td>\n",
              "      <td>5.756</td>\n",
              "      <td>2.937</td>\n",
              "      <td>-4.113</td>\n",
              "      <td>-5.969</td>\n",
              "      <td>3.469</td>\n",
              "      <td>2.207</td>\n",
              "      <td>-1.149</td>\n",
              "      <td>-0.866</td>\n",
              "      <td>4.044</td>\n",
              "      <td>-2.464</td>\n",
              "      <td>-2.378</td>\n",
              "      <td>...</td>\n",
              "      <td>17.493</td>\n",
              "      <td>2.969</td>\n",
              "      <td>4.053</td>\n",
              "      <td>-4.405</td>\n",
              "      <td>-9.354</td>\n",
              "      <td>-14.412</td>\n",
              "      <td>4.844</td>\n",
              "      <td>3.781</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-4.523</td>\n",
              "      <td>-7.735</td>\n",
              "      <td>-2.653</td>\n",
              "      <td>2.713</td>\n",
              "      <td>8.332</td>\n",
              "      <td>-1.430</td>\n",
              "      <td>-6.357</td>\n",
              "      <td>-8.647</td>\n",
              "      <td>-4.331</td>\n",
              "      <td>2.934</td>\n",
              "      <td>-10.608</td>\n",
              "      <td>6.888</td>\n",
              "      <td>1.899</td>\n",
              "      <td>14.219</td>\n",
              "      <td>2.975</td>\n",
              "      <td>0.054</td>\n",
              "      <td>-4.724</td>\n",
              "      <td>13.055</td>\n",
              "      <td>3.712</td>\n",
              "      <td>-10.768</td>\n",
              "      <td>-11.103</td>\n",
              "      <td>2.921</td>\n",
              "      <td>1.673</td>\n",
              "      <td>10.400</td>\n",
              "      <td>-5.808</td>\n",
              "      <td>-7.439</td>\n",
              "      <td>9.300</td>\n",
              "      <td>1.014</td>\n",
              "      <td>-1.361</td>\n",
              "      <td>-1.561</td>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T16</th>\n",
              "      <td>-0.004</td>\n",
              "      <td>1.196</td>\n",
              "      <td>-7.011</td>\n",
              "      <td>7.548</td>\n",
              "      <td>-14.105</td>\n",
              "      <td>-4.221</td>\n",
              "      <td>3.637</td>\n",
              "      <td>2.752</td>\n",
              "      <td>1.552</td>\n",
              "      <td>6.531</td>\n",
              "      <td>-8.739</td>\n",
              "      <td>-13.379</td>\n",
              "      <td>2.595</td>\n",
              "      <td>2.285</td>\n",
              "      <td>-10.380</td>\n",
              "      <td>-4.365</td>\n",
              "      <td>7.802</td>\n",
              "      <td>2.476</td>\n",
              "      <td>-2.729</td>\n",
              "      <td>-9.391</td>\n",
              "      <td>14.290</td>\n",
              "      <td>6.356</td>\n",
              "      <td>-4.505</td>\n",
              "      <td>-10.141</td>\n",
              "      <td>4.732</td>\n",
              "      <td>-6.279</td>\n",
              "      <td>-7.953</td>\n",
              "      <td>-2.796</td>\n",
              "      <td>3.693</td>\n",
              "      <td>-16.896</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>-12.631</td>\n",
              "      <td>8.450</td>\n",
              "      <td>21.401</td>\n",
              "      <td>-5.846</td>\n",
              "      <td>-6.684</td>\n",
              "      <td>-7.358</td>\n",
              "      <td>-9.992</td>\n",
              "      <td>-4.456</td>\n",
              "      <td>-12.826</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.032</td>\n",
              "      <td>4.842</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>-0.902</td>\n",
              "      <td>-1.901</td>\n",
              "      <td>12.245</td>\n",
              "      <td>2.381</td>\n",
              "      <td>9.453</td>\n",
              "      <td>-2.698</td>\n",
              "      <td>-2.386</td>\n",
              "      <td>-9.080</td>\n",
              "      <td>-3.803</td>\n",
              "      <td>1.908</td>\n",
              "      <td>8.986</td>\n",
              "      <td>8.868</td>\n",
              "      <td>1.671</td>\n",
              "      <td>-8.802</td>\n",
              "      <td>-5.220</td>\n",
              "      <td>4.554</td>\n",
              "      <td>-9.570</td>\n",
              "      <td>2.932</td>\n",
              "      <td>-2.553</td>\n",
              "      <td>22.416</td>\n",
              "      <td>13.039</td>\n",
              "      <td>4.907</td>\n",
              "      <td>6.334</td>\n",
              "      <td>6.723</td>\n",
              "      <td>-7.903</td>\n",
              "      <td>-4.828</td>\n",
              "      <td>10.221</td>\n",
              "      <td>6.668</td>\n",
              "      <td>-2.118</td>\n",
              "      <td>-2.096</td>\n",
              "      <td>-25.380</td>\n",
              "      <td>-8.052</td>\n",
              "      <td>-7.748</td>\n",
              "      <td>15.534</td>\n",
              "      <td>9.953</td>\n",
              "      <td>5.679</td>\n",
              "      <td>-9.461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T17</th>\n",
              "      <td>0.710</td>\n",
              "      <td>7.395</td>\n",
              "      <td>-3.044</td>\n",
              "      <td>-4.428</td>\n",
              "      <td>-11.849</td>\n",
              "      <td>-4.010</td>\n",
              "      <td>-4.894</td>\n",
              "      <td>-4.280</td>\n",
              "      <td>-15.619</td>\n",
              "      <td>-5.902</td>\n",
              "      <td>-5.924</td>\n",
              "      <td>-18.789</td>\n",
              "      <td>-5.739</td>\n",
              "      <td>-16.839</td>\n",
              "      <td>-0.743</td>\n",
              "      <td>-21.017</td>\n",
              "      <td>2.703</td>\n",
              "      <td>-2.244</td>\n",
              "      <td>-8.805</td>\n",
              "      <td>-16.564</td>\n",
              "      <td>7.047</td>\n",
              "      <td>-7.005</td>\n",
              "      <td>1.052</td>\n",
              "      <td>-4.649</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>-2.818</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-4.734</td>\n",
              "      <td>0.881</td>\n",
              "      <td>-15.216</td>\n",
              "      <td>3.353</td>\n",
              "      <td>-19.790</td>\n",
              "      <td>6.341</td>\n",
              "      <td>-16.186</td>\n",
              "      <td>3.502</td>\n",
              "      <td>-10.398</td>\n",
              "      <td>-7.822</td>\n",
              "      <td>-4.147</td>\n",
              "      <td>1.410</td>\n",
              "      <td>-4.401</td>\n",
              "      <td>...</td>\n",
              "      <td>-10.580</td>\n",
              "      <td>-4.005</td>\n",
              "      <td>16.832</td>\n",
              "      <td>4.494</td>\n",
              "      <td>-8.867</td>\n",
              "      <td>3.280</td>\n",
              "      <td>-2.357</td>\n",
              "      <td>8.133</td>\n",
              "      <td>2.820</td>\n",
              "      <td>-1.678</td>\n",
              "      <td>1.369</td>\n",
              "      <td>1.125</td>\n",
              "      <td>4.672</td>\n",
              "      <td>-14.206</td>\n",
              "      <td>3.978</td>\n",
              "      <td>6.578</td>\n",
              "      <td>0.297</td>\n",
              "      <td>2.257</td>\n",
              "      <td>-1.226</td>\n",
              "      <td>-3.433</td>\n",
              "      <td>-4.614</td>\n",
              "      <td>-5.883</td>\n",
              "      <td>-11.449</td>\n",
              "      <td>8.679</td>\n",
              "      <td>4.974</td>\n",
              "      <td>1.390</td>\n",
              "      <td>-5.200</td>\n",
              "      <td>-6.586</td>\n",
              "      <td>5.372</td>\n",
              "      <td>6.244</td>\n",
              "      <td>6.207</td>\n",
              "      <td>0.130</td>\n",
              "      <td>12.090</td>\n",
              "      <td>-13.236</td>\n",
              "      <td>-2.328</td>\n",
              "      <td>-7.148</td>\n",
              "      <td>0.831</td>\n",
              "      <td>4.116</td>\n",
              "      <td>7.533</td>\n",
              "      <td>-3.801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T18</th>\n",
              "      <td>-8.059</td>\n",
              "      <td>1.560</td>\n",
              "      <td>-5.139</td>\n",
              "      <td>10.415</td>\n",
              "      <td>-2.907</td>\n",
              "      <td>2.013</td>\n",
              "      <td>-2.374</td>\n",
              "      <td>8.692</td>\n",
              "      <td>-14.323</td>\n",
              "      <td>10.472</td>\n",
              "      <td>6.365</td>\n",
              "      <td>-6.716</td>\n",
              "      <td>2.697</td>\n",
              "      <td>5.079</td>\n",
              "      <td>-2.502</td>\n",
              "      <td>25.996</td>\n",
              "      <td>3.406</td>\n",
              "      <td>3.353</td>\n",
              "      <td>2.448</td>\n",
              "      <td>-11.296</td>\n",
              "      <td>-6.335</td>\n",
              "      <td>-0.188</td>\n",
              "      <td>-3.286</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>-6.052</td>\n",
              "      <td>2.928</td>\n",
              "      <td>2.344</td>\n",
              "      <td>-0.169</td>\n",
              "      <td>15.491</td>\n",
              "      <td>6.868</td>\n",
              "      <td>-14.883</td>\n",
              "      <td>-5.164</td>\n",
              "      <td>-10.001</td>\n",
              "      <td>1.397</td>\n",
              "      <td>0.719</td>\n",
              "      <td>8.425</td>\n",
              "      <td>-4.485</td>\n",
              "      <td>-2.007</td>\n",
              "      <td>1.313</td>\n",
              "      <td>0.707</td>\n",
              "      <td>...</td>\n",
              "      <td>1.214</td>\n",
              "      <td>1.436</td>\n",
              "      <td>2.079</td>\n",
              "      <td>-3.699</td>\n",
              "      <td>-16.484</td>\n",
              "      <td>5.075</td>\n",
              "      <td>0.367</td>\n",
              "      <td>-8.421</td>\n",
              "      <td>8.624</td>\n",
              "      <td>-2.601</td>\n",
              "      <td>1.364</td>\n",
              "      <td>5.992</td>\n",
              "      <td>-2.243</td>\n",
              "      <td>1.417</td>\n",
              "      <td>2.783</td>\n",
              "      <td>-1.819</td>\n",
              "      <td>10.899</td>\n",
              "      <td>-4.226</td>\n",
              "      <td>-2.254</td>\n",
              "      <td>8.366</td>\n",
              "      <td>-8.007</td>\n",
              "      <td>6.010</td>\n",
              "      <td>-13.034</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>-3.091</td>\n",
              "      <td>7.353</td>\n",
              "      <td>-11.932</td>\n",
              "      <td>4.777</td>\n",
              "      <td>-3.824</td>\n",
              "      <td>3.147</td>\n",
              "      <td>-6.696</td>\n",
              "      <td>3.621</td>\n",
              "      <td>6.133</td>\n",
              "      <td>-8.193</td>\n",
              "      <td>-15.605</td>\n",
              "      <td>8.725</td>\n",
              "      <td>-3.899</td>\n",
              "      <td>-7.752</td>\n",
              "      <td>2.716</td>\n",
              "      <td>5.327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T19</th>\n",
              "      <td>6.718</td>\n",
              "      <td>0.940</td>\n",
              "      <td>-1.921</td>\n",
              "      <td>-8.834</td>\n",
              "      <td>18.563</td>\n",
              "      <td>1.383</td>\n",
              "      <td>-8.026</td>\n",
              "      <td>-4.078</td>\n",
              "      <td>-5.389</td>\n",
              "      <td>-0.768</td>\n",
              "      <td>5.837</td>\n",
              "      <td>13.603</td>\n",
              "      <td>0.561</td>\n",
              "      <td>-11.831</td>\n",
              "      <td>1.329</td>\n",
              "      <td>-2.145</td>\n",
              "      <td>13.092</td>\n",
              "      <td>-4.706</td>\n",
              "      <td>2.388</td>\n",
              "      <td>11.373</td>\n",
              "      <td>-9.325</td>\n",
              "      <td>3.593</td>\n",
              "      <td>-4.489</td>\n",
              "      <td>-1.905</td>\n",
              "      <td>3.319</td>\n",
              "      <td>-2.603</td>\n",
              "      <td>4.446</td>\n",
              "      <td>3.754</td>\n",
              "      <td>5.648</td>\n",
              "      <td>1.683</td>\n",
              "      <td>-10.669</td>\n",
              "      <td>15.137</td>\n",
              "      <td>-11.945</td>\n",
              "      <td>8.141</td>\n",
              "      <td>-1.403</td>\n",
              "      <td>-13.650</td>\n",
              "      <td>6.796</td>\n",
              "      <td>-1.725</td>\n",
              "      <td>1.764</td>\n",
              "      <td>-4.386</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.061</td>\n",
              "      <td>3.195</td>\n",
              "      <td>-6.546</td>\n",
              "      <td>7.212</td>\n",
              "      <td>-1.673</td>\n",
              "      <td>-16.380</td>\n",
              "      <td>-1.855</td>\n",
              "      <td>-3.314</td>\n",
              "      <td>7.340</td>\n",
              "      <td>0.712</td>\n",
              "      <td>1.095</td>\n",
              "      <td>3.249</td>\n",
              "      <td>7.176</td>\n",
              "      <td>4.775</td>\n",
              "      <td>12.656</td>\n",
              "      <td>-3.875</td>\n",
              "      <td>8.224</td>\n",
              "      <td>4.046</td>\n",
              "      <td>-2.546</td>\n",
              "      <td>5.642</td>\n",
              "      <td>1.133</td>\n",
              "      <td>-4.502</td>\n",
              "      <td>-20.376</td>\n",
              "      <td>6.445</td>\n",
              "      <td>-1.812</td>\n",
              "      <td>12.130</td>\n",
              "      <td>4.752</td>\n",
              "      <td>-3.833</td>\n",
              "      <td>1.793</td>\n",
              "      <td>-12.922</td>\n",
              "      <td>-1.787</td>\n",
              "      <td>11.395</td>\n",
              "      <td>-2.220</td>\n",
              "      <td>18.530</td>\n",
              "      <td>18.511</td>\n",
              "      <td>10.131</td>\n",
              "      <td>-5.468</td>\n",
              "      <td>0.078</td>\n",
              "      <td>-8.699</td>\n",
              "      <td>-7.329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T20</th>\n",
              "      <td>-2.578</td>\n",
              "      <td>3.143</td>\n",
              "      <td>2.272</td>\n",
              "      <td>-4.329</td>\n",
              "      <td>-17.284</td>\n",
              "      <td>-5.878</td>\n",
              "      <td>-0.445</td>\n",
              "      <td>-4.820</td>\n",
              "      <td>-8.981</td>\n",
              "      <td>0.533</td>\n",
              "      <td>1.220</td>\n",
              "      <td>-9.818</td>\n",
              "      <td>-0.938</td>\n",
              "      <td>3.332</td>\n",
              "      <td>12.563</td>\n",
              "      <td>8.231</td>\n",
              "      <td>0.791</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>-3.614</td>\n",
              "      <td>6.318</td>\n",
              "      <td>3.821</td>\n",
              "      <td>0.046</td>\n",
              "      <td>-4.723</td>\n",
              "      <td>0.907</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>-3.191</td>\n",
              "      <td>-5.396</td>\n",
              "      <td>-1.873</td>\n",
              "      <td>4.808</td>\n",
              "      <td>-13.105</td>\n",
              "      <td>-10.036</td>\n",
              "      <td>-15.923</td>\n",
              "      <td>-6.952</td>\n",
              "      <td>10.000</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-4.134</td>\n",
              "      <td>-6.581</td>\n",
              "      <td>-11.724</td>\n",
              "      <td>1.125</td>\n",
              "      <td>-6.764</td>\n",
              "      <td>...</td>\n",
              "      <td>4.126</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-2.276</td>\n",
              "      <td>-0.531</td>\n",
              "      <td>-12.852</td>\n",
              "      <td>6.537</td>\n",
              "      <td>4.942</td>\n",
              "      <td>-2.612</td>\n",
              "      <td>8.298</td>\n",
              "      <td>-7.929</td>\n",
              "      <td>-3.134</td>\n",
              "      <td>-4.031</td>\n",
              "      <td>-2.847</td>\n",
              "      <td>12.050</td>\n",
              "      <td>2.962</td>\n",
              "      <td>-0.454</td>\n",
              "      <td>6.752</td>\n",
              "      <td>-10.446</td>\n",
              "      <td>0.237</td>\n",
              "      <td>-1.862</td>\n",
              "      <td>-2.274</td>\n",
              "      <td>2.424</td>\n",
              "      <td>3.162</td>\n",
              "      <td>3.459</td>\n",
              "      <td>2.814</td>\n",
              "      <td>4.721</td>\n",
              "      <td>7.620</td>\n",
              "      <td>-9.816</td>\n",
              "      <td>-1.115</td>\n",
              "      <td>5.720</td>\n",
              "      <td>2.517</td>\n",
              "      <td>-6.949</td>\n",
              "      <td>4.475</td>\n",
              "      <td>55.384</td>\n",
              "      <td>-7.124</td>\n",
              "      <td>7.663</td>\n",
              "      <td>13.723</td>\n",
              "      <td>-0.664</td>\n",
              "      <td>-2.455</td>\n",
              "      <td>0.781</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 1740 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0       1       2       3     ...    1736    1737    1738    1739\n",
              "T1   49.207  19.875  32.480  41.207  ...  46.058  25.953  43.850  26.103\n",
              "T2    5.732   6.621  -3.491  28.411  ... -11.195   1.214 -31.794  -6.085\n",
              "T3   42.819   6.901 -12.635   7.661  ... -45.472  -3.542  28.409   4.660\n",
              "T4    5.160 -12.437   8.834 -28.256  ...  23.541  -5.380   2.059  -8.330\n",
              "T5   22.560   0.486  -2.550   7.780  ... -20.470  -7.368 -24.891  -4.438\n",
              "T6  -25.837 -12.345  -5.989 -24.211  ... -19.396   0.502  -0.492  -1.667\n",
              "T7    5.353   2.338   1.101  32.674  ...  15.697   0.390   6.574  -9.650\n",
              "T8  -11.635  -9.726  -5.436  -6.190  ... -16.684   0.343  -4.835   6.520\n",
              "T9    3.665  -4.675   7.998  -6.820  ... -20.269   5.660  -8.082   1.863\n",
              "T10  20.994  -0.788   0.395 -28.475  ...   8.319   8.089   0.428  -6.606\n",
              "T11  -3.092  -5.121  -1.234  18.031  ...  11.858  -0.546   2.691   3.622\n",
              "T12   4.517  -1.583   8.024  -8.204  ...  -1.885   6.153  -4.379   5.275\n",
              "T13  -6.655  -2.442   4.533  14.893  ...   3.144  -9.127   3.959  -2.501\n",
              "T14  10.184  -6.719 -12.230  -8.543  ...  -5.362   5.956   1.637  -0.414\n",
              "T15   0.827  -5.620  -1.186  -3.123  ...   1.014  -1.361  -1.561   0.800\n",
              "T16  -0.004   1.196  -7.011   7.548  ...  15.534   9.953   5.679  -9.461\n",
              "T17   0.710   7.395  -3.044  -4.428  ...   0.831   4.116   7.533  -3.801\n",
              "T18  -8.059   1.560  -5.139  10.415  ...  -3.899  -7.752   2.716   5.327\n",
              "T19   6.718   0.940  -1.921  -8.834  ...  -5.468   0.078  -8.699  -7.329\n",
              "T20  -2.578   3.143   2.272  -4.329  ...  13.723  -0.664  -2.455   0.781\n",
              "\n",
              "[20 rows x 1740 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgpkBNpJFrzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "3cbebcb2-3b52-4ede-9cff-34a909d438e7"
      },
      "source": [
        "document_numbers = [1, 4, 10]\n",
        "\n",
        "for document_number in document_numbers:\n",
        "    top_topics = list(dt_df.columns[np.argsort(-np.absolute(dt_df.iloc[document_number].values))[:3]])\n",
        "    print('Document #'+str(document_number)+':')\n",
        "    print('Dominant Topics (top 3):', top_topics)\n",
        "    print('Paper Summary:')\n",
        "    print(papers[document_number][:500])\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document #1:\n",
            "Dominant Topics (top 3): ['T1', 'T4', 'T6']\n",
            "Paper Summary:\n",
            "1 \n",
            "CONNECTIVITY VERSUS ENTROPY \n",
            "Yaser S. Abu-Mostafa \n",
            "California Institute of Technology \n",
            "Pasadena, CA 91125 \n",
            "ABSTRACT \n",
            "How does the connectivity of a neural network (number of synapses per \n",
            "neuron) relate to the complexity of the problems it can handle (measured by \n",
            "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
            "functions can be implemented using a circuit with very low connectivity (e.g., \n",
            "using two-input NAND gates). However, for a network that learns a pr\n",
            "\n",
            "Document #4:\n",
            "Dominant Topics (top 3): ['T1', 'T8', 'T19']\n",
            "Paper Summary:\n",
            "174 \n",
            "A Neural Network C1A-sifier Based on Coding Theory \n",
            "Tzi-Dar Chiueh and Rodney Goodman \n",
            "California Institute of Technology, Pasadena, California 91125 \n",
            "ABSTRACT\n",
            "The new neural network classifier we propose transforms the \n",
            "classification problem into the coding theory problem of decoding a noisy \n",
            "codeword. An input vector in the feature space is transformed into an internal \n",
            "representation which is a codeword in the code space, and then error correction \n",
            "decoded in this space to classify the \n",
            "\n",
            "Document #10:\n",
            "Dominant Topics (top 3): ['T1', 'T16', 'T13']\n",
            "Paper Summary:\n",
            "554 \n",
            "STABILITY RESULTS FOR NEURAL NETWORKS \n",
            "A. N. Michel  , J. A. Farrell  , and W. Porod 2 \n",
            "Department of Electrical and Computer Engineering \n",
            "University of Notre Dame \n",
            "Notre Dame, IN 46556 \n",
            "ABSTRACT \n",
            "In the present paper we survey mad utilize results from the qualitative theory of large \n",
            "scale interconnected dynamical systems in order to develop a qualitative theory for the \n",
            "Hop field model of neural networks. In our approach we view such networks as an inter- \n",
            "connection of many single neur\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41qraMQLFrzR",
        "colab_type": "text"
      },
      "source": [
        "# Topic Models with Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz3weBk_FrzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be566954-b818-401d-eeb0-73a6e148e8ba"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50,\n",
        "                                      learning_method='online', batch_size=1740, learning_offset=50., \n",
        "                                      random_state=42, n_jobs=16)\n",
        "document_topics = lda_model.fit_transform(cv_features)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 19s, sys: 15.9 s, total: 1min 35s\n",
            "Wall time: 20min 41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkRTbcYiQ0lh",
        "colab_type": "text"
      },
      "source": [
        "CPU times: user 13min 14s, sys: 1min 41s, total: 14min 56s\n",
        "Wall time: 55min 32s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-PQVqQIFrzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_terms = lda_model.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZhAfSBGFrzW",
        "colab_type": "code",
        "colab": {},
        "outputId": "f0c2b4f0-5be4-4992-f85a-c7379ab0df2b"
      },
      "source": [
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms] #20\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, principal component</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                              Terms per Topic\n",
              "Topic1   neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig                                              \n",
              "Topic2   image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line                                                  \n",
              "Topic3   neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition                                \n",
              "Topic4   cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map                                              \n",
              "Topic5   image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule                                \n",
              "Topic6   state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation                                     \n",
              "Topic7   sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio                                     \n",
              "Topic8   memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone                      \n",
              "Topic9   activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self                             \n",
              "Topic10  state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field                                         \n",
              "Topic11  generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, principal component\n",
              "Topic12  feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition                                                \n",
              "Topic13  image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map                                                               \n",
              "Topic14  neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction                                           \n",
              "Topic15  distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density                                 \n",
              "Topic16  state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high                                                  \n",
              "Topic17  state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class                                       \n",
              "Topic18  state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm                  \n",
              "Topic19  control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal                                        \n",
              "Topic20  mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq                         "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnNrKTZEFrzZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb5b8e50-3e5d-4a6b-ba86-3d9c4d8b962d"
      },
      "source": [
        "pd.options.display.float_format = '{:,.3f}'.format\n",
        "dt_df = pd.DataFrame(document_topics, \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1730</th>\n",
              "      <th>1731</th>\n",
              "      <th>1732</th>\n",
              "      <th>1733</th>\n",
              "      <th>1734</th>\n",
              "      <th>1735</th>\n",
              "      <th>1736</th>\n",
              "      <th>1737</th>\n",
              "      <th>1738</th>\n",
              "      <th>1739</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T1</th>\n",
              "      <td>0.011</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.063</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.028</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.773</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.786</td>\n",
              "      <td>0.269</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T5</th>\n",
              "      <td>0.227</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.591</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.466</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T6</th>\n",
              "      <td>0.446</td>\n",
              "      <td>0.579</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T7</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T8</th>\n",
              "      <td>0.062</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T9</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T10</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T11</th>\n",
              "      <td>0.037</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T12</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T13</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T14</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T15</th>\n",
              "      <td>0.183</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.710</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.937</td>\n",
              "      <td>0.907</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T16</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T17</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T18</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T19</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T20</th>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 1740 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2     3     4     5     6     7     8     9  ...   1730  \\\n",
              "T1  0.011 0.137 0.017 0.000 0.219 0.034 0.477 0.218 0.120 0.063  ...  0.000   \n",
              "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T4  0.028 0.000 0.663 0.773 0.262 0.802 0.030 0.000 0.786 0.269  ...  0.000   \n",
              "T5  0.227 0.035 0.045 0.212 0.024 0.086 0.086 0.370 0.089 0.402  ...  0.210   \n",
              "T6  0.446 0.579 0.000 0.000 0.238 0.022 0.102 0.022 0.000 0.036  ...  0.000   \n",
              "T7  0.000 0.000 0.026 0.015 0.008 0.000 0.000 0.000 0.005 0.000  ...  0.000   \n",
              "T8  0.062 0.000 0.000 0.000 0.000 0.017 0.041 0.013 0.000 0.000  ...  0.040   \n",
              "T9  0.000 0.025 0.113 0.000 0.000 0.021 0.116 0.000 0.000 0.000  ...  0.003   \n",
              "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T11 0.037 0.002 0.026 0.000 0.000 0.000 0.000 0.042 0.000 0.000  ...  0.000   \n",
              "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T15 0.183 0.222 0.000 0.000 0.191 0.004 0.000 0.333 0.000 0.225  ...  0.710   \n",
              "T16 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T18 0.000 0.000 0.109 0.000 0.057 0.008 0.146 0.000 0.000 0.004  ...  0.019   \n",
              "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...  0.000   \n",
              "T20 0.006 0.000 0.000 0.000 0.000 0.004 0.000 0.000 0.000 0.000  ...  0.018   \n",
              "\n",
              "     1731  1732  1733  1734  1735  1736  1737  1738  1739  \n",
              "T1  0.000 0.000 0.000 0.000 0.000 0.115 0.000 0.000 0.482  \n",
              "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T4  0.107 0.000 0.000 0.000 0.999 0.000 0.000 0.130 0.518  \n",
              "T5  0.591 0.043 0.007 0.019 0.000 0.431 0.466 0.000 0.000  \n",
              "T6  0.094 0.000 0.000 0.330 0.000 0.162 0.000 0.000 0.000  \n",
              "T7  0.000 0.000 0.003 0.002 0.000 0.000 0.000 0.003 0.000  \n",
              "T8  0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000  \n",
              "T9  0.000 0.000 0.000 0.000 0.000 0.015 0.000 0.000 0.000  \n",
              "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T11 0.170 0.000 0.011 0.000 0.000 0.000 0.074 0.000 0.000  \n",
              "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T15 0.028 0.937 0.907 0.597 0.000 0.238 0.446 0.403 0.000  \n",
              "T16 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T18 0.000 0.019 0.037 0.000 0.000 0.014 0.000 0.433 0.000  \n",
              "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
              "T20 0.009 0.000 0.034 0.051 0.000 0.010 0.014 0.030 0.000  \n",
              "\n",
              "[20 rows x 1740 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZKWNiKFrzb",
        "colab_type": "code",
        "colab": {},
        "outputId": "a4a54cb8-311c-4b37-d4b6-a9191c365183"
      },
      "source": [
        "pd.options.display.float_format = '{:,.5f}'.format\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "max_contrib_topics = dt_df.max(axis=0)\n",
        "dominant_topics = max_contrib_topics.index\n",
        "contrib_perc = max_contrib_topics.values\n",
        "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0]\n",
        "                       for t in dominant_topics]\n",
        "documents = [papers[i] for i in document_numbers]\n",
        "\n",
        "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
        "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
        "                          'Paper Name': documents})\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Contribution %</th>\n",
              "      <th>Paper Num</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Paper Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>T1</td>\n",
              "      <td>0.99938</td>\n",
              "      <td>1122</td>\n",
              "      <td>neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig</td>\n",
              "      <td>Improved Silicon Cochlea \\nusing \\nCompatible Lateral Bipolar Transistors \\nAndr6 van Schaik, Eric Fragnire, Eric Vittoz \\nMANTRA Center for Neuromimetic Systems \\nSwiss Federal Institute of Tech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>T2</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>T3</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>T4</td>\n",
              "      <td>0.99947</td>\n",
              "      <td>1735</td>\n",
              "      <td>cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map</td>\n",
              "      <td>Can V1 mechanisms account for \\nfigure-ground and medial axis effects? \\nZhaoping Li \\nGatsby Computational Neuroscience Unit \\nUniversity College London \\nzhaopinggat shy. ucl. ac. uk \\nAbstract...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>T5</td>\n",
              "      <td>0.99949</td>\n",
              "      <td>177</td>\n",
              "      <td>image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule</td>\n",
              "      <td>215 \\nConsonant Recognition by Modular Construction of \\nLarge Phonemic Time-Delay Neural Networks \\nAlex Waibel \\nCarnegie-Mellon University \\nPittsburgh, PA 15213, \\nATR Interpreting Telephony R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>T6</td>\n",
              "      <td>0.99684</td>\n",
              "      <td>1128</td>\n",
              "      <td>state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation</td>\n",
              "      <td>Finite State Automata that Recurrent \\nCascade-Correlation Cannot Represent \\nStefan C. Kremer \\nDepartment of Computing Science \\nUniversity of Alberta \\nEdmonton, Alberta, CANADA T6H 5B5 \\nAbstr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>T7</td>\n",
              "      <td>0.99956</td>\n",
              "      <td>283</td>\n",
              "      <td>sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio</td>\n",
              "      <td>A Neural Network to Detect \\nHomologies in Proteins \\nYoshua Bengio \\nSchool of Computer Science \\nMcGill University \\nMontreal, Canada H3A 2A7 \\nSamy Bengio \\nDepartement d'Informatique \\nUnivers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>T8</td>\n",
              "      <td>0.98167</td>\n",
              "      <td>892</td>\n",
              "      <td>memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone</td>\n",
              "      <td>A solvable connectionist model of \\nimmediate recall of ordered lists \\nNell Burgess \\nDepartment of Anatomy, University College London \\nLondon WCiE 6BT, England \\n(e-mail: n .burgessucl. ac. uk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>T9</td>\n",
              "      <td>0.99929</td>\n",
              "      <td>227</td>\n",
              "      <td>activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self</td>\n",
              "      <td>44 Beer and Chiei \\nNeural \\nImplementation of Motivated Behavior: \\nFeeding in an Artificial Insect \\nRandall D. Beer t,2 and Hillel J. Chiel 2 \\nDepartments of t Computer Engineering and Science...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>T10</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>T11</td>\n",
              "      <td>0.99917</td>\n",
              "      <td>1396</td>\n",
              "      <td>generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, pri...</td>\n",
              "      <td>I I II \\nThe Storage Capacity \\nof a Fully-Connected Committee Machine \\nYuansheng Xiong \\nDepartment of Physics, Pohang Institute of Science and Technology, \\nHyoja San 31, Pohang, Kyongbuk, Kore...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>T12</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>T13</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>T14</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>T15</td>\n",
              "      <td>0.99947</td>\n",
              "      <td>609</td>\n",
              "      <td>distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density</td>\n",
              "      <td>On the Use of Evidence in Neural Networks \\nDavid H. Wolpert \\nThe Santa Fe Institute \\n1660 Old Pecos Trail \\nSanta Fe, NM 87501 \\nAbstract \\nThe Bayesian \"evidence\" approximation has recently be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>T16</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>T17</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>T18</td>\n",
              "      <td>0.99929</td>\n",
              "      <td>940</td>\n",
              "      <td>state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm</td>\n",
              "      <td>An Actor/Critic Algorithm that is \\nEquivalent to Q-Learning \\nRobert H. Crites \\nComputer Science Department \\nUniversity of Massachusetts \\nAmherst, MA 01003 \\ncrit escs .umass. edu \\nAndrew G....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>T19</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>151</td>\n",
              "      <td>control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal</td>\n",
              "      <td>794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>T20</td>\n",
              "      <td>0.99939</td>\n",
              "      <td>1089</td>\n",
              "      <td>mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq</td>\n",
              "      <td>A Unified Learning Scheme: \\nBayesian-Kullback Ying-Yang Machine \\nLei Xu \\n1. Computer Science Dept., The Chinese University of HK, Hong Kong \\n2. National Machine Perception Lab, Peking Universi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topic  Contribution %  Paper Num  \\\n",
              "Topic1              T1         0.99938       1122   \n",
              "Topic2              T2         0.00033        151   \n",
              "Topic3              T3         0.00033        151   \n",
              "Topic4              T4         0.99947       1735   \n",
              "Topic5              T5         0.99949        177   \n",
              "Topic6              T6         0.99684       1128   \n",
              "Topic7              T7         0.99956        283   \n",
              "Topic8              T8         0.98167        892   \n",
              "Topic9              T9         0.99929        227   \n",
              "Topic10            T10         0.00033        151   \n",
              "Topic11            T11         0.99917       1396   \n",
              "Topic12            T12         0.00033        151   \n",
              "Topic13            T13         0.00033        151   \n",
              "Topic14            T14         0.00033        151   \n",
              "Topic15            T15         0.99947        609   \n",
              "Topic16            T16         0.00033        151   \n",
              "Topic17            T17         0.00033        151   \n",
              "Topic18            T18         0.99929        940   \n",
              "Topic19            T19         0.00033        151   \n",
              "Topic20            T20         0.99939       1089   \n",
              "\n",
              "                                                                                                                                                                                                           Topic  \\\n",
              "Topic1                                    neuron, circuit, analog, chip, current, voltage, signal, threshold, bit, noise, vlsi, implementation, channel, gate, pulse, processor, element, synapse, parallel, fig   \n",
              "Topic2                                        image, feature, structure, state, layer, neuron, distribution, local, cell, recognition, node, motion, matrix, net, sequence, object, gaussian, hidden, size, line   \n",
              "Topic3                      neuron, cell, image, class, state, response, rule, feature, rate, probability, representation, hidden, dynamic, et al, frequency, spike, distribution, component, level, recognition   \n",
              "Topic4                                    cell, neuron, response, visual, stimulus, activity, field, spike, motion, synaptic, direction, frequency, signal, cortex, firing, orientation, spatial, eye, rate, map   \n",
              "Topic5                      image, feature, recognition, layer, hidden, task, object, speech, trained, representation, test, net, classification, classifier, class, level, architecture, experiment, node, rule   \n",
              "Topic6                           state, dynamic, rule, matrix, recurrent, equation, gradient, net, signal, fixed, sequence, node, source, attractor, hidden, structure, step, fixed point, component, activation   \n",
              "Topic7                           sequence, chain, region, structure, markov, protein, prediction, hmms, markov model, hidden markov, site, hidden, gene, class, receptor, length, human, distance, mouse, bengio   \n",
              "Topic8            memory, word, context, similarity, item, recall, probability, phoneme, short, representation, association, activation, list, serial, short term, address, term memory, store, proximity, phone   \n",
              "Topic9                   activation, motor, behavior, winner, take, winner take, competitive, active, command, connection, movement, sensory, feedback, wta, net, sensor, body, activation function, level, self   \n",
              "Topic10                              state, cell, distribution, neuron, probability, control, response, signal, task, rate, layer, architecture, random, hidden, test, image, change, fig, generalization, field   \n",
              "Topic11  generalization, face, hidden unit, hidden, capacity, teacher, pca, student, generalization error, solution, principal, committee, phase, image, limit, correlation, average, line, training set, pri...   \n",
              "Topic12                                     feature, image, distribution, neuron, class, state, hidden, probability, node, layer, equation, size, prediction, line, rate, matrix, et, signal, noise, recognition   \n",
              "Topic13                                                    image, state, cell, object, rule, layer, et al, step, distribution, ii, neuron, visual, signal, field, dynamic, feature, probability, matrix, et, map   \n",
              "Topic14                                neuron, map, state, cell, rate, hidden, field, equation, probability, node, representation, signal, layer, dynamic, et al, noise, test, sequence, recognition, prediction   \n",
              "Topic15                      distribution, probability, variable, class, approximation, gaussian, sample, bound, estimate, noise, matrix, let, optimal, prior, size, variance, xi, equation, prediction, density   \n",
              "Topic16                                       state, neuron, rule, probability, layer, rate, image, distribution, memory, equation, signal, solution, response, theory, class, et, step, variable, feature, high   \n",
              "Topic17                            state, feature, probability, layer, image, cell, field, neuron, task, rate, recognition, dynamic, rule, control, variable, distribution, equation, net, representation, class   \n",
              "Topic18       state, control, action, policy, reinforcement, step, task, optimal, dynamic, controller, trajectory, robot, reinforcement learning, environment, reward, path, goal, value function, decision, arm   \n",
              "Topic19                             control, state, feature, probability, architecture, hidden, neuron, task, rate, level, estimate, et, local, component, net, distribution, signal, response, dynamic, optimal   \n",
              "Topic20              mixture, likelihood, em, cluster, clustering, density, component, log, em algorithm, maximum, code, maximum likelihood, mixture model, hmm, entropy, mi, unsupervised, gaussian, mutual, eq   \n",
              "\n",
              "                                                                                                                                                                                                      Paper Name  \n",
              "Topic1   Improved Silicon Cochlea \\nusing \\nCompatible Lateral Bipolar Transistors \\nAndr6 van Schaik, Eric Fragnire, Eric Vittoz \\nMANTRA Center for Neuromimetic Systems \\nSwiss Federal Institute of Tech...  \n",
              "Topic2   794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic3   794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic4   Can V1 mechanisms account for \\nfigure-ground and medial axis effects? \\nZhaoping Li \\nGatsby Computational Neuroscience Unit \\nUniversity College London \\nzhaopinggat shy. ucl. ac. uk \\nAbstract...  \n",
              "Topic5   215 \\nConsonant Recognition by Modular Construction of \\nLarge Phonemic Time-Delay Neural Networks \\nAlex Waibel \\nCarnegie-Mellon University \\nPittsburgh, PA 15213, \\nATR Interpreting Telephony R...  \n",
              "Topic6   Finite State Automata that Recurrent \\nCascade-Correlation Cannot Represent \\nStefan C. Kremer \\nDepartment of Computing Science \\nUniversity of Alberta \\nEdmonton, Alberta, CANADA T6H 5B5 \\nAbstr...  \n",
              "Topic7   A Neural Network to Detect \\nHomologies in Proteins \\nYoshua Bengio \\nSchool of Computer Science \\nMcGill University \\nMontreal, Canada H3A 2A7 \\nSamy Bengio \\nDepartement d'Informatique \\nUnivers...  \n",
              "Topic8   A solvable connectionist model of \\nimmediate recall of ordered lists \\nNell Burgess \\nDepartment of Anatomy, University College London \\nLondon WCiE 6BT, England \\n(e-mail: n .burgessucl. ac. uk...  \n",
              "Topic9   44 Beer and Chiei \\nNeural \\nImplementation of Motivated Behavior: \\nFeeding in an Artificial Insect \\nRandall D. Beer t,2 and Hillel J. Chiel 2 \\nDepartments of t Computer Engineering and Science...  \n",
              "Topic10  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic11  I I II \\nThe Storage Capacity \\nof a Fully-Connected Committee Machine \\nYuansheng Xiong \\nDepartment of Physics, Pohang Institute of Science and Technology, \\nHyoja San 31, Pohang, Kyongbuk, Kore...  \n",
              "Topic12  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic13  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic14  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic15  On the Use of Evidence in Neural Networks \\nDavid H. Wolpert \\nThe Santa Fe Institute \\n1660 Old Pecos Trail \\nSanta Fe, NM 87501 \\nAbstract \\nThe Bayesian \"evidence\" approximation has recently be...  \n",
              "Topic16  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic17  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic18  An Actor/Critic Algorithm that is \\nEquivalent to Q-Learning \\nRobert H. Crites \\nComputer Science Department \\nUniversity of Massachusetts \\nAmherst, MA 01003 \\ncrit escs .umass. edu \\nAndrew G....  \n",
              "Topic19  794 \\nNEURAL ARCHITECTURE \\nValentino Braitenberg \\nMax Planck Institute \\nFederal Republic of Germany \\nABSTRACT\\nWhile we are waiting for the ultimate biophysics of cell membranes and synapses \\...  \n",
              "Topic20  A Unified Learning Scheme: \\nBayesian-Kullback Ying-Yang Machine \\nLei Xu \\n1. Computer Science Dept., The Chinese University of HK, Hong Kong \\n2. National Machine Perception Lab, Peking Universi...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p7Ow1juFrze",
        "colab_type": "text"
      },
      "source": [
        "# Topic Models with Non-Negative Matrix Factorization (NMF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfzN_5FsFrze",
        "colab_type": "code",
        "colab": {},
        "outputId": "daf4957f-156a-40d1-bbab-faec0b3cde35"
      },
      "source": [
        "%%time\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500,\n",
        "                random_state=42, alpha=.1, l1_ratio=.85)\n",
        "document_topics = nmf_model.fit_transform(cv_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 39s, sys: 47.5 s, total: 12min 26s\n",
            "Wall time: 46.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVjQwUSJFrzh",
        "colab_type": "code",
        "colab": {},
        "outputId": "0e42ec9e-7b43-4338-cb29-28562cb9f474"
      },
      "source": [
        "topic_terms = nmf_model.components_\n",
        "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
        "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
        "topics = [', '.join(topic) for topic in topic_keyterms]\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "topics_df = pd.DataFrame(topics,\n",
        "                         columns = ['Terms per Topic'],\n",
        "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
        "topics_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terms per Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, decision</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, training set, learn, test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                     Terms per Topic\n",
              "Topic1   bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum                                            \n",
              "Topic2   neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing                                \n",
              "Topic3   state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, decision                 \n",
              "Topic4   image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface                                                 \n",
              "Topic5   hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, training set, learn, test\n",
              "Topic6   cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property                                                        \n",
              "Topic7   word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state                               \n",
              "Topic8   signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig                                                        \n",
              "Topic9   control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant                                             \n",
              "Topic10  circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp                                                   \n",
              "Topic11  spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution                                \n",
              "Topic12  rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned                                         \n",
              "Topic13  node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation                                        \n",
              "Topic14  feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra                    \n",
              "Topic15  classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian                       \n",
              "Topic16  distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum                     \n",
              "Topic17  motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed                                                       \n",
              "Topic18  object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint                                      \n",
              "Topic19  field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation                    \n",
              "Topic20  memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol                    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnukcPhbFrzk",
        "colab_type": "code",
        "colab": {},
        "outputId": "d61f3eb7-29cc-4c63-e394-6c0b78abf6f8"
      },
      "source": [
        "pd.options.display.float_format = '{:,.3f}'.format\n",
        "dt_df = pd.DataFrame(document_topics, \n",
        "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
        "dt_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>T8</th>\n",
              "      <th>T9</th>\n",
              "      <th>T10</th>\n",
              "      <th>T11</th>\n",
              "      <th>T12</th>\n",
              "      <th>T13</th>\n",
              "      <th>T14</th>\n",
              "      <th>T15</th>\n",
              "      <th>T16</th>\n",
              "      <th>T17</th>\n",
              "      <th>T18</th>\n",
              "      <th>T19</th>\n",
              "      <th>T20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.444</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.394</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.032</td>\n",
              "      <td>0.619</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.656</td>\n",
              "      <td>0.277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.265</td>\n",
              "      <td>1.019</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>1.299</td>\n",
              "      <td>0.291</td>\n",
              "      <td>1.268</td>\n",
              "      <td>0.295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.060</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.682</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.402</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.749</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.344</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.679</td>\n",
              "      <td>7.510</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.326</td>\n",
              "      <td>1.146</td>\n",
              "      <td>1.923</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.426</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000</td>\n",
              "      <td>1.415</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.147</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.084</td>\n",
              "      <td>1.760</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.592</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.395</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.258</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     T1    T2    T3    T4    T5    T6    T7    T8    T9   T10   T11   T12  \\\n",
              "0 0.444 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
              "1 0.394 0.595 0.463 0.019 0.187 0.037 0.000 0.228 0.130 0.029 0.000 0.254   \n",
              "2 0.032 0.619 0.003 0.067 0.016 0.378 0.029 0.027 0.448 0.000 0.075 0.036   \n",
              "3 0.000 0.274 0.000 0.102 0.265 1.019 0.000 0.000 0.000 0.000 0.000 0.000   \n",
              "4 0.060 0.188 0.682 0.257 0.167 1.402 0.000 0.093 0.000 0.001 0.000 0.020   \n",
              "5 0.000 0.383 0.000 0.000 0.679 7.510 0.016 0.000 0.000 0.326 1.146 1.923   \n",
              "6 0.000 1.415 0.020 0.000 0.046 0.044 0.000 0.114 0.333 0.040 0.000 0.032   \n",
              "7 0.147 0.029 0.000 0.000 0.274 0.008 0.042 0.000 0.045 0.080 0.008 0.025   \n",
              "8 0.084 1.760 0.013 0.012 0.000 1.592 0.000 0.000 0.257 0.068 0.273 0.055   \n",
              "9 0.395 0.000 0.040 1.258 0.127 0.000 0.000 0.370 0.075 0.076 0.000 0.042   \n",
              "\n",
              "    T13   T14   T15   T16   T17   T18   T19   T20  \n",
              "0 0.004 0.263 0.000 0.000 0.000 0.000 0.000 3.437  \n",
              "1 0.000 0.000 0.000 0.000 0.000 0.106 0.000 0.210  \n",
              "2 0.024 0.184 0.100 0.000 0.126 0.000 0.656 0.277  \n",
              "3 0.218 0.011 0.000 0.004 1.299 0.291 1.268 0.295  \n",
              "4 1.749 0.037 0.000 0.344 0.000 0.000 0.164 0.121  \n",
              "5 0.098 0.000 0.000 0.202 0.000 0.426 0.646 0.641  \n",
              "6 0.124 0.000 0.041 0.041 0.075 0.030 0.000 0.615  \n",
              "7 0.022 0.009 0.000 0.023 0.000 0.007 0.000 0.096  \n",
              "8 0.122 0.000 0.119 0.000 0.000 0.027 0.514 0.353  \n",
              "9 0.000 0.017 0.000 0.053 0.041 0.133 0.427 0.000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMp9iIF5Frzn",
        "colab_type": "code",
        "colab": {},
        "outputId": "282702ba-4117-4e6c-fdd5-520a66295502"
      },
      "source": [
        "pd.options.display.float_format = '{:,.5f}'.format\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "max_score_topics = dt_df.max(axis=0)\n",
        "dominant_topics = max_score_topics.index\n",
        "term_score = max_score_topics.values\n",
        "document_numbers = [dt_df[dt_df[t] == max_score_topics.loc[t]].index[0]\n",
        "                       for t in dominant_topics]\n",
        "documents = [papers[i] for i in document_numbers]\n",
        "\n",
        "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Max Score': term_score,\n",
        "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
        "                          'Paper Name': documents})\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Max Score</th>\n",
              "      <th>Paper Num</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Paper Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>T1</td>\n",
              "      <td>1.64138</td>\n",
              "      <td>991</td>\n",
              "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
              "      <td>A Bound on the Error of Cross Validation Using \\nthe Approximation and Estimation Rates, with \\nConsequences for the Training-Test Split \\nMichael Kearns \\nAT&amp;T Research \\nABSTRACT\\n1 INTRODUCTION...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>T2</td>\n",
              "      <td>3.58149</td>\n",
              "      <td>383</td>\n",
              "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
              "      <td>Signal Processing by Multiplexing and \\nDemultiplexing in Neurons \\nDavid C. Tam \\nDivision of Neuroscience \\nBaylor College of Medicine \\nHouston, TX 77030 \\ndtamCnext-cns.neusc.bcm.tmc.edu \\nAb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic3</th>\n",
              "      <td>T3</td>\n",
              "      <td>5.83072</td>\n",
              "      <td>1167</td>\n",
              "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...</td>\n",
              "      <td>Reinforcement Learning for Mixed \\nOpen-loop and Closed-loop Control \\nEric A. Hansen, Andrew G. Barto, and Shlomo Zilbersteln \\nDepartment of Computer Science \\nUniversity of Massachusetts \\nAmhe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic4</th>\n",
              "      <td>T4</td>\n",
              "      <td>3.93349</td>\n",
              "      <td>1731</td>\n",
              "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
              "      <td>Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic5</th>\n",
              "      <td>T5</td>\n",
              "      <td>2.98750</td>\n",
              "      <td>33</td>\n",
              "      <td>hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, tr...</td>\n",
              "      <td>5O5 \\nCONNECTING TO THE PAST \\nBruce A. MacDonald, Assistant Professor \\nKnowledge Sciences Laboratory, Computer Science Department \\nThe University of Calgary, 2500 University Drive NW \\nCalgary,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic6</th>\n",
              "      <td>T6</td>\n",
              "      <td>7.51003</td>\n",
              "      <td>5</td>\n",
              "      <td>cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property</td>\n",
              "      <td>317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic7</th>\n",
              "      <td>T7</td>\n",
              "      <td>4.89525</td>\n",
              "      <td>1318</td>\n",
              "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
              "      <td>Comparison of Human and Machine Word \\nRecognition \\nM. Schenkel \\nDept of Electrical Eng. \\nUniversity of Sydney \\nSydney, NSW 2006, Australia \\nschenkel@sedal.usyd.edu.au \\nC. Latimer \\nDept of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic8</th>\n",
              "      <td>T8</td>\n",
              "      <td>3.67982</td>\n",
              "      <td>235</td>\n",
              "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
              "      <td>232 Sejnowski, Yuhas, Goldstein and Jenkins \\nCombining Visual and \\nwith a Neural Network \\nAcoustic Speech Signals \\nImproves Intelligibility \\nT.J. Sejnowski \\nThe Salk Institute \\nand \\nDepart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic9</th>\n",
              "      <td>T9</td>\n",
              "      <td>4.88831</td>\n",
              "      <td>948</td>\n",
              "      <td>control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant</td>\n",
              "      <td>An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic10</th>\n",
              "      <td>T10</td>\n",
              "      <td>2.95973</td>\n",
              "      <td>1690</td>\n",
              "      <td>circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp</td>\n",
              "      <td>Kirchoff Law Markov Fields for Analog \\nCircuit Design \\nRichard M. Golden * \\nRMG Consulting Inc. \\n2000 Fresno Road, Plano, Texas 75074 \\nRMG CONS UL T@A OL. COM, \\nwww. neural-network. corn \\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic11</th>\n",
              "      <td>T11</td>\n",
              "      <td>6.04910</td>\n",
              "      <td>987</td>\n",
              "      <td>spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution</td>\n",
              "      <td>Information through a Spiking Neuron \\nCharles F. Stevens and Anthony Zador \\nSalk Institute MNL/S \\nLa Jolla, CA 92037 \\nzador@salk.edu \\nAbstract \\nWhile it is generally agreed that neurons tran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic12</th>\n",
              "      <td>T12</td>\n",
              "      <td>6.18613</td>\n",
              "      <td>902</td>\n",
              "      <td>rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned</td>\n",
              "      <td>Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic13</th>\n",
              "      <td>T13</td>\n",
              "      <td>3.55312</td>\n",
              "      <td>1665</td>\n",
              "      <td>node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation</td>\n",
              "      <td>Boosting with Multi-Way Branching in \\nDecision Trees \\nYishay Mansour \\nDavid McAllester \\nAT&amp;T Labs-Research \\n180 Park Ave \\nFlorham Park NJ 07932 \\n{mansour, dmac}@research.att.com \\nAbstract ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic14</th>\n",
              "      <td>T14</td>\n",
              "      <td>3.98397</td>\n",
              "      <td>250</td>\n",
              "      <td>feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra</td>\n",
              "      <td>266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic15</th>\n",
              "      <td>T15</td>\n",
              "      <td>5.13757</td>\n",
              "      <td>679</td>\n",
              "      <td>classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian</td>\n",
              "      <td>A Boundary Hunting Radial Basis Function \\nClassifier Which Allocates Centers \\nConstructively \\nEric I. Chang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173-0073, USA \\nAb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic16</th>\n",
              "      <td>T16</td>\n",
              "      <td>2.71369</td>\n",
              "      <td>1124</td>\n",
              "      <td>distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum</td>\n",
              "      <td>Discovering Structure in Continuous \\nVariables Using Bayesian Networks \\nReimar Hofmann and Volker Tresp* \\nSiemens AG, Central Research \\nOtto-Hahn-Ring 6 \\n81730 Mfinchen, Germany \\nAbstract \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic17</th>\n",
              "      <td>T17</td>\n",
              "      <td>5.55943</td>\n",
              "      <td>1102</td>\n",
              "      <td>motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed</td>\n",
              "      <td>A model of transparent motion and \\nnon-transparent motion aftereffects \\nAlexander Grunewald* \\nMax-Planck Institut fiir biologische Kybernetik \\nSpemannstral]e 38 \\nD-72076 Tiibingen, Germany \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic18</th>\n",
              "      <td>T18</td>\n",
              "      <td>5.65911</td>\n",
              "      <td>537</td>\n",
              "      <td>object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint</td>\n",
              "      <td>Linear Operator for Object Recognition \\nPonen Basil Shimon Ullman* \\nM.I.T. Artificial Intelligence Laboratory \\nand Department of Brain and Cognitive Science \\n545 Technology Square \\nCambridge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic19</th>\n",
              "      <td>T19</td>\n",
              "      <td>3.73027</td>\n",
              "      <td>911</td>\n",
              "      <td>field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation</td>\n",
              "      <td>Ocular Dominance and Patterned Lateral \\nConnections in a Self-Organizing Model of the \\nPrimary Visual Cortex \\nJoseph Sirosh and Risto Miikkulainen \\nDepartment of Computer Sciences \\nUniversity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic20</th>\n",
              "      <td>T20</td>\n",
              "      <td>6.01090</td>\n",
              "      <td>72</td>\n",
              "      <td>memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol</td>\n",
              "      <td>73O \\nAnalysis of distributed representation of \\nconstituent structure in connectionist systems \\nPaul Smolensky \\nDepartment of Computer Science, University of Colorado, Boulder, CO 80309-0430 \\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topic  Max Score  Paper Num  \\\n",
              "Topic1              T1    1.64138        991   \n",
              "Topic2              T2    3.58149        383   \n",
              "Topic3              T3    5.83072       1167   \n",
              "Topic4              T4    3.93349       1731   \n",
              "Topic5              T5    2.98750         33   \n",
              "Topic6              T6    7.51003          5   \n",
              "Topic7              T7    4.89525       1318   \n",
              "Topic8              T8    3.67982        235   \n",
              "Topic9              T9    4.88831        948   \n",
              "Topic10            T10    2.95973       1690   \n",
              "Topic11            T11    6.04910        987   \n",
              "Topic12            T12    6.18613        902   \n",
              "Topic13            T13    3.55312       1665   \n",
              "Topic14            T14    3.98397        250   \n",
              "Topic15            T15    5.13757        679   \n",
              "Topic16            T16    2.71369       1124   \n",
              "Topic17            T17    5.55943       1102   \n",
              "Topic18            T18    5.65911        537   \n",
              "Topic19            T19    3.73027        911   \n",
              "Topic20            T20    6.01090         72   \n",
              "\n",
              "                                                                                                                                                                                                           Topic  \\\n",
              "Topic1                           bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
              "Topic2               neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing   \n",
              "Topic3   state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...   \n",
              "Topic4                                image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
              "Topic5   hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, tr...   \n",
              "Topic6                                       cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property   \n",
              "Topic7              word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state   \n",
              "Topic8                                       signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig   \n",
              "Topic9                            control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant   \n",
              "Topic10                                 circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp   \n",
              "Topic11              spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution   \n",
              "Topic12                       rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned   \n",
              "Topic13                      node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation   \n",
              "Topic14  feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra   \n",
              "Topic15     classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian   \n",
              "Topic16   distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum   \n",
              "Topic17                                     motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed   \n",
              "Topic18                    object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint   \n",
              "Topic19  field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation   \n",
              "Topic20  memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol   \n",
              "\n",
              "                                                                                                                                                                                                      Paper Name  \n",
              "Topic1   A Bound on the Error of Cross Validation Using \\nthe Approximation and Estimation Rates, with \\nConsequences for the Training-Test Split \\nMichael Kearns \\nAT&T Research \\nABSTRACT\\n1 INTRODUCTION...  \n",
              "Topic2   Signal Processing by Multiplexing and \\nDemultiplexing in Neurons \\nDavid C. Tam \\nDivision of Neuroscience \\nBaylor College of Medicine \\nHouston, TX 77030 \\ndtamCnext-cns.neusc.bcm.tmc.edu \\nAb...  \n",
              "Topic3   Reinforcement Learning for Mixed \\nOpen-loop and Closed-loop Control \\nEric A. Hansen, Andrew G. Barto, and Shlomo Zilbersteln \\nDepartment of Computer Science \\nUniversity of Massachusetts \\nAmhe...  \n",
              "Topic4   Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...  \n",
              "Topic5   5O5 \\nCONNECTING TO THE PAST \\nBruce A. MacDonald, Assistant Professor \\nKnowledge Sciences Laboratory, Computer Science Department \\nThe University of Calgary, 2500 University Drive NW \\nCalgary,...  \n",
              "Topic6   317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...  \n",
              "Topic7   Comparison of Human and Machine Word \\nRecognition \\nM. Schenkel \\nDept of Electrical Eng. \\nUniversity of Sydney \\nSydney, NSW 2006, Australia \\nschenkel@sedal.usyd.edu.au \\nC. Latimer \\nDept of ...  \n",
              "Topic8   232 Sejnowski, Yuhas, Goldstein and Jenkins \\nCombining Visual and \\nwith a Neural Network \\nAcoustic Speech Signals \\nImproves Intelligibility \\nT.J. Sejnowski \\nThe Salk Institute \\nand \\nDepart...  \n",
              "Topic9   An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...  \n",
              "Topic10  Kirchoff Law Markov Fields for Analog \\nCircuit Design \\nRichard M. Golden * \\nRMG Consulting Inc. \\n2000 Fresno Road, Plano, Texas 75074 \\nRMG CONS UL T@A OL. COM, \\nwww. neural-network. corn \\nA...  \n",
              "Topic11  Information through a Spiking Neuron \\nCharles F. Stevens and Anthony Zador \\nSalk Institute MNL/S \\nLa Jolla, CA 92037 \\nzador@salk.edu \\nAbstract \\nWhile it is generally agreed that neurons tran...  \n",
              "Topic12  Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...  \n",
              "Topic13  Boosting with Multi-Way Branching in \\nDecision Trees \\nYishay Mansour \\nDavid McAllester \\nAT&T Labs-Research \\n180 Park Ave \\nFlorham Park NJ 07932 \\n{mansour, dmac}@research.att.com \\nAbstract ...  \n",
              "Topic14  266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...  \n",
              "Topic15  A Boundary Hunting Radial Basis Function \\nClassifier Which Allocates Centers \\nConstructively \\nEric I. Chang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173-0073, USA \\nAb...  \n",
              "Topic16  Discovering Structure in Continuous \\nVariables Using Bayesian Networks \\nReimar Hofmann and Volker Tresp* \\nSiemens AG, Central Research \\nOtto-Hahn-Ring 6 \\n81730 Mfinchen, Germany \\nAbstract \\n...  \n",
              "Topic17  A model of transparent motion and \\nnon-transparent motion aftereffects \\nAlexander Grunewald* \\nMax-Planck Institut fiir biologische Kybernetik \\nSpemannstral]e 38 \\nD-72076 Tiibingen, Germany \\n...  \n",
              "Topic18  Linear Operator for Object Recognition \\nPonen Basil Shimon Ullman* \\nM.I.T. Artificial Intelligence Laboratory \\nand Department of Brain and Cognitive Science \\n545 Technology Square \\nCambridge...  \n",
              "Topic19  Ocular Dominance and Patterned Lateral \\nConnections in a Self-Organizing Model of the \\nPrimary Visual Cortex \\nJoseph Sirosh and Risto Miikkulainen \\nDepartment of Computer Sciences \\nUniversity...  \n",
              "Topic20  73O \\nAnalysis of distributed representation of \\nconstituent structure in connectionist systems \\nPaul Smolensky \\nDepartment of Computer Science, University of Colorado, Boulder, CO 80309-0430 \\...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD1aI6MDFrzp",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Topics for New Research Papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RP1qj5KFrzq",
        "colab_type": "code",
        "colab": {},
        "outputId": "e41c76b9-ed68-4543-82a4-aed29c2a659a"
      },
      "source": [
        "import glob\n",
        "# papers manually downloaded from NIPS 16\n",
        "# https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016\n",
        "\n",
        "new_paper_files = glob.glob('./test_data/nips16*.txt')\n",
        "new_papers = []\n",
        "for fn in new_paper_files:\n",
        "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
        "        data = f.read()\n",
        "        new_papers.append(data)\n",
        "              \n",
        "print('Total New Papers:', len(new_papers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total New Papers: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXy8rOuBFrzs",
        "colab_type": "code",
        "colab": {},
        "outputId": "15428bd0-f809-4b1f-bac3-8fee1c07401a"
      },
      "source": [
        "norm_new_papers = normalize_corpus(new_papers)\n",
        "cv_new_features = cv.transform(norm_new_papers)\n",
        "cv_new_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 14408)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_hih3Q2Frzv",
        "colab_type": "code",
        "colab": {},
        "outputId": "95a61db8-4dc4-460c-e98f-f73d1d888127"
      },
      "source": [
        "topic_predictions = nmf_model.transform(cv_new_features)\n",
        "best_topics = [[(topic, round(sc, 3)) \n",
        "                    for topic, sc in sorted(enumerate(topic_predictions[i]), \n",
        "                                            key=lambda row: -row[1])[:2]] \n",
        "                        for i in range(len(topic_predictions))]\n",
        "best_topics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1.312), (7, 0.966)],\n",
              " [(2, 4.121), (0, 0.864)],\n",
              " [(3, 2.154), (1, 1.335)],\n",
              " [(3, 3.074), (6, 2.19)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNfcRPgRFrzx",
        "colab_type": "code",
        "colab": {},
        "outputId": "dfcf1893-844f-4da8-9938-af685c81ff36"
      },
      "source": [
        "results_df = pd.DataFrame()\n",
        "results_df['Papers'] = range(1, len(new_papers)+1)\n",
        "results_df['Dominant Topics'] = [[topic_num+1 for topic_num, sc in item] for item in best_topics]\n",
        "res = results_df.set_index(['Papers'])['Dominant Topics'].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
        "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
        "results_df['Topic Score'] = [topic_sc for topic_list in \n",
        "                                        [[round(sc*100, 2) \n",
        "                                              for topic_num, sc in item] \n",
        "                                                 for item in best_topics] \n",
        "                                    for topic_sc in topic_list]\n",
        "\n",
        "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic'] for t in results_df['Dominant Topics'].values]\n",
        "results_df['Paper Desc'] = [new_papers[i-1][:200] for i in results_df.index.values]\n",
        "\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topics</th>\n",
              "      <th>Topic Score</th>\n",
              "      <th>Topic Desc</th>\n",
              "      <th>Paper Desc</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Papers</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>131.20000</td>\n",
              "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
              "      <td>Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>96.60000</td>\n",
              "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
              "      <td>Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>412.10000</td>\n",
              "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...</td>\n",
              "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>86.40000</td>\n",
              "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
              "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>215.40000</td>\n",
              "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
              "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>133.50000</td>\n",
              "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
              "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>307.40000</td>\n",
              "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
              "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>219.00000</td>\n",
              "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
              "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dominant Topics  Topic Score  \\\n",
              "Papers                                 \n",
              "1                     1    131.20000   \n",
              "1                     8     96.60000   \n",
              "2                     3    412.10000   \n",
              "2                     1     86.40000   \n",
              "3                     4    215.40000   \n",
              "3                     2    133.50000   \n",
              "4                     4    307.40000   \n",
              "4                     7    219.00000   \n",
              "\n",
              "                                                                                                                                                                                                     Topic Desc  \\\n",
              "Papers                                                                                                                                                                                                            \n",
              "1                               bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
              "1                                           signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig   \n",
              "2       state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...   \n",
              "2                               bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
              "3                                    image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
              "3                   neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing   \n",
              "4                                    image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
              "4                  word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state   \n",
              "\n",
              "                                                                                                                                                                                                     Paper Desc  \n",
              "Papers                                                                                                                                                                                                           \n",
              "1       Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...  \n",
              "1       Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...  \n",
              "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...  \n",
              "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...  \n",
              "3       Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...  \n",
              "3       Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...  \n",
              "4       Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...  \n",
              "4       Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2bSwtcYFrzz",
        "colab_type": "text"
      },
      "source": [
        "# Persisting Model and Transformers\n",
        "\n",
        "### This is just for visualizing the topics in the other notebook (since PyLDAViz expands the notebook size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVYlEcL4Frz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dill\n",
        "\n",
        "with open('nmf_model.pkl', 'wb') as f:\n",
        "    dill.dump(nmf_model, f)\n",
        "with open('cv_features.pkl', 'wb') as f:\n",
        "    dill.dump(cv_features, f)\n",
        "with open('cv.pkl', 'wb') as f:\n",
        "    dill.dump(cv, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}