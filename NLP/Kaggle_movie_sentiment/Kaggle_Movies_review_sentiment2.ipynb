{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movies_review_sentiment.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_movie_sentiment/Movies_review_sentiment2.ipynb",
      "authorship_tag": "ABX9TyNF808BvSoN1dqVUbsNr8od",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/NLP/Kaggle_movie_sentiment/Movies_review_sentiment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pZdaGiJRDCE",
        "colab_type": "text"
      },
      "source": [
        "# Prepare X_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1LtJ8a-WuP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORWZxYZWnzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "test_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/test.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_9x2xcHXO8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9fdc9786-cff6-431c-ee9f-06cd8ee38e10"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AV4Gg7XX_vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "61999790-c1a4-4f3d-f82d-1b6e9eb48dc6"
      },
      "source": [
        "train_df['Sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    79582\n",
              "3    32927\n",
              "1    27273\n",
              "4     9206\n",
              "0     7072\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiw_EN5lZjVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRxn1HsUaFvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b1edfe19-953c-4f1e-a8d0-898295182dcf"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         A series of escapades demonstrating the adage ...\n",
              "1         A series of escapades demonstrating the adage ...\n",
              "2                                                  A series\n",
              "3                                                         A\n",
              "4                                                    series\n",
              "                                ...                        \n",
              "156055                                            Hearst 's\n",
              "156056                            forced avuncular chortles\n",
              "156057                                   avuncular chortles\n",
              "156058                                            avuncular\n",
              "156059                                             chortles\n",
              "Name: Phrase, Length: 156060, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaT8XZqRaCCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1ffead9e-ff36-4511-8811-a74fbdf400c1"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         1\n",
              "1         2\n",
              "2         2\n",
              "3         2\n",
              "4         2\n",
              "         ..\n",
              "156055    2\n",
              "156056    1\n",
              "156057    3\n",
              "156058    2\n",
              "156059    2\n",
              "Name: Sentiment, Length: 156060, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4196zxaXZ3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "807b02fc-2737-42ef-dd75-1d88d52fecb0"
      },
      "source": [
        "test_df=pd.read_csv(test_path,sep='\\t')\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(66292, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>8545</td>\n",
              "      <td>An</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase\n",
              "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
              "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
              "2    156063        8545                                                 An\n",
              "3    156064        8545  intermittently pleasing but mostly routine effort\n",
              "4    156065        8545         intermittently pleasing but mostly routine"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcoTIoTZCvT",
        "colab_type": "text"
      },
      "source": [
        "Text have no label, so we split training set to train and text set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRjqjw3YJsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMJZjXxOZXlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4CHcYlYZqu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7806e8d8-2707-4af4-cadc-d3e191bab375"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDf2PKyGZyCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dbde6b27-f6c7-4048-8927-b2248492ef2e"
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509950\n",
              "3    0.210990\n",
              "1    0.174759\n",
              "4    0.058988\n",
              "0    0.045312\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK9lRF3zZ8MB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fa3eaa16-808b-4250-923a-0c16dbd04669"
      },
      "source": [
        "y_test.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509932\n",
              "3    0.210987\n",
              "1    0.174762\n",
              "4    0.058994\n",
              "0    0.045324\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4GK8KGaZVg",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification chap 8 Twitter_Sentiment analysis Orelilly NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXtMI2SGafbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install text_normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPP_3GpAa_Gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2dc6a858-19e8-450e-94be-5bb1f184d518"
      },
      "source": [
        "!pip install demoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/da/0b/d008f26ebbfd86d21117267e627f2f7359c76e5ecbeba08d8f631f4092c4/demoji-0.2.1-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji) (49.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.3 demoji-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ODLlPqFqL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8a5b7959-38da-4eec-d47c-6657e36f7b24"
      },
      "source": [
        "'''\n",
        "Contains helper funtions for preprocessing of twitter documents before getting their corresponding vectors\n",
        "from word2vec_twitter model\n",
        "Author: Anuj Gupta\n",
        "'''\n",
        "\n",
        "'''\n",
        "functionality:\n",
        "Discard non-english tweets\n",
        "Discard Replies\n",
        "Discard RTs\n",
        "For now this is handle while fetch tweet text from mongo documents\n",
        "process constantbrands mentions\n",
        "process any other mentions\n",
        "process constant brand name if any \n",
        "process hanstags\n",
        "process URLs\n",
        "process websites\n",
        "process process_EmailIds\n",
        "process \n",
        "process alphanums\n",
        "'''\n",
        "\n",
        "import re\n",
        "import string\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "#gobal\n",
        "PunctChars = r'''[`'“\".?!,:;]'''\n",
        "Punct = '%s+' % PunctChars\n",
        "Entity = '&(amp|lt|gt|quot);'\n",
        "printable = set(string.printable)\n",
        "\n",
        "# helper functoins\n",
        "def regex_or(*items):\n",
        "\tr = '|'.join(items)\n",
        "\tr = '(' + r + ')'\n",
        "\treturn r\n",
        "\n",
        "def pos_lookahead(r):\n",
        "\treturn '(?=' + r + ')'\n",
        "\n",
        "def neg_lookahead(r):\n",
        "\treturn '(?!' + r + ')'\n",
        "\n",
        "def optional(r):\n",
        "\treturn '(%s)?' % r\n",
        "\n",
        "def trim(transient_tweet_text):\n",
        "\t''' \n",
        "\ttrim leading and trailing spaces in the tweet text\n",
        "\t'''\n",
        "\treturn transient_tweet_text.strip()\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def to_LowerCase(transient_tweet_text):\n",
        "\t'''\n",
        "\tConvert tweet text to lower to lower case alphabets\n",
        "\t'''\n",
        "\ttransient_tweet_text = transient_tweet_text.lower()\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_unicode(transient_tweet_text):\n",
        "    '''\n",
        "    Strip all unicode characters from a tweet\n",
        "    '''\n",
        "    tweet = ''.join(i for i in transient_tweet_text if ord(i)<128)\n",
        "    return tweet \n",
        "\n",
        "def process_URLs(transient_tweet_text):\n",
        "\t'''\n",
        "\treplace all URLs in the tweet text\n",
        "\t'''\n",
        "\tUrlStart1 = regex_or('https?://', r'www\\.',r'bit.ly/')\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tUrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + pos_lookahead(r'[/ \\W\\b]')\n",
        "\tUrlBody = r'[^ \\t\\r\\n<>]*?'  # * not + for case of:  \"go to bla.com.\" -- don't want period\n",
        "\tUrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n",
        "\tUrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n",
        "\tUrl = \t(optional(r'\\b') + \n",
        "    \t\tregex_or(UrlStart1, UrlStart2) + \n",
        "    \t\tUrlBody + \n",
        "    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n",
        "\n",
        "\tUrl_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(Url_RE, \" constanturl \", transient_tweet_text)\n",
        "\n",
        "\t# fix to handle unicodes in URL\n",
        "\tURL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*'\n",
        "\ttransient_tweet_text = re.sub(URL_regex2, \" constanturl \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Websites(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify website mentioned if any \n",
        "\t'''\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tsep = r'[.]'\n",
        "\twebsite_regex = r'(?<!#)?(\\b)?[a-zA-Z0-9.]+' + sep + CommonTLDs\n",
        "\twebsite_RE = re.compile(\"(%s)\" % website_regex, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(website_RE, ' constantwebsite ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_EmailIds(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify email mentioned if any\n",
        "\t'''\n",
        "\temail_regex = r'(\\b)?[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(\\b)?'\n",
        "\ttransient_tweet_text = re.sub(email_regex, ' constantemailid ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Mentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify mentions if any\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", \" constantnonbrandmention \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "def process_HashTags(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all Hashtags from a tweet\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"#(\\w+)\\b\", ' constanthashtag ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Dates(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify date and convert it to constant\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'(\\d+/\\d+/\\d+)', \" constantdate \" , transient_tweet_text)\n",
        "\t#transient_tweet_text = re.sub(r'constantnum[\\s]?(/|-)[\\s]?constantnum[\\s]?(/|-)[\\s]?constantnum', \" constantdate \" , transient_tweet_text)\n",
        "\t#date_regex = r'(constantnum)[\\s]*(st|nd|rd|th)[\\s]*(january|jan|february|feb|march|mar|april|may|june|jun|july|august|aug|september|sep|october|oct|november|nov|december|dec)'\n",
        "\tdate_regex1 = r'\\b((0|1|2|3)?[0-9][\\s]*)[-./]([\\s]*([012]?[0-9])[\\s]*)([-./]([\\s]*(19|20)[0-9][0-9]))?\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex1, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex2 = r'\\b((19|20)[0-9][0-9][\\s]*[-./]?)?[\\s]*([012]?[0-9])[\\s]*[-./][\\s]*(0|1|2|3)?[0-9]\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex2, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tMonths = regex_or('january','jan','february','feb','march','mar','april','may','june','jun','july','jul','august','aug','september','sep','october','oct','november','nov','december','dec')\n",
        "\tdate_regex3 = r'\\d+[\\s]*(st|nd|rd|th)[\\s]*' + Months \n",
        "\ttransient_tweet_text = re.sub(date_regex3, ' constantdate ', transient_tweet_text)\n",
        "\tdate_regex4 = Months + r'[\\s]*\\d+[\\s]*(st|nd|rd|th)*\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex4, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex5 = r'[\\s]?\\b(19|20)[0-9][0-9]\\b[\\s]?'\n",
        "\ttransient_tweet_text = re.sub(date_regex5, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tdate_regex6 = r'([\\s]*(constantdate))+'\n",
        "\ttransient_tweet_text = re.sub(date_regex6, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Times(transient_tweet_text):\n",
        "\t'''\n",
        "\tIndentify time and convert it to constant\n",
        "\t'''\n",
        "\ttime_regex1 = r'([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9][\\s]*(am|pm)?[\\s]*([iescm](st)|gmt|utc|[pmce](dt))?'\n",
        "\ttransient_tweet_text = re.sub(time_regex1, ' constanttime ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com','shopohelp','shopo','mobikwikswat',\n",
        "\t\t'mobikwik','taxiforsure','tfscares','zoomcarindia','freshmenuindia','freshmenucares','grofers','jetairways',\n",
        "\t\t'cleartrip','olacabs','support_ola','makemytrip','makemytripcare','chai_point')\n",
        "\n",
        "\tBrandMentionRegex = r'@(\\b)*' + constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandMentionRegex, ' constantbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_NonBrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all Mentions left, if any, in tweet text\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", ' constantnonbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandName(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com', 'bluestone','shopohelp','shopo',\n",
        "\t\t'mobikwikswat','mobikwik','taxiforsure','tfscares', 'tfs','zoomcarindia', 'zoomcar','freshmenuindia','freshmenucares', \n",
        "\t\t'freshmenu','grofers','jetairways', 'jet','cleartrip','olacabs','support_ola', 'olacab', 'ola' ,'makemytrip',\n",
        "\t\t'makemytripcare', 'chai_point')\n",
        "\n",
        "\tBrandNameRegex = constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandNameRegex, ' constantbrandname ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_Savings(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify sale/save offers\n",
        "\t'''\n",
        "\t#sale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*(constantnum)*[\\s]*[%]?[\\s]*(-|~)?[\\s]*(constantnum)*[\\s]*[%]?'\n",
        "\tsale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(-|~|or)?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?'\n",
        "\ttransient_tweet_text = re.sub(sale_regex, \" constantdiscount \", transient_tweet_text)\n",
        "\t#discount_List = []\n",
        "\t#discount_List = re.findall(r'constantdiscount', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Offers(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify cashbacks and off / substrings of the form \"30% off\" or \"30% cashback\" or \"$30 off\"\n",
        "\tReplace them by constantOFFER\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'[rs|$]?[ ]*[constantnum][ ]*[%]?[ ]?[off|cashback|offer]', \"constantoffer\", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:(up[\\s]?to)?((rs|\\$)*[\\s]*(constantnum))[\\s]*[%]?)?[\\s]*[-|~|or]?[\\.]?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(offer|off|cashback|cash|cash back)', \" constantoffer \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:cashback|cash back|cash)\\b', \" constantoffer \", transient_tweet_text)\n",
        "\t#Offer_List = []\n",
        "\t#Offer_List = re.findall(r'constantoffer', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Promos(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify coupons/promos with promo codes\n",
        "\tAssumption - promo code can be alphanumeric. But it immediately follows text of promo/code/promocode etc\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'\\b(promocode|promo code|promo|code)[s]?[\\s]*[a-z]*(constantnum)*[a-z]*[\\s]+', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*(constantalphanum)\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[a-z]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[0-9]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code|coupon)[s]?\\b', \" constantpromo \", transient_tweet_text)\n",
        "\t#Promo_List = []\n",
        "\t#Promo_List = re.findall(r'constantpromo', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Money(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify money in the tweet text but outside offers. This includes $,Rs, pound, Euro\n",
        "\t'''\n",
        "\tmoney_regex1 = r'\\b(rs|\\$)[\\s]*(constantnum)?[\\.]?[\\s]*constantnum\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex1, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex2 = r'[\\s]*[\\.]?[\\s]*constantnum(cent[s]?|\\$|c)\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex2, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex3 = r'(\\$|rs)[\\s]*constantalphanum'\n",
        "\ttransient_tweet_text = re.sub(money_regex3, \" constantmoney \", transient_tweet_text)\n",
        "\t#Money_List = []\n",
        "\t#Money_List = re.findall(r'constantmoney', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_freebies(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify freebies in tweets if any - free offers, free shipping, free trial, \n",
        "\t'''\n",
        "\tfreebies_regex1 = r'(?<!#)\\b(?:free)[\\s]+[a-z]+\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex1, \" constantfreebies \", transient_tweet_text)\n",
        "\tfreebies_regex2 = r'(?<!#)\\b(?:free)[\\s]+[a-z]*\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex2, \" constantfreebies \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def replace_numbers(transient_tweet_text):\n",
        "\t'''\n",
        "\tGiven any number/interger in tweet text, we want it to be replaced by constantnum\n",
        "\t'''\n",
        "\t# we want to process only those numbers that are not in a hashtag - below logic does this\n",
        "\tnum_regex = r'(?<!#)\\b(?:[-+]?[\\d,]*[\\.]?[\\d,]*[\\d]+|\\d+)\\b'\n",
        "\ttransient_tweet_text = re.sub(num_regex, \" constantnum \" , transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_AlphaNumerics(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify alpha numerics - this helps in identifying product codes/models, promocodes, Order IDs\n",
        "\t'''\n",
        "\tAlphaNumeric_regex = r'(?<!#)\\b(?:([a-z]+[0-9]+[a-z]*|[a-z]*[0-9]+[a-z]+)[a-z,0-9]*)\\b'\n",
        "\ttransient_tweet_text = re.sub(AlphaNumeric_regex, \" constantalphanum \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_emoji(transient_tweet_text):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokenized_tweet = tweet_tokenizer.tokenize(transient_tweet_text)\n",
        "    emojis_present = demoji.findall(transient_tweet_text)\n",
        "    tweet_no_emoji=''\n",
        "    for i,s in enumerate(tokenized_tweet):\n",
        "        if s in emojis_present.keys():\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + emojis_present[s]\n",
        "        else:\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + s\n",
        "    return tweet_no_emoji\n",
        "\n",
        "def deEmojify(transient_tweet_text):\n",
        "    return transient_tweet_text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "# def print_test():\n",
        "    \n",
        "#     test_tweet = \"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"\n",
        "#     #General Proprocessing\n",
        "#     test_tweet = test_tweet.lower()\n",
        "#     test_tweet = strip_unicode(test_tweet)\n",
        "    \n",
        "#     #function tests\n",
        "#     print(\"Process URLS:\\n\",process_URLs(test_tweet))\n",
        "#     print(\"Remove websites:\\n\",process_Websites(test_tweet))\n",
        "#     print(\"Remove mentions:\\n\",process_Mentions(test_tweet))\n",
        "#     print('Remove Emailid:\\n',process_EmailIds(test_tweet))\n",
        "#     print(\"Remove Hashtags:\\n\",process_HashTags(test_tweet))\n",
        "#     print(\"Remove Dates:\\n\",process_Dates(test_tweet))\n",
        "#     print(\"Process Time:\\n\",process_Times(test_tweet))\n",
        "#     print(\"Process Brand Mention:\\n\",process_BrandMentions(test_tweet))\n",
        "#     print(\"Process non Brand Mention:\\n\",process_NonBrandMentions(test_tweet))\n",
        "#     print(\"Process Brand Name:\\n\",process_BrandName(test_tweet))\n",
        "#     print(\"Process Savings:\\n\",identify_Savings(test_tweet))\n",
        "#     print(\"Process Offers:\\n\",indentify_Offers(test_tweet))\n",
        "#     print(\"Identiy Promos:\\n\",indentify_Promos(test_tweet))\n",
        "# ############\n",
        "# print_test()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_TweetText(tweet_text):\n",
        "\t'''\n",
        "\tTakes tweet_text and preprocesses it \n",
        "\tOrder of preprocessing:\n",
        "\t'''\n",
        "\n",
        "\t# get utf-8 encoding, lowercase, trim and remove multiple white spaces\n",
        "\ttransient_tweet_text = tweet_text\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\t#print \"PROCESSED: \", transient_tweet_text\n",
        "\n",
        "\ttransient_tweet_text = to_LowerCase(transient_tweet_text)\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\ttransient_tweet_text = remove_spl_words(transient_tweet_text)\n",
        "\n",
        " \n",
        "\t#emoji\n",
        "\ttransient_tweet_text = remove_emoji(transient_tweet_text) \n",
        "\ttransient_tweet_text = deEmojify(transient_tweet_text)\n",
        "\t# process Hastags, URLs, Websites, process_EmailIds\n",
        "\t# Give precedence to url over hashtag\n",
        "\ttransient_tweet_text = process_URLs(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_HashTags(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_Websites(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_EmailIds(transient_tweet_text)\n",
        "\n",
        "\t# process for brand mention, any other mention and brand Name\n",
        "\t#transient_tweet_text = process_BrandMentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_NonBrandMentions(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Mentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_BrandName(transient_tweet_text)\n",
        "\n",
        "\t# remove any unicodes\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\n",
        "\t# identify Date / Time if any\n",
        "\ttransient_tweet_text = process_Times(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Dates(transient_tweet_text)\n",
        "\n",
        "\t# indentify alphanums and nums\n",
        "\ttransient_tweet_text = identify_AlphaNumerics(transient_tweet_text)\n",
        "\ttransient_tweet_text = replace_numbers(transient_tweet_text)\n",
        "\t\n",
        "\t# identify promos, savings, offers, money and freebies\n",
        "\ttransient_tweet_text = indentify_Promos(transient_tweet_text)\n",
        "\ttransient_tweet_text = identify_Savings(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Offers(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Money(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_freebies(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = prune_multple_consecutive_same_char(transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# print(process_TweetText(\"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.26 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTAVLjkqFtNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1fffd566-ea15-4a15-96ab-8c44245d26d5"
      },
      "source": [
        "#Making the necessary imports\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#preprocessing_path = \"/home/etherealenvy/Downloads/practical-nlp/Ch8/O5_smtd_preprocessing.py\"\n",
        "#sys.path.append(os.path.abspath(preprocessing_path))\n",
        "\n",
        "#import O5_smtd_preprocessing\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrtahTLTG5MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9aebed88-ff38-4be9-94a9-4486e2dbcfcf"
      },
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "X_train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142974</th>\n",
              "      <td>Laced with liberal doses of dark humor , gorge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78231</th>\n",
              "      <td>Offers absolutely nothing I had n't already se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7375</th>\n",
              "      <td>Ivan is a prince of a fellow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115288</th>\n",
              "      <td>the stomach-knotting suspense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>Quirky</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>109242 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase\n",
              "150238  make you wish you were at home watching that m...\n",
              "133360  , the tale has turned from sweet to bitterswee...\n",
              "49191   can say that about most of the flicks moving i...\n",
              "137709                   Does n't deliver a great story ,\n",
              "73297                                  unrecoverable life\n",
              "...                                                   ...\n",
              "142974  Laced with liberal doses of dark humor , gorge...\n",
              "78231   Offers absolutely nothing I had n't already se...\n",
              "7375                         Ivan is a prince of a fellow\n",
              "115288                      the stomach-knotting suspense\n",
              "2467                                               Quirky\n",
              "\n",
              "[109242 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_FCZjmtGGQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5ff9960c-6a8b-49c7-d3cc-e4f12fc6e76b"
      },
      "source": [
        "X_train_df['Phrase'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['make you wish you were at home watching that movie instead of in the theater watching this one',\n",
              "       ', the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .',\n",
              "       'can say that about most of the flicks moving in and out of the multiplex',\n",
              "       ..., 'Ivan is a prince of a fellow',\n",
              "       'the stomach-knotting suspense', 'Quirky'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T75lcJa9GiOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3a551c8f-db48-41f1-f95a-a51e3d6255a6"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150238    1\n",
              "133360    4\n",
              "49191     2\n",
              "137709    1\n",
              "73297     1\n",
              "         ..\n",
              "142974    4\n",
              "78231     1\n",
              "7375      3\n",
              "115288    3\n",
              "2467      2\n",
              "Name: Sentiment, Length: 109242, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3216ea7FvXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_df['Phrase_process'] = X_train_df['Phrase'].apply(lambda x: process_TweetText(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EV5FOdGIDNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8196e617-c2b7-4ca8-ab40-7022ede73b28"
      },
      "source": [
        "X_train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "      <td>does n't deliver a great story ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>unrecoverable life</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase                                     Phrase_process\n",
              "150238  make you wish you were at home watching that m...  make you wish you were at home watching that m...\n",
              "133360  , the tale has turned from sweet to bitterswee...  , the tale has turned from sweet to bitterswee...\n",
              "49191   can say that about most of the flicks moving i...  can say that about most of the flicks moving i...\n",
              "137709                   Does n't deliver a great story ,                   does n't deliver a great story ,\n",
              "73297                                  unrecoverable life                                 unrecoverable life"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPdGgTRuHz4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_df['phrase_tokens'] = X_train_df['Phrase_process'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
        "X_train_df['phrase_no_stopwords'] = X_train_df['phrase_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCWK_obIcHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "bd227208-3d0a-4faf-9651-72fe7ef8bc18"
      },
      "source": [
        "X_train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "      <th>phrase_tokens</th>\n",
              "      <th>phrase_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>[make, you, wish, you, were, at, home, watchin...</td>\n",
              "      <td>[make, wish, home, watching, movie, instead, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>[,, the, tale, has, turned, from, sweet, to, b...</td>\n",
              "      <td>[,, tale, turned, sweet, bittersweet, ,, tears...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>[can, say, that, about, most, of, the, flicks,...</td>\n",
              "      <td>[say, flicks, moving, multiplex]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "      <td>does n't deliver a great story ,</td>\n",
              "      <td>[does, n't, deliver, a, great, story, ,]</td>\n",
              "      <td>[n't, deliver, great, story, ,]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>[unrecoverable, life]</td>\n",
              "      <td>[unrecoverable, life]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase  ...                                phrase_no_stopwords\n",
              "150238  make you wish you were at home watching that m...  ...  [make, wish, home, watching, movie, instead, t...\n",
              "133360  , the tale has turned from sweet to bitterswee...  ...  [,, tale, turned, sweet, bittersweet, ,, tears...\n",
              "49191   can say that about most of the flicks moving i...  ...                   [say, flicks, moving, multiplex]\n",
              "137709                   Does n't deliver a great story ,  ...                    [n't, deliver, great, story, ,]\n",
              "73297                                  unrecoverable life  ...                              [unrecoverable, life]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQJi0JZKNmPt",
        "colab_type": "text"
      },
      "source": [
        "## Train your own Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCM4cUNjNtjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase_processed=X_train_df['Phrase_process'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zBH1EprIaym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0023cd61-a00e-4a56-df68-b8028bca96c7"
      },
      "source": [
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(phrase_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW Model Training Complete.\n",
            "Time taken for training is:7.45960 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y78qQja0N2-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba210c79-0da4-42cb-afb8-3694126b7aec"
      },
      "source": [
        "#Create document vectors by averaging word vectors.\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 100\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this/count_for_this if count_for_this > 0 else feat_for_this)        \n",
        "    return feats\n",
        "\n",
        "train_vectors = embedding_feats(X_train_df['phrase_no_stopwords'].values)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT1q2Z8INxV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1aadb69e-7079-49f9-d294-78595bd858fb"
      },
      "source": [
        "#Take any classifier (LogisticRegression here)\n",
        "classifier = LogisticRegression(random_state=2020)\n",
        "classifier.fit(train_vectors, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=2020, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soCmXeipOSal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "18423103-7124-4c82-bd26-3541df3aed0d"
      },
      "source": [
        "X_test_df=pd.DataFrame(X_test)\n",
        "X_test_df['Phrase_process'] = X_test_df['Phrase'].apply(lambda x: process_TweetText(x))\n",
        "X_test_df['phrase_tokens'] = X_test_df['Phrase_process'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
        "X_test_df['phrase_no_stopwords'] = X_test_df['phrase_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
        "X_test_df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "      <th>phrase_tokens</th>\n",
              "      <th>phrase_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105687</th>\n",
              "      <td>seems just as expectant of an adoring , wide-s...</td>\n",
              "      <td>seems just as expectant of an adoring , wide-s...</td>\n",
              "      <td>[seems, just, as, expectant, of, an, adoring, ...</td>\n",
              "      <td>[seems, expectant, adoring, ,, wide-smiling, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133291</th>\n",
              "      <td>the dream world of teen life ,</td>\n",
              "      <td>the dream world of teen life ,</td>\n",
              "      <td>[the, dream, world, of, teen, life, ,]</td>\n",
              "      <td>[dream, world, teen, life, ,]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36948</th>\n",
              "      <td>purpose and finesse</td>\n",
              "      <td>purpose and finesse</td>\n",
              "      <td>[purpose, and, finesse]</td>\n",
              "      <td>[purpose, finesse]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101915</th>\n",
              "      <td>Argento , at only 26</td>\n",
              "      <td>argento , at only constantnum</td>\n",
              "      <td>[argento, ,, at, only, constantnum]</td>\n",
              "      <td>[argento, ,, constantnum]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150567</th>\n",
              "      <td>you may ask</td>\n",
              "      <td>you may ask</td>\n",
              "      <td>[you, may, ask]</td>\n",
              "      <td>[may, ask]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase  ...                                phrase_no_stopwords\n",
              "105687  seems just as expectant of an adoring , wide-s...  ...  [seems, expectant, adoring, ,, wide-smiling, r...\n",
              "133291                     the dream world of teen life ,  ...                      [dream, world, teen, life, ,]\n",
              "36948                                 purpose and finesse  ...                                 [purpose, finesse]\n",
              "101915                               Argento , at only 26  ...                          [argento, ,, constantnum]\n",
              "150567                                        you may ask  ...                                         [may, ask]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw_Lz-BNPig8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dfb542f5-3940-47c1-e3a1-a34b906ba13d"
      },
      "source": [
        "phrase_processed=X_test_df['Phrase_process'].values\n",
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(phrase_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW Model Training Complete.\n",
            "Time taken for training is:3.24631 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9qoVDD0PW6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2164b72-403d-45f2-c5a1-027b989b23a7"
      },
      "source": [
        "test_vectors = embedding_feats(X_test_df['phrase_no_stopwords'].values)\n",
        "print(len(test_vectors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2QtcKeRPrSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "605d1991-0764-47c4-bbad-efd3e81f3764"
      },
      "source": [
        "preds = classifier.predict(test_vectors)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.13      0.12      2122\n",
            "           1       0.19      0.13      0.15      8182\n",
            "           2       0.53      0.84      0.65     23874\n",
            "           3       0.18      0.02      0.03      9878\n",
            "           4       0.00      0.00      0.00      2762\n",
            "\n",
            "    accuracy                           0.46     46818\n",
            "   macro avg       0.20      0.22      0.19     46818\n",
            "weighted avg       0.35      0.46      0.37     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsYInHiTSUBl",
        "colab_type": "text"
      },
      "source": [
        "# Deep learning (Chap 4 classification Oreillly NLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxK_OYasYMkM",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ksS0pLDQAw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_58AP4rSg35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXF3sxVPSzN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "test_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/test.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYCNgHLrS-0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3a9622d4-ee32-4081-abd1-54ae47a068b9"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FWeIEmXTAqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5cb4fddf-51f0-4417-ff40-5cd9eb9c0d74"
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n36erOnFTScR",
        "colab_type": "text"
      },
      "source": [
        "## 1. Tokenize the texts and convert them into word index vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjot__KCTbvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1e0bdddc-3d8a-4ec1-dcbc-5d2a3c1833cf"
      },
      "source": [
        "X_train.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['make you wish you were at home watching that movie instead of in the theater watching this one',\n",
              "       ', the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .',\n",
              "       'can say that about most of the flicks moving in and out of the multiplex',\n",
              "       ..., 'Ivan is a prince of a fellow',\n",
              "       'the stomach-knotting suspense', 'Quirky'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv-TZ9w0TBmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c804bd2-46e0-445a-e1d2-4d074e2e4753"
      },
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) #20000\n",
        "tokenizer.fit_on_texts(X_train.values)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train.values) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test.values)\n",
        "word_index = tokenizer.word_index #(X_train, luc nay chua phan ra X_train_new va X_valid)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "# max 20000 words keep in sequences, word index is just a show case, not use (> max words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15264 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPhIB9eCTkk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ee3af20-746a-4151-d2f0-ca81447702c9"
      },
      "source": [
        "print(train_sequences[0]) # max 10000 words in sequences\n",
        "print(len(train_sequences[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[77, 22, 970, 22, 175, 30, 448, 227, 9, 17, 378, 3, 7, 1, 317, 227, 18, 26]\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xmcdp8Ty6I",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 2. Pad the text sequences so that all text vectors are of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_NNjyMiTtRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "train_sequences_padding = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) #1000\n",
        "test_sequences_padding = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) #1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_-i79_mUB7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f18b52ec-0fc5-4675-ba30-38cf85b15168"
      },
      "source": [
        "#Before transform\n",
        "print(len(train_sequences)) # train_texts --> train_sequences\n",
        "print(len(train_sequences[0]))\n",
        "print(train_sequences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109242\n",
            "18\n",
            "[77, 22, 970, 22, 175, 30, 448, 227, 9, 17, 378, 3, 7, 1, 317, 227, 18, 26]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDu5J-nJUDTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "65f9bf72-001b-4354-8148-1bbb0949f8a4"
      },
      "source": [
        "#After transform\n",
        "print(train_sequences_padding.shape)\n",
        "train_sequences_padding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242, 1000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  227,   18,   26],\n",
              "       [   0,    0,    0, ...,  140, 1487, 3657],\n",
              "       [   0,    0,    0, ...,    3,    1, 4473],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    3,    2, 2915],\n",
              "       [   0,    0,    0, ..., 1308, 9589,  436],\n",
              "       [   0,    0,    0, ...,    0,    0,  429]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-dcDpkvUV4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "07c7b201-6609-4c18-956a-d081cd23e16b"
      },
      "source": [
        "train_labels = to_categorical(np.asarray(y_train))\n",
        "test_labels = to_categorical(np.asarray(y_test))\n",
        "\n",
        "print(y_train[:3])\n",
        "print(train_labels.shape)\n",
        "train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150238    1\n",
            "133360    4\n",
            "49191     2\n",
            "Name: Sentiment, dtype: int64\n",
            "(109242, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4eO9vYuU7BT",
        "colab_type": "text"
      },
      "source": [
        "## Split the training data into a training set and a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFsSgxNGclHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b53b30bb-9cdf-445f-d61a-0e688dd09432"
      },
      "source": [
        "X_test=test_sequences_padding\n",
        "y_test=test_labels\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(46818, 1000)\n",
            "(46818, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxzb9ztBUn21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a41977f0-176a-4685-b765-a0c73446f196"
      },
      "source": [
        "X_train_new,X_valid,y_train_new,y_valid=train_test_split(train_sequences_padding,train_labels,\n",
        "                                                         test_size=0.3,\n",
        "                                               stratify=train_labels,random_state=42)\n",
        "print(X_train_new.shape)\n",
        "print(y_train_new.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76469, 1000)\n",
            "(76469, 5)\n",
            "(32773, 1000)\n",
            "(32773, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBfYWJRYZ0O2",
        "colab_type": "text"
      },
      "source": [
        "##3. Map every word index to an embedding vector (for pre-train model| can skip if no need pre train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9R4jMVZouf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "7d0f11b9-0b12-4e64-d045-0038a74561ef"
      },
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/Data/NLP/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # vocabulary name\n",
        "        coefs = np.asarray(values[1:], dtype='float32') # embedding 100 dim\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "print(embeddings_index[\"google\"])\n",
        "print(len(embeddings_index[\"google\"]))\n",
        "print(MAX_NUM_WORDS)\n",
        "print(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "[ 0.22575  -0.56253  -0.05156  -0.079389  1.1876   -0.48397  -0.23342\n",
            " -0.85278   0.97495  -0.33344   0.71692   0.12644   0.31962  -1.4136\n",
            " -0.57903  -0.037286 -0.0164    0.45155  -0.29005   0.52599  -0.22534\n",
            " -0.29556  -0.032407  1.5608   -0.013499 -0.064558  0.26625   0.78595\n",
            " -0.71693  -0.93025   0.80461   1.6035   -0.30602  -0.34764   0.93872\n",
            "  0.38137  -0.26743  -0.56519   0.58899  -0.14554  -0.34324   0.21291\n",
            " -0.39887   0.090042 -0.8495    0.38803  -0.5045   -0.22488   1.0644\n",
            " -0.2624    1.0334    0.06348  -0.39989   0.24236  -0.65636  -1.8107\n",
            " -0.061801  0.13795   1.1658   -0.30046  -0.50143   0.16509   0.039835\n",
            "  0.62541   0.56935   0.64125   0.21308   0.30276   0.39673   0.38973\n",
            "  0.28183   0.79481  -0.11962  -0.49598  -0.53195  -0.14897   0.51254\n",
            " -0.39208  -0.58535  -0.078509  0.81721  -0.73497  -0.68131   0.099243\n",
            " -0.87608   0.029632  0.33402  -0.14305   0.16964  -0.035178  0.39777\n",
            "  0.71769   0.25867  -0.36201   0.45698  -0.39156  -0.49343  -0.11224\n",
            "  0.29046   0.73216 ]\n",
            "100\n",
            "20000\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8uhTlrDaDHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "223e529a-5580-4eae-d4c5-f5422945fb92"
      },
      "source": [
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "print(num_words)\n",
        "print(embedding_matrix.shape)\n",
        "print(len(word_index)) # max 20000 tu hay gap nhat thoi, index nay khong xai het\n",
        "print(len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15265\n",
            "(15265, 100)\n",
            "15264\n",
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNv0IKplaQZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS: # chi thuc hien get embedding word < max_num_words(20000)\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word) #lay 100d embedding cho tung tu\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstnO3lKaZOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "066289c4-540d-426a-f878-cbaacfa6945a"
      },
      "source": [
        "print(embedding_matrix.shape)\n",
        "embedding_matrix[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15265, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
              "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
              "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
              "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
              "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
              "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
              "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
              "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
              "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
              "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
              "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
              "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
              "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
              "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
              "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
              "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
              "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
              "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
              "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
              "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYuYJgWZQNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4830f85e-05c7-49dd-ea02-0a3fdc7a78e5"
      },
      "source": [
        "labels_index=len(np.unique(y))\n",
        "labels_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPCsv-Upcp44",
        "colab_type": "text"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQX8E0XHvtpb",
        "colab_type": "text"
      },
      "source": [
        "## Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcaa_Qkpy0ZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_test = test_sequences_padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp3NOeAKcs5y",
        "colab_type": "text"
      },
      "source": [
        "### 1D CNN Model with pre-trained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znReffWAaajm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78d10e12-6be9-4086-edcd-16c5c1973508"
      },
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(input_dim=num_words,#15265 (max 20000)\n",
        "                            output_dim=EMBEDDING_DIM, #100\n",
        "                            embeddings_initializer=Constant(embedding_matrix), #(15265, 100)\n",
        "                            input_length=MAX_SEQUENCE_LENGTH, #1000\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSqhIrVduB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b31dd077-8c41-48d0-f9b6-bef85155563f"
      },
      "source": [
        "labels_index=len(np.unique(y))\n",
        "labels_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bRn1CR5afRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "ff46a351-9520-4cb9-f42a-f2f0c9755518"
      },
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(labels_index, activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "cnnmodel.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Define a 1D CNN model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 100)         1526500   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 996, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 195, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 39, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 35, 128)           82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,771,881\n",
            "Trainable params: 245,381\n",
            "Non-trainable params: 1,526,500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r2JxL9wc0rL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "49f4c43f-6865-4e4f-ca79-991d82c23978"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "cnnmodel.fit(X_train_new, y_train_new,\n",
        "          batch_size=200,\n",
        "          epochs=1, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 22s 59ms/step - loss: 1.1060 - acc: 0.5578 - val_loss: 1.0369 - val_acc: 0.5784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8b9d18d780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zCciAlIer-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c54b3fd7-e48a-43f7-e62d-1237a7d6d166"
      },
      "source": [
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_sequences_padding, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1464/1464 [==============================] - 10s 7ms/step - loss: 1.0420 - acc: 0.5764\n",
            "Test accuracy with CNN: 0.5764449834823608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUJeNgcAlkZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcd8d411-f8c8-4fb2-ace4-79928bf4f139"
      },
      "source": [
        "y_pred=cnnmodel.predict_classes(test_sequences_padding)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 1, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNx6ytuPlzAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d73b88-4075-4045-a3a5-6385cd1170a9"
      },
      "source": [
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "test_labels_reverse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, ..., 3, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZYFJsYjt-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8aa43407-fa1d-4983-b6fe-eff9b5645cd1"
      },
      "source": [
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.35      0.31      2122\n",
            "           1       0.39      0.26      0.31      8182\n",
            "           2       0.65      0.85      0.74     23874\n",
            "           3       0.51      0.36      0.42      9878\n",
            "           4       0.66      0.09      0.15      2762\n",
            "\n",
            "    accuracy                           0.58     46818\n",
            "   macro avg       0.50      0.38      0.39     46818\n",
            "weighted avg       0.56      0.58      0.54     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNAfM91cgRwP",
        "colab_type": "text"
      },
      "source": [
        "### 1D CNN model with training your own embedding¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qVjvVghgSSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn4fIB6UgT0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "ff04101f-1d28-452b-9355-cf44c472f42c"
      },
      "source": [
        "\n",
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel2 = Sequential()\n",
        "cnnmodel2.add(Embedding(MAX_NUM_WORDS, 128)) # co the thieu input length = MAX_SEQUENCE_LENGTH (1000)\n",
        "cnnmodel2.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel2.add(MaxPooling1D(5))\n",
        "cnnmodel2.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel2.add(MaxPooling1D(5))\n",
        "cnnmodel2.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel2.add(GlobalMaxPooling1D())\n",
        "cnnmodel2.add(Dense(128, activation='relu'))\n",
        "cnnmodel2.add(Dense(labels_index, activation='softmax'))\n",
        "\n",
        "cnnmodel2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "print(cnnmodel2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 2,823,301\n",
            "Trainable params: 2,823,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4K_DQ7igcQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c23706ba-0782-4a03-f4f1-fc453e2b1c49"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "cnnmodel2.fit(X_train_new, y_train_new,\n",
        "          batch_size=200, #75000 X_train shape\n",
        "          epochs=1, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 41s 107ms/step - loss: 1.1858 - acc: 0.5329 - val_loss: 1.0873 - val_acc: 0.5795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8bacdb2828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoNdDcD7mv2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "0ad304e8-d487-40e9-90d6-5dbea4545fd7"
      },
      "source": [
        "#Evaluate on test set:\n",
        "y_pred=cnnmodel2.predict_classes(test_sequences_padding)\n",
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2122\n",
            "           1       0.40      0.22      0.29      8182\n",
            "           2       0.63      0.87      0.73     23874\n",
            "           3       0.47      0.40      0.43      9878\n",
            "           4       0.49      0.15      0.23      2762\n",
            "\n",
            "    accuracy                           0.58     46818\n",
            "   macro avg       0.40      0.33      0.34     46818\n",
            "weighted avg       0.52      0.58      0.53     46818\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhbiiiM4gy5G",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Model with training your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i43pq6begzUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "8225ea77-c840-4e6a-872a-8540028c7004"
      },
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#model\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(labels_index, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "rnnmodel.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 2,692,229\n",
            "Trainable params: 2,692,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BNsvkOg4A-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "8af11ca4-9791-4316-b2c6-081af808d687"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "rnnmodel.fit(X_train_new, y_train_new,\n",
        "          batch_size=200, #75000 X_train shape\n",
        "          epochs=1, validation_data=(X_valid, y_valid))\n",
        "#  1472s 4s/step - loss: 0.3740 - accuracy: 0.5546 - val_loss: 0.3232 - val_accuracy: 0.6187\n",
        "#Evaluate on test set:\n",
        "y_pred=rnnmodel.predict_classes(test_sequences_padding)\n",
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 1472s 4s/step - loss: 0.3740 - accuracy: 0.5546 - val_loss: 0.3232 - val_accuracy: 0.6187\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.01      0.01      2122\n",
            "           1       0.50      0.38      0.43      8182\n",
            "           2       0.70      0.83      0.76     23874\n",
            "           3       0.49      0.59      0.53      9878\n",
            "           4       0.59      0.05      0.09      2762\n",
            "\n",
            "    accuracy                           0.62     46818\n",
            "   macro avg       0.54      0.37      0.37     46818\n",
            "weighted avg       0.60      0.62      0.58     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kde4min4g7IA",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Model using pre-trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AURWp4UKg8IA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "e80ccfe8-01e4-45fd-d622-7788c542d1cd"
      },
      "source": [
        "\n",
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(labels_index, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print(rnnmodel2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 100)         1526500   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,644,393\n",
            "Trainable params: 117,893\n",
            "Non-trainable params: 1,526,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr1TypbahDWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "1ee40ce9-8f55-4919-a7ab-c93ade3a65e2"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "rnnmodel2.fit(X_train_new, y_train_new,\n",
        "          batch_size=200, #75000 X_train shape\n",
        "          epochs=1, validation_data=(X_valid, y_valid))\n",
        "#Evaluate on test set:\n",
        "y_pred=rnnmodel2.predict_classes(test_sequences_padding)\n",
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 1357s 4s/step - loss: 0.3639 - accuracy: 0.5577 - val_loss: 0.3349 - val_accuracy: 0.5898\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.01      0.02      2122\n",
            "           1       0.44      0.27      0.33      8182\n",
            "           2       0.67      0.84      0.74     23874\n",
            "           3       0.46      0.53      0.49      9878\n",
            "           4       0.52      0.06      0.11      2762\n",
            "\n",
            "    accuracy                           0.59     46818\n",
            "   macro avg       0.49      0.34      0.34     46818\n",
            "weighted avg       0.56      0.59      0.55     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kajyJPe3wgbn",
        "colab_type": "text"
      },
      "source": [
        "- DL model is better than ML model. (improve 20% macro and weighted avg, 10% accuracy)\n",
        "- RNN model is better than CNN model, but very long time training (~ 20 times lower), accuracy is not imporve so much (just 4%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ_DVjwqdCL2",
        "colab_type": "text"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WaUDpTYdbKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "eb1b9656-a2ec-4eae-ade2-2d09852d14d3"
      },
      "source": [
        "# install BERT\n",
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.7MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.37)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.37)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert, pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEp5PqoMdDb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing a few necessary packages and setting the DATA directory\n",
        "DATA_DIR=\".\"\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    torch.cuda.get_device_name(0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgYJam6YdDew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0d988d34-9606-4530-ad5b-36672fb5b05d"
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SACxHfyleWnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "78d3bff9-230f-4876-a49e-223c7428ded7"
      },
      "source": [
        "train_df=train_df[['Phrase','Sentiment']]\n",
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  Sentiment\n",
              "0  A series of escapades demonstrating the adage ...          1\n",
              "1  A series of escapades demonstrating the adage ...          2\n",
              "2                                           A series          2\n",
              "3                                                  A          2\n",
              "4                                             series          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-I5KdDVde85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "22a3251d-6f76-4f23-da1c-48c2f0ec4be1"
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvnguMFLe1qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ad4a7d51-518a-44d0-c2fa-29598e97e17f"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150238    make you wish you were at home watching that m...\n",
              "133360    , the tale has turned from sweet to bitterswee...\n",
              "49191     can say that about most of the flicks moving i...\n",
              "137709                     Does n't deliver a great story ,\n",
              "73297                                    unrecoverable life\n",
              "                                ...                        \n",
              "142974    Laced with liberal doses of dark humor , gorge...\n",
              "78231     Offers absolutely nothing I had n't already se...\n",
              "7375                           Ivan is a prince of a fellow\n",
              "115288                        the stomach-knotting suspense\n",
              "2467                                                 Quirky\n",
              "Name: Phrase, Length: 109242, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-aPoEj6dfCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaning the text\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def strip(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = re.sub('\\[[^]]*\\]', '', soup.get_text())\n",
        "    pattern=r\"[^a-zA-z0-9\\s,']\"\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "X_train=X_train.apply(strip)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAyr83dpdfAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "10b8714b-0a06-4bf6-f68c-2110c5489a40"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150238    make you wish you were at home watching that m...\n",
              "133360    , the tale has turned from sweet to bitterswee...\n",
              "49191     can say that about most of the flicks moving i...\n",
              "137709                     Does n't deliver a great story ,\n",
              "73297                                    unrecoverable life\n",
              "                                ...                        \n",
              "142974    Laced with liberal doses of dark humor , gorge...\n",
              "78231     Offers absolutely nothing I had n't already seen \n",
              "7375                           Ivan is a prince of a fellow\n",
              "115288                         the stomachknotting suspense\n",
              "2467                                                 Quirky\n",
              "Name: Phrase, Length: 109242, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUo7s1lQdDl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BERT needs to understand 2 things:\n",
        "#1) The start and end of each sentiment\n",
        "# so we declare a special token CLS which tells BERT that its a classification task\n",
        "sentences = X_train\n",
        "sentence = [\"[CLS] \"+i+\" [SEP]\" for i in sentences]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kVxPLYdfD2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c6d18aad-9415-4390-ed97-961785fd6f17"
      },
      "source": [
        "sentence[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] make you wish you were at home watching that movie instead of in the theater watching this one [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbrV1HPHfD0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a6f80ff-1496-45a3-b3b2-a6710a57caaf"
      },
      "source": [
        "# Tokenize with BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Restricting the max size of Tokens to 512(BERT doest accept any more than this)\n",
        "tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:510] , sentence))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 883968.76B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TtxLJ78dDpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "14f3eebd-5c87-46f7-e4be-84abf5162500"
      },
      "source": [
        "print (sentence[0])\n",
        "print (tokenized_texts[0])\n",
        "print(len(tokenized_texts[0]))\n",
        "print (sentence[1])\n",
        "print (tokenized_texts[1])\n",
        "print(len(tokenized_texts[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] make you wish you were at home watching that movie instead of in the theater watching this one [SEP]\n",
            "['[CLS]', 'make', 'you', 'wish', 'you', 'were', 'at', 'home', 'watching', 'that', 'movie', 'instead', 'of', 'in', 'the', 'theater', 'watching', 'this', 'one', '[SEP]']\n",
            "20\n",
            "[CLS] , the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned  [SEP]\n",
            "['[CLS]', ',', 'the', 'tale', 'has', 'turned', 'from', 'sweet', 'to', 'bitter', '##sw', '##eet', ',', 'and', 'when', 'the', 'tears', 'come', 'during', 'that', 'final', ',', 'beautiful', 'scene', ',', 'they', 'finally', 'feel', 'absolutely', 'earned', '[SEP]']\n",
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOlpPSWfYiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = list(y_train)#storing the labels\n",
        "#print(labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuKdt4XxfYsZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "1987bae4-5b06-4fe7-f173-cb1e3cb89cad"
      },
      "source": [
        "len_toeknized = pd.Series(list(map(lambda t: len(t) , tokenized_texts)))\n",
        "len_toeknized.hist()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0c00c32438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wikXGwSxfYqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c79afe5b-fd3a-4ae2-bd71-e64cb888eb0e"
      },
      "source": [
        "MAX_LEN = 128\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "print(input_ids[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2191, 2017, 4299, 2017, 2020, 2012, 2188, 3666, 2008, 3185, 2612, 1997, 1999, 1996, 4258, 3666, 2023, 2028, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkOtMe4ofYmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bc5ab27b-5bc3-41d1-b2fe-c250f9ffea39"
      },
      "source": [
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "print(input_ids.shape)\n",
        "input_ids"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  101,  2191,  2017, ...,     0,     0,     0],\n",
              "       [  101,  1010,  1996, ...,     0,     0,     0],\n",
              "       [  101,  2064,  2360, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  7332,  2003, ...,     0,     0,     0],\n",
              "       [  101,  1996,  4308, ...,     0,     0,     0],\n",
              "       [  101, 21864, 15952, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__JmwJEri11O",
        "colab_type": "text"
      },
      "source": [
        "BERT is a MLM(Masked Language Model). We have to define its mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb_pLhLTiKMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y-3toOeiKSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. \n",
        "batch_size = 20\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjgWg2QFjOUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05b2a4fc-63ba-4d1d-e678-c9539ea15bbd"
      },
      "source": [
        "num_labels=len(set(labels))\n",
        "num_labels"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bhDkTdbiKXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3246fb9-ccd0-480c-915f-d61e18f4c56d"
      },
      "source": [
        "#Loading pre trained BERT\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=num_labels)#multi classification\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-KouIVJiKPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2f739756-ec9b-48e3-cf32-a9eabb41f79c"
      },
      "source": [
        "# BERT fine-tuning parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "torch.cuda.empty_cache() \n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs \n",
        "epochs = 4\n",
        "\n",
        "# BERT training loop\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "  \n",
        "  ## TRAINING\n",
        "  \n",
        "  # Set our model to training mode\n",
        "  model.train()  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "       \n",
        "  ## VALIDATION\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# plot training performance\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n",
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.8056901631144803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [51:34<2:34:44, 3094.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6957952468007317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkKexcnvqaa",
        "colab_type": "text"
      },
      "source": [
        "t_total value of -1 results in schedule not being applied\n",
        "\n",
        "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]Train loss: 0.8056901631144803\n",
        "\n",
        "Epoch:  25%|██▌       | 1/4 [51:34<2:34:44, 3094.77s/it]Validation Accuracy: 0.6957952468007317"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ebJ783sk-ff",
        "colab_type": "text"
      },
      "source": [
        "Evaluate test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ykJFpnjiKKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load test data\n",
        "X_test=X_test.apply(strip)\n",
        "sentences = X_test\n",
        "sentence = [\"[CLS] \"+i+\" [SEP]\" for i in sentences]\n",
        "labels = list(y_test)#\n",
        "\n",
        "# tokenize test data\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# create test tensors\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "batch_size = 32  \n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "## Prediction on test set\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  \n",
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "for i in range(len(true_labels)):\n",
        "  matthews = matthews_corrcoef(true_labels[i],\n",
        "                 np.argmax(predictions[i], axis=1).flatten())\n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKy3AW_7vkGW",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch (not complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifncwBWMcez2",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWOt3QZiBUg6",
        "colab_type": "text"
      },
      "source": [
        "LSTM pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-wB3X34Ao78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#library imports\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "torch.manual_seed(0)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import string\n",
        "from sklearn.metrics import mean_squared_error,accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq6l6TMssiN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_zGR9Z5Z4yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "test_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/test.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0BUz7eaZ49X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6064dc6b-8d40-47eb-b6ee-9bd8f2bd8a8e"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbN8MnDLZ5F0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cca374f7-597d-413e-cd03-18ff85e19ec5"
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii4tbQ-YZ5D9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3265e03-7a72-44af-f42c-a20f18f0b445"
      },
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) #20000\n",
        "tokenizer.fit_on_texts(X_train.values)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train.values) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test.values)\n",
        "word_index = tokenizer.word_index #(X_train, luc nay chua phan ra X_train_new va X_valid)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "# max 20000 words keep in sequences, word index is just a show case, not use (> max words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15264 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko073qPyZ5BF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dba5f74f-7161-45cf-b80f-2e530ef1f15c"
      },
      "source": [
        "print(train_sequences[0]) # max 10000 words in sequences\n",
        "print(len(train_sequences[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[77, 22, 970, 22, 175, 30, 448, 227, 9, 17, 378, 3, 7, 1, 317, 227, 18, 26]\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIr2yUaQZ47B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "train_sequences_padding = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) #1000\n",
        "test_sequences_padding = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) #1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdX0Bi-WZ_7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d4310e68-cd87-48bc-b191-57fff23d3a35"
      },
      "source": [
        "#Before transform\n",
        "print(len(train_sequences)) # train_texts --> train_sequences\n",
        "print(len(train_sequences[0]))\n",
        "print(train_sequences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109242\n",
            "18\n",
            "[77, 22, 970, 22, 175, 30, 448, 227, 9, 17, 378, 3, 7, 1, 317, 227, 18, 26]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcbTFN3GZ_45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a145d625-7ab6-434e-b641-18d7a94825b1"
      },
      "source": [
        "#After transform\n",
        "print(train_sequences_padding.shape)\n",
        "train_sequences_padding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242, 1000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  227,   18,   26],\n",
              "       [   0,    0,    0, ...,  140, 1487, 3657],\n",
              "       [   0,    0,    0, ...,    3,    1, 4473],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    3,    2, 2915],\n",
              "       [   0,    0,    0, ..., 1308, 9589,  436],\n",
              "       [   0,    0,    0, ...,    0,    0,  429]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StLCvD3cZ_2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7fe68bd5-c0b5-47d5-fd84-817d49e96f6b"
      },
      "source": [
        "train_labels = to_categorical(np.asarray(y_train))\n",
        "test_labels = to_categorical(np.asarray(y_test))\n",
        "\n",
        "print(y_train[:3])\n",
        "print(train_labels.shape)\n",
        "train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150238    1\n",
            "133360    4\n",
            "49191     2\n",
            "Name: Sentiment, dtype: int64\n",
            "(109242, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB4_7iNSZ_0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bbcf89d9-2789-464c-f340-245bcf9fa445"
      },
      "source": [
        "X_train_new,X_valid,y_train_new,y_valid=train_test_split(train_sequences_padding,train_labels,\n",
        "                                                         test_size=0.3,\n",
        "                                               stratify=train_labels,random_state=42)\n",
        "print(X_train_new.shape)\n",
        "print(y_train_new.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76469, 1000)\n",
            "(76469, 5)\n",
            "(32773, 1000)\n",
            "(32773, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gIsyNz7agpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9fef35a6-3fa7-4363-e3d8-1208a1743e02"
      },
      "source": [
        "X_test=test_sequences_padding\n",
        "y_test=test_labels\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(46818, 1000)\n",
            "(46818, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt2iZJfDaFj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5747bc49-88cc-4a26-8ecc-27998ea12967"
      },
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/Data/NLP/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # vocabulary name\n",
        "        coefs = np.asarray(values[1:], dtype='float32') # embedding 100 dim\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "print(embeddings_index[\"google\"])\n",
        "print(len(embeddings_index[\"google\"]))\n",
        "print(MAX_NUM_WORDS)\n",
        "print(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "[ 0.22575  -0.56253  -0.05156  -0.079389  1.1876   -0.48397  -0.23342\n",
            " -0.85278   0.97495  -0.33344   0.71692   0.12644   0.31962  -1.4136\n",
            " -0.57903  -0.037286 -0.0164    0.45155  -0.29005   0.52599  -0.22534\n",
            " -0.29556  -0.032407  1.5608   -0.013499 -0.064558  0.26625   0.78595\n",
            " -0.71693  -0.93025   0.80461   1.6035   -0.30602  -0.34764   0.93872\n",
            "  0.38137  -0.26743  -0.56519   0.58899  -0.14554  -0.34324   0.21291\n",
            " -0.39887   0.090042 -0.8495    0.38803  -0.5045   -0.22488   1.0644\n",
            " -0.2624    1.0334    0.06348  -0.39989   0.24236  -0.65636  -1.8107\n",
            " -0.061801  0.13795   1.1658   -0.30046  -0.50143   0.16509   0.039835\n",
            "  0.62541   0.56935   0.64125   0.21308   0.30276   0.39673   0.38973\n",
            "  0.28183   0.79481  -0.11962  -0.49598  -0.53195  -0.14897   0.51254\n",
            " -0.39208  -0.58535  -0.078509  0.81721  -0.73497  -0.68131   0.099243\n",
            " -0.87608   0.029632  0.33402  -0.14305   0.16964  -0.035178  0.39777\n",
            "  0.71769   0.25867  -0.36201   0.45698  -0.39156  -0.49343  -0.11224\n",
            "  0.29046   0.73216 ]\n",
            "100\n",
            "20000\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOBPwYtIaFt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0e0bba73-940b-4733-a6f1-302198f4b7e7"
      },
      "source": [
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "print(num_words)\n",
        "print(embedding_matrix.shape)\n",
        "print(len(word_index)) # max 20000 tu hay gap nhat thoi, index nay khong xai het\n",
        "print(len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15265\n",
            "(15265, 100)\n",
            "15264\n",
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln2k-3MjaFr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS: # chi thuc hien get embedding word < max_num_words(20000)\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word) #lay 100d embedding cho tung tu\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyLz8JgoaFps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5f49b5f8-0c72-4dc8-b2d1-ad6db77c21a1"
      },
      "source": [
        "print(embedding_matrix.shape)\n",
        "embedding_matrix[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15265, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
              "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
              "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
              "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
              "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
              "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
              "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
              "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
              "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
              "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
              "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
              "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
              "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
              "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
              "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
              "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
              "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
              "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
              "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
              "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5W5U9u8aFnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35f4e643-24b1-47ee-d83f-f27328ca79f4"
      },
      "source": [
        "labels_index=len(np.unique(y))\n",
        "labels_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdXVyKkauU9",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHL03IoPnE05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe787cd4-00b6-4f34-fe53-6e4a0ba5aa97"
      },
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(input_dim=num_words,#15265 (max 20000)\n",
        "                            output_dim=EMBEDDING_DIM, #100\n",
        "                            embeddings_initializer=Constant(embedding_matrix), #(15265, 100)\n",
        "                            input_length=MAX_SEQUENCE_LENGTH, #1000\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STo9lwxGZIsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "924617ad-72f8-4f40-823d-e40ec38a3032"
      },
      "source": [
        "labels_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thVx2h4dCixi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, n_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, n_layers, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_size, labels_index)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out, _ = self.lstm(out)\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.output(out)\n",
        "        out = out[-1,0]\n",
        "        out = torch.sigmoid(out).unsqueeze(0)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3kmsF2QWodk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "db99e57c-69c7-4b17-8dab-bdff0778627a"
      },
      "source": [
        "model = LSTM(MAX_NUM_WORDS, 100, 128, 1)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(20000, 100)\n",
              "  (lstm): LSTM(100, 128, batch_first=True)\n",
              "  (output): Linear(in_features=128, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyfuz2tYXVx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = nn.CrossEntropyLoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZrj6se3YVTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96bb7e77-35d0-4d3d-f89b-00c56f49b7e8"
      },
      "source": [
        "losses = []\n",
        "acc = []\n",
        "for e in range(1, epochs+1):\n",
        "    single_loss = []\n",
        "    preds = []\n",
        "    targets = []\n",
        "    for i, r in enumerate(X_train_new):\n",
        "      if len(r) <= 1:\n",
        "          continue\n",
        "      x = torch.Tensor([r]).long()\n",
        "      y = torch.Tensor([y_train_new[i]])\n",
        "      print('x',x)\n",
        "      print('y',y)\n",
        "      pred = model(x)\n",
        "      print('pred',pred)\n",
        "      loss = loss_function(pred, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      final_pred = np.round(pred.detach().numpy())\n",
        "      preds.append(final_pred)\n",
        "      targets.append(y)\n",
        "      single_loss.append(loss.item())\n",
        "        \n",
        "    losses.append(np.mean(single_loss))\n",
        "    accuracy = accuracy_score(targets,preds)\n",
        "    acc.append(accuracy)\n",
        "    if e%1 == 0:\n",
        "        print(\"Epoch: \", e, \"... Loss function: \", losses[-1], \"... Accuracy: \", acc[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1, 2089,\n",
            "            3,    2, 9465,  451]])\n",
            "y tensor([[0., 0., 1., 0., 0.]])\n",
            "pred tensor([0.5143], grad_fn=<UnsqueezeBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-78274b8b05be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m    941\u001b[0m     def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n\u001b[1;32m    942\u001b[0m                  reduce=None, reduction: str = 'mean') -> None:\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4P6rCt0ZAeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_range = range(len(losses))\n",
        "plt.plot(x_range, losses)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOZmRHghxVbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_range = range(len(acc))\n",
        "plt.plot(x_range, acc)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Accuracy score\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxOCvIxu_Asr",
        "colab_type": "text"
      },
      "source": [
        "#  Text_analytic_apress (Ch05b Text classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fdg4v0B_BGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Aug 01 01:11:02 2016\n",
        "@author: DIP\n",
        "\"\"\"\n",
        "\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6dsecLc_DHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "63562b54-bf39-4675-d331-2716c061387b"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import unicodedata\n",
        "#from contractions import CONTRACTION_MAP\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "import collections\n",
        "#from textblob import Word\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
        "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
        "\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    if bool(soup.find()):\n",
        "        [s.extract() for s in soup(['iframe', 'script'])]\n",
        "        stripped_text = soup.get_text()\n",
        "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    else:\n",
        "        stripped_text = text\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "#def correct_spellings_textblob(tokens):\n",
        "#\treturn [Word(token).correct() for token in tokens]  \n",
        "\n",
        "\n",
        "def simple_porter_stemming(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_repeated_characters(tokens):\n",
        "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    match_substitution = r'\\1\\2\\3'\n",
        "    def replace(old_word):\n",
        "        if wordnet.synsets(old_word):\n",
        "            return old_word\n",
        "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "        return replace(new_word) if new_word != old_word else new_word\n",
        "            \n",
        "    correct_tokens = [replace(word) for word in tokens]\n",
        "    return correct_tokens\n",
        "\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_stemming=False, text_lemmatization=True, \n",
        "                     special_char_removal=True, remove_digits=True,\n",
        "                     stopword_removal=True, stopwords=stopword_list):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "\n",
        "        # remove extra newlines\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "\n",
        "        # stem text\n",
        "        if text_stemming and not text_lemmatization:\n",
        "        \tdoc = simple_porter_stemming(doc)\n",
        "\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "\n",
        "         # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = doc.strip()\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei7g59TF_nNK",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecaQHyd_EZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#import text_normalizer as tn # copy py file from text book apress\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghpww7fW_OYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYPI-YBK_qEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "47d9b06b-f4f7-4594-e8bb-b1b16006e928"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df=train_df[['Phrase','Sentiment']]\n",
        "train_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  Sentiment\n",
              "0  A series of escapades demonstrating the adage ...          1\n",
              "1  A series of escapades demonstrating the adage ...          2\n",
              "2                                           A series          2\n",
              "3                                                  A          2\n",
              "4                                             series          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqVrQgwP_rT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "06919358-7ae1-4e53-d680-0a1778649603"
      },
      "source": [
        "train_df.isnull().sum()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Phrase       0\n",
              "Sentiment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peFIHm4V_6jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "# just to keep negation if any in bi-grams\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "\n",
        "# normalize our corpus\n",
        "norm_corpus = normalize_corpus(corpus=train_df['Phrase'], html_stripping=True, contraction_expansion=True, \n",
        "                                  accented_char_removal=True, text_lower_case=True, text_lemmatization=True, \n",
        "                                  text_stemming=False, special_char_removal=True, remove_digits=True,\n",
        "                                  stopword_removal=True, stopwords=stopword_list)\n",
        "train_df['Clean Article'] = norm_corpus"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZUeLcyGAK45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6c8f43fb-86e6-4db1-9d10-05480dc7f2cc"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Clean Article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapade demonstrate adage good goose a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "      <td>series escapade demonstrate adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  ...                                      Clean Article\n",
              "0  A series of escapades demonstrating the adage ...  ...  series escapade demonstrate adage good goose a...\n",
              "1  A series of escapades demonstrating the adage ...  ...       series escapade demonstrate adage good goose\n",
              "2                                           A series  ...                                             series\n",
              "3                                                  A  ...                                                   \n",
              "4                                             series  ...                                             series\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQYH4h0jAUvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_df.to_csv('/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train_df_clean_apress.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYVRBH0NTedZ",
        "colab_type": "text"
      },
      "source": [
        "## Data cleaned (Begin here):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38Lqx29X76h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import text_normalizer as tn # copy py file from text book apress\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfloHhicC57B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8357eb80-ce83-4d37-86fb-02f9f1851d4e"
      },
      "source": [
        "train_df_clean=pd.read_csv('/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train_df_clean_apress.csv')\n",
        "train_df_clean.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Clean Article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapade demonstrate adage good goose a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "      <td>series escapade demonstrate adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  ...                                      Clean Article\n",
              "0  A series of escapades demonstrating the adage ...  ...  series escapade demonstrate adage good goose a...\n",
              "1  A series of escapades demonstrating the adage ...  ...       series escapade demonstrate adage good goose\n",
              "2                                           A series  ...                                             series\n",
              "3                                                  A  ...                                                NaN\n",
              "4                                             series  ...                                             series\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmMxDUu7E_ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ed9c9178-deff-47a7-c464-4be6e6c70e06"
      },
      "source": [
        "print(train_df_clean.shape)\n",
        "train_df_clean = train_df_clean.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
        "print(train_df_clean.shape)\n",
        "train_df_clean=train_df_clean.dropna()\n",
        "print(train_df_clean.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 3)\n",
            "(156060, 3)\n",
            "(154663, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b134eqxEFhqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_df_clean['Clean Article']\n",
        "y=train_df_clean['Sentiment']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4gQsC4jAU5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "926688f0-a78a-4714-f704-71c9dff7399e"
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(108264,)\n",
            "(108264,)\n",
            "(46399,)\n",
            "(46399,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofwHR0hZYC3B",
        "colab_type": "text"
      },
      "source": [
        "## ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBggrSV7cyPS",
        "colab_type": "text"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebLh05-GAUyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# build BOW features on train articles\n",
        "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
        "cv_train_features = cv.fit_transform(X_train)\n",
        "# transform test articles into features\n",
        "cv_test_features = cv.transform(X_test)\n",
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t73syXFZFsDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "834b9b29-893a-4044-e0d6-1a8ecfa28e8c"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB(alpha=1)\n",
        "mnb.fit(cv_train_features, y_train)\n",
        "mnb_bow_cv_scores = cross_val_score(mnb,cv_train_features, y_train, cv=5)\n",
        "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
        "mnb_bow_test_score = mnb.score(cv_test_features, y_test)\n",
        "print('Test Accuracy:', mnb_bow_test_score)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.59871611 0.6009329  0.60532028 0.59599132 0.59551081]\n",
            "Mean CV Accuracy: 0.5992942826472758\n",
            "Test Accuracy: 0.6053794262807388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNS894LrUcA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b0839932-1c5d-497b-fc5d-a3a9d38d3712"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
        "svm_sgd.fit(cv_train_features, y_train)\n",
        "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, y_train, cv=5)\n",
        "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
        "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, y_test)\n",
        "print('Test Accuracy:', svmsgd_bow_test_score)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.60314968 0.60194892 0.60338059 0.59816192 0.59634214]\n",
            "Mean CV Accuracy: 0.6005966502760753\n",
            "Test Accuracy: 0.5983534127890687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quoJhBv7aYGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7bd69615-445a-42f2-c7d7-d6276c45e7b8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "mnb = LogisticRegression(penalty='l2', max_iter=10000, C=1, random_state=42)\n",
        "mnb.fit(cv_train_features, y_train)\n",
        "mnb_bow_cv_scores = cross_val_score(mnb,cv_train_features, y_train, cv=5)\n",
        "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
        "mnb_bow_test_score = mnb.score(cv_test_features, y_test)\n",
        "print('Test Accuracy:', mnb_bow_test_score)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.63095183 0.63169076 0.63760218 0.62998199 0.62682431]\n",
            "Mean CV Accuracy: 0.6314102140517419\n",
            "Test Accuracy: 0.6371473523136274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLOxkcksYhcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Very long time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "svm_sgd = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "svm_sgd.fit(cv_train_features, y_train)\n",
        "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, y_train, cv=5)\n",
        "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
        "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, y_test)\n",
        "print('Test Accuracy:', svmsgd_bow_test_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqBKvVtiYqc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Very long time\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "svm_sgd = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
        "svm_sgd.fit(cv_train_features, y_train)\n",
        "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, y_train, cv=5)\n",
        "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
        "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, y_test)\n",
        "print('Test Accuracy:', svmsgd_bow_test_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3T6MhcpY8NO",
        "colab_type": "text"
      },
      "source": [
        "Creating a list of the classifiers\n",
        "\n",
        "classifiers = {\n",
        "    #'KNN':KNeighborsClassifier(),   #verylong 600s with this data set ?  \n",
        "    #'SVC':LinearSVC(penalty='l2', C=1, random_state=42), #Very Long\n",
        "    #'XGB':XGBClassifier(), #New Boosting, XGB is crashed\n",
        "    #'LGBM':LGBMClassifier(), #New Boosting, LGBM is crashed\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmCMpLHkIlZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e8a1140b-7cec-4a77-9dfc-f9e7891a1d1c"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB(alpha=1)\n",
        "model.fit(cv_train_features, y_train)\n",
        "y_pred = model.predict(cv_test_features)\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print('Accuracy score:',accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  540   990   529    56     6]\n",
            " [  475  3278  3958   428    33]\n",
            " [  169  1993 18783  2358   182]\n",
            " [   19   394  4142  4736   568]\n",
            " [    1    52   492  1465   752]]\n",
            "Accuracy score: 0.6053794262807388\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.25      0.32      2121\n",
            "           1       0.49      0.40      0.44      8172\n",
            "           2       0.67      0.80      0.73     23485\n",
            "           3       0.52      0.48      0.50      9859\n",
            "           4       0.49      0.27      0.35      2762\n",
            "\n",
            "    accuracy                           0.61     46399\n",
            "   macro avg       0.52      0.44      0.47     46399\n",
            "weighted avg       0.59      0.61      0.59     46399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHwH4hxzLs23",
        "colab_type": "text"
      },
      "source": [
        "### TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y1I1ueHLv-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e6181d37-bf89-4a14-b3a8-cf3c534fc668"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# build BOW features on train articles\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
        "tv_train_features = tv.fit_transform(X_train)\n",
        "# transform test articles into features\n",
        "tv_test_features = tv.transform(X_test)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFIDF model:> Train features shape: (108264, 13286)  Test features shape: (46399, 13286)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9b6wOXHaQoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "79d3b0bf-16d5-4185-c3ff-423830cef8ae"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB(alpha=1)\n",
        "mnb.fit(tv_train_features, y_train)\n",
        "mnb_bow_cv_scores = cross_val_score(mnb,tv_train_features, y_train, cv=5)\n",
        "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
        "mnb_bow_test_score = mnb.score(tv_test_features, y_test)\n",
        "print('Test Accuracy:', mnb_bow_test_score)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.5740544  0.57410059 0.57701011 0.57664065 0.57204877]\n",
            "Mean CV Accuracy: 0.574770905175001\n",
            "Test Accuracy: 0.5788271298950408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBkX9ztvaQlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "23d98268-cc8b-4f14-fc84-c30db4a7a8de"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "mnb = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
        "mnb.fit(tv_train_features, y_train)\n",
        "mnb_bow_cv_scores = cross_val_score(mnb,tv_train_features, y_train, cv=5)\n",
        "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
        "mnb_bow_test_score = mnb.score(tv_test_features, y_test)\n",
        "print('Test Accuracy:', mnb_bow_test_score)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.5594144  0.55673579 0.55849074 0.55442664 0.55671531]\n",
            "Mean CV Accuracy: 0.5571565761355468\n",
            "Test Accuracy: 0.5550981702191857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3RhBqjhaQi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d52f149d-e9b9-45eb-c980-7c3cbf573ff4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "mnb = LogisticRegression(penalty='l2', max_iter=10000, C=1, random_state=42)\n",
        "mnb.fit(tv_train_features, y_train)\n",
        "mnb_bow_cv_scores = cross_val_score(mnb,tv_train_features, y_train, cv=5)\n",
        "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
        "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
        "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
        "mnb_bow_test_score = mnb.score(tv_test_features, y_test)\n",
        "print('Test Accuracy:', mnb_bow_test_score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.61825151 0.6193599  0.62379347 0.61506489 0.61292259]\n",
            "Mean CV Accuracy: 0.6178784733994712\n",
            "Test Accuracy: 0.6238065475549042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPemtMjbaQhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "eb77e0e8-f883-4463-b19a-e52d92f0975a"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB(alpha=1)\n",
        "model.fit(tv_train_features, y_train)\n",
        "y_pred = model.predict(tv_test_features)\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print('Accuracy score:',accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   82   839  1159    40     1]\n",
            " [   41  1991  5932   207     1]\n",
            " [   12  1008 21095  1358    12]\n",
            " [    3   124  6091  3575    66]\n",
            " [    0    13  1049  1586   114]]\n",
            "Accuracy score: 0.5788271298950408\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.04      0.07      2121\n",
            "           1       0.50      0.24      0.33      8172\n",
            "           2       0.60      0.90      0.72     23485\n",
            "           3       0.53      0.36      0.43      9859\n",
            "           4       0.59      0.04      0.08      2762\n",
            "\n",
            "    accuracy                           0.58     46399\n",
            "   macro avg       0.56      0.32      0.33     46399\n",
            "weighted avg       0.56      0.58      0.52     46399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC6R-Hrfckqv",
        "colab_type": "text"
      },
      "source": [
        "## Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miShI4tFaQer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "mnb_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                        ('mnb', MultinomialNB())\n",
        "                       ])\n",
        "\n",
        "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "              'mnb__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]\n",
        "}\n",
        "\n",
        "gs_mnb = GridSearchCV(mnb_pipeline, param_grid, cv=5, verbose=0)\n",
        "gs_mnb = gs_mnb.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97U6IFihc-bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "baeff7f3-2bb7-439c-cf5e-92fd4b39be3d"
      },
      "source": [
        "gs_mnb.best_estimator_.get_params()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'memory': None,\n",
              " 'mnb': MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True),\n",
              " 'mnb__alpha': 0.1,\n",
              " 'mnb__class_prior': None,\n",
              " 'mnb__fit_prior': True,\n",
              " 'steps': [('tfidf',\n",
              "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                   min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
              "  ('mnb', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
              " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
              " 'tfidf__analyzer': 'word',\n",
              " 'tfidf__binary': False,\n",
              " 'tfidf__decode_error': 'strict',\n",
              " 'tfidf__dtype': numpy.float64,\n",
              " 'tfidf__encoding': 'utf-8',\n",
              " 'tfidf__input': 'content',\n",
              " 'tfidf__lowercase': True,\n",
              " 'tfidf__max_df': 1.0,\n",
              " 'tfidf__max_features': None,\n",
              " 'tfidf__min_df': 1,\n",
              " 'tfidf__ngram_range': (1, 2),\n",
              " 'tfidf__norm': 'l2',\n",
              " 'tfidf__preprocessor': None,\n",
              " 'tfidf__smooth_idf': True,\n",
              " 'tfidf__stop_words': None,\n",
              " 'tfidf__strip_accents': None,\n",
              " 'tfidf__sublinear_tf': False,\n",
              " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'tfidf__tokenizer': None,\n",
              " 'tfidf__use_idf': True,\n",
              " 'tfidf__vocabulary': None,\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfaGYae0dDIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "8acbde74-96ed-4b74-8496-eb41ae22bd92"
      },
      "source": [
        "cv_results = gs_mnb.cv_results_\n",
        "results_df = pd.DataFrame({'rank': cv_results['rank_test_score'],\n",
        "                           'params': cv_results['params'], \n",
        "                           'cv score (mean)': cv_results['mean_test_score'], \n",
        "                           'cv score (std)': cv_results['std_test_score']} \n",
        "              )\n",
        "results_df = results_df.sort_values(by=['rank'], ascending=True)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "results_df"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>params</th>\n",
              "      <th>cv score (mean)</th>\n",
              "      <th>cv score (std)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>{'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}</td>\n",
              "      <td>0.612891</td>\n",
              "      <td>0.004460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>{'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 2)}</td>\n",
              "      <td>0.597373</td>\n",
              "      <td>0.004251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>{'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 2)}</td>\n",
              "      <td>0.593411</td>\n",
              "      <td>0.003794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>{'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 2)}</td>\n",
              "      <td>0.593198</td>\n",
              "      <td>0.003552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>{'mnb__alpha': 1, 'tfidf__ngram_range': (1, 2)}</td>\n",
              "      <td>0.590676</td>\n",
              "      <td>0.001442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>{'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 1)}</td>\n",
              "      <td>0.587942</td>\n",
              "      <td>0.002642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>{'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 1)}</td>\n",
              "      <td>0.587915</td>\n",
              "      <td>0.002698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>{'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 1)}</td>\n",
              "      <td>0.587878</td>\n",
              "      <td>0.002468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9</td>\n",
              "      <td>{'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 1)}</td>\n",
              "      <td>0.586612</td>\n",
              "      <td>0.002281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10</td>\n",
              "      <td>{'mnb__alpha': 1, 'tfidf__ngram_range': (1, 1)}</td>\n",
              "      <td>0.574919</td>\n",
              "      <td>0.001732</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rank  ... cv score (std)\n",
              "7     1  ...       0.004460\n",
              "5     2  ...       0.004251\n",
              "3     3  ...       0.003794\n",
              "1     4  ...       0.003552\n",
              "9     5  ...       0.001442\n",
              "2     6  ...       0.002642\n",
              "0     7  ...       0.002698\n",
              "4     8  ...       0.002468\n",
              "6     9  ...       0.002281\n",
              "8    10  ...       0.001732\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xvCnXrMdPar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2a9a3224-12a0-4f67-e7b5-f0cf87ca2ca6"
      },
      "source": [
        "best_mnb_test_score = gs_mnb.score(X_test, y_test)\n",
        "print('Test Accuracy :', best_mnb_test_score)\n",
        "y_pred=gs_mnb.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy : 0.6237849953662794\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.39      0.41      2121\n",
            "           1       0.51      0.48      0.50      8172\n",
            "           2       0.72      0.76      0.74     23485\n",
            "           3       0.54      0.53      0.54      9859\n",
            "           4       0.48      0.40      0.44      2762\n",
            "\n",
            "    accuracy                           0.62     46399\n",
            "   macro avg       0.54      0.51      0.52     46399\n",
            "weighted avg       0.62      0.62      0.62     46399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJAOrKtdZUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Long time, need to investigate late\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                        ('lr', LogisticRegression(penalty='l2', max_iter=1000, random_state=42))\n",
        "                       ])\n",
        "\n",
        "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "              'lr__C': [1, 5, 10]\n",
        "}\n",
        "\n",
        "gs_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, verbose=2)\n",
        "gs_lr = gs_lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YloRavgjd64g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_mnb_test_score = gs_lr.score(X_test, y_test)\n",
        "print('Test Accuracy :', best_mnb_test_score)\n",
        "y_pred=gs_lr.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhwGetl1dxmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "sgd_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                        ('sgd', SGDClassifier(random_state=42))\n",
        "                       ])\n",
        "\n",
        "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "              'sgd__alpha': [1e-7, 1e-6, 1e-5, 1e-4]\n",
        "}\n",
        "\n",
        "gs_sgd = GridSearchCV(sgd_pipeline, param_grid, cv=5, verbose=2)\n",
        "gs_sgd = gs_sgd.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGYHjwDGeOpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3e326c66-71c2-4dc1-dab8-a2c307a3bb3c"
      },
      "source": [
        "best_mnb_test_score = gs_sgd.score(X_test, y_test)\n",
        "print('Test Accuracy :', best_mnb_test_score)\n",
        "y_pred=gs_sgd.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy : 0.6399922412120951\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.32      0.40      2121\n",
            "           1       0.55      0.42      0.47      8172\n",
            "           2       0.69      0.85      0.76     23485\n",
            "           3       0.58      0.48      0.53      9859\n",
            "           4       0.54      0.33      0.41      2762\n",
            "\n",
            "    accuracy                           0.64     46399\n",
            "   macro avg       0.58      0.48      0.51     46399\n",
            "weighted avg       0.62      0.64      0.62     46399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjSHqB84gnVs",
        "colab_type": "text"
      },
      "source": [
        "# Text_analytic_apress (Ch05c Text classification II)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCg_wl0eJtv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import text_normalizer as tn # copy py file from text book apress\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUDysv74fuLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "851c56b9-1930-4fdd-a509-b2c0df437931"
      },
      "source": [
        "data_df =pd.read_csv('/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train_df_clean_apress.csv')\n",
        "data_df .head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Clean Article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapade demonstrate adage good goose a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "      <td>series escapade demonstrate adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  ...                                      Clean Article\n",
              "0  A series of escapades demonstrating the adage ...  ...  series escapade demonstrate adage good goose a...\n",
              "1  A series of escapades demonstrating the adage ...  ...       series escapade demonstrate adage good goose\n",
              "2                                           A series  ...                                             series\n",
              "3                                                  A  ...                                                NaN\n",
              "4                                             series  ...                                             series\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBhw_jMP8Y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b6b6a855-07d6-48c5-f3b6-d27a47cad890"
      },
      "source": [
        "print(data_df.shape)\n",
        "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
        "print(data_df.shape)\n",
        "data_df=data_df.dropna()\n",
        "print(data_df.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 3)\n",
            "(156060, 3)\n",
            "(154663, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE-5SpxUJytE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c1839ba7-e0e5-4f59-f8e0-5dce27117f52"
      },
      "source": [
        "X=data_df['Clean Article']\n",
        "y=data_df['Sentiment']\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(108264,)\n",
            "(108264,)\n",
            "(46399,)\n",
            "(46399,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tuWW-22NNDk",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineer with tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9iZa9XaJ6Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_train = [tokenizer.tokenize(text) for text in X_train]\n",
        "tokenized_test = [tokenizer.tokenize(text) for text in X_test]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKQpS_rBKDb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6a73d0de-619b-4eb2-fe47-af6edee39573"
      },
      "source": [
        "print(tokenized_train[0])\n",
        "print(tokenized_train[1])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hard', 'conceive', 'anyone', 'else', 'role']\n",
            "['vivid', 'spicy', 'footnote', 'history', 'movie', 'grip', 'hold', 'rapt', 'attention']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzzgOVv7Lm4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import gensim\n",
        "# build word2vec model\n",
        "w2v_num_features = 1000\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=100,\n",
        "                                   min_count=2, sample=1e-3, sg=1, iter=5, workers=10)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGsmqa84Lv8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def document_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvcBN3a3L1HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = document_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = document_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpPpYsjMFEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8a4620e0-ecad-4577-b9cf-cf48dd1c098c"
      },
      "source": [
        "avg_wv_train_features"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01749741, -0.02980809, -0.08201467, ..., -0.095428  ,\n",
              "         0.04070027, -0.00811128],\n",
              "       [ 0.02002444,  0.04953169, -0.09878676, ...,  0.02323646,\n",
              "        -0.02160464,  0.02102729],\n",
              "       [ 0.051041  ,  0.07597369, -0.08308967, ...,  0.05694336,\n",
              "         0.04472401, -0.01539668],\n",
              "       ...,\n",
              "       [-0.10634755,  0.22360346, -0.11974651, ...,  0.12893571,\n",
              "        -0.11238696,  0.19007462],\n",
              "       [-0.01335325,  0.0123023 , -0.0322293 , ...,  0.03551866,\n",
              "         0.01090056,  0.01617339],\n",
              "       [ 0.01104445,  0.01386654,  0.00407562, ...,  0.11965852,\n",
              "         0.09281164, -0.0274487 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzchBQuQL8jm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "210a3f75-3ace-41a9-977f-0879842e175f"
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, \n",
        "      ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (108264, 1000)  Test features shape: (46399, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWjQ0UWrL-AB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c57b0017-0e24-4d10-8553-ce9d9365e93c"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
        "svm.fit(avg_wv_train_features, y_train)\n",
        "svm_w2v_cv_scores = cross_val_score(svm, avg_wv_train_features, y_train, cv=5)\n",
        "svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
        "print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
        "print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
        "svm_w2v_test_score = svm.score(avg_wv_test_features, y_test)\n",
        "print('Test Accuracy:', svm_w2v_test_score)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.51226158 0.50741237 0.50653489 0.51036808 0.5077591 ]\n",
            "Mean CV Accuracy: 0.5088672032429545\n",
            "Test Accuracy: 0.5037824091036445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrnmiF6yMsS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "46c0f8ee-9e5d-40a2-8301-8b01df3da6fd"
      },
      "source": [
        "y_pred = svm.predict(avg_wv_test_features)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print('Accuracy score:',accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  101    38  1972     9     1]\n",
            " [  283   145  7648    85    11]\n",
            " [  369   245 22424   379    68]\n",
            " [  213    72  8828   650    96]\n",
            " [   63    20  2340   284    55]]\n",
            "Accuracy score: 0.5037824091036445\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.05      0.06      2121\n",
            "           1       0.28      0.02      0.03      8172\n",
            "           2       0.52      0.95      0.67     23485\n",
            "           3       0.46      0.07      0.12      9859\n",
            "           4       0.24      0.02      0.04      2762\n",
            "\n",
            "    accuracy                           0.50     46399\n",
            "   macro avg       0.32      0.22      0.18     46399\n",
            "weighted avg       0.43      0.50      0.38     46399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbvqaeztM7kX",
        "colab_type": "text"
      },
      "source": [
        "## feature engineering with GloVe model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p0yiAdHMSCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b70806a4-93ee-4baf-ea1f-5d545f99bd56"
      },
      "source": [
        "train_nlp = [nlp(item) for item in X_train]\n",
        "train_glove_features = np.array([item.vector for item in train_nlp])\n",
        "\n",
        "test_nlp = [nlp(item) for item in X_test]\n",
        "test_glove_features = np.array([item.vector for item in test_nlp])\n",
        "\n",
        "print('GloVe model:> Train features shape:', train_glove_features.shape, \n",
        "      ' Test features shape:', test_glove_features.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GloVe model:> Train features shape: (108264, 96)  Test features shape: (46399, 96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQIPmmEPSOOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "63345c43-3ee9-4d48-cb08-ef4b21fb5536"
      },
      "source": [
        "train_glove_features"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.69651496, -0.58739054, -0.4476394 , ...,  2.668711  ,\n",
              "         1.0262415 ,  0.8236513 ],\n",
              "       [ 0.61140436, -0.56227183, -0.9054608 , ...,  2.2462704 ,\n",
              "         1.4750795 ,  0.61924607],\n",
              "       [-0.04170674, -1.9687984 , -0.84633666, ...,  2.4241426 ,\n",
              "         0.391186  ,  0.08026916],\n",
              "       ...,\n",
              "       [ 1.4768339 , -1.043088  ,  0.08320889, ...,  2.5933266 ,\n",
              "        -0.34999055,  0.13659966],\n",
              "       [-0.8503346 , -0.7636716 , -3.1226912 , ...,  1.3520709 ,\n",
              "         1.07134   ,  0.95008326],\n",
              "       [ 0.56617945, -1.2792655 , -0.63506716, ...,  1.5612081 ,\n",
              "         0.644849  ,  0.81388825]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XretCByeRZEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5a9b8907-df81-4ece-97de-08d55bc3cbe4"
      },
      "source": [
        "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
        "svm.fit(train_glove_features, y_train)\n",
        "svm_glove_cv_scores = cross_val_score(svm, train_glove_features, y_train, cv=5)\n",
        "svm_glove_cv_mean_score = np.mean(svm_glove_cv_scores)\n",
        "print('CV Accuracy (5-fold):', svm_glove_cv_scores)\n",
        "print('Mean CV Accuracy:', svm_glove_cv_mean_score)\n",
        "\n",
        "svm_glove_test_score = svm.score(test_glove_features, y_test)\n",
        "print('Test Accuracy:', svm_glove_test_score)\n",
        "\n",
        "y_pred = svm.predict(test_glove_features)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print('Accuracy score:',accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.4948506  0.5092135  0.48663003 0.51433981 0.50346388]\n",
            "Mean CV Accuracy: 0.5016995655464209\n",
            "Test Accuracy: 0.5042565572533891\n",
            "[[  128    17  1914    50    12]\n",
            " [  204    39  7842    73    14]\n",
            " [  214    74 23055   123    19]\n",
            " [  233    62  9367   154    43]\n",
            " [  133    15  2490   103    21]]\n",
            "Accuracy score: 0.5042565572533891\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.06      0.08      2121\n",
            "           1       0.19      0.00      0.01      8172\n",
            "           2       0.52      0.98      0.68     23485\n",
            "           3       0.31      0.02      0.03      9859\n",
            "           4       0.19      0.01      0.01      2762\n",
            "\n",
            "    accuracy                           0.50     46399\n",
            "   macro avg       0.27      0.21      0.16     46399\n",
            "weighted avg       0.38      0.50      0.36     46399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA9rh3dRtmv",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineer with Fast text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ5YJBYJO9Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "ft_num_features = 1000\n",
        "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
        "ft_model = FastText(X_train, size=ft_num_features, window=100, \n",
        "                    min_count=2, sample=1e-3, sg=1, iter=5, workers=10)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPvoBwpvRyjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_ft_train_features = document_vectorizer(corpus=X_train, model=ft_model,\n",
        "                                                     num_features=ft_num_features)\n",
        "avg_ft_test_features = document_vectorizer(corpus=X_test, model=ft_model,\n",
        "                                                    num_features=ft_num_features)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoCoVq4uSI2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "af286f89-b040-431c-8a6a-91548f789f91"
      },
      "source": [
        "avg_ft_train_features"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02222717, -0.00758671, -0.0162897 , ..., -0.00951721,\n",
              "        -0.00479766,  0.02819935],\n",
              "       [ 0.02768434, -0.00993421, -0.01953908, ..., -0.00962588,\n",
              "        -0.00421079,  0.03015218],\n",
              "       [ 0.02835886, -0.00818061, -0.02128811, ..., -0.00931483,\n",
              "        -0.00551764,  0.03362453],\n",
              "       ...,\n",
              "       [ 0.03395242, -0.01106577, -0.0238804 , ..., -0.00900555,\n",
              "        -0.00419262,  0.02550046],\n",
              "       [ 0.02684787, -0.00918894, -0.01918294, ..., -0.00982829,\n",
              "        -0.00504065,  0.02865739],\n",
              "       [ 0.02562106, -0.00780863, -0.01850835, ..., -0.00962557,\n",
              "        -0.00457306,  0.02755714]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LxUvAB0R067",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8f498f22-615a-4529-f568-365de5e92b2b"
      },
      "source": [
        "print('FastText model:> Train features shape:', avg_ft_train_features.shape, \n",
        "      ' Test features shape:', avg_ft_test_features.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FastText model:> Train features shape: (108264, 1000)  Test features shape: (46399, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH5-E-ApR1tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "b1b634fa-031f-4c78-fb97-86dfcc41b4b9"
      },
      "source": [
        "\n",
        "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
        "svm.fit(avg_ft_train_features, y_train)\n",
        "svm_ft_cv_scores = cross_val_score(svm, avg_ft_train_features, y_train, cv=5)\n",
        "svm_ft_cv_mean_score = np.mean(svm_ft_cv_scores)\n",
        "print('CV Accuracy (5-fold):', svm_ft_cv_scores)\n",
        "print('Mean CV Accuracy:', svm_ft_cv_mean_score)\n",
        "\n",
        "svm_ft_test_score = svm.score(avg_ft_test_features, y_test)\n",
        "print('Test Accuracy:', svm_ft_test_score)\n",
        "\n",
        "y_pred = svm.predict(avg_ft_test_features)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print('Accuracy score:',accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy (5-fold): [0.50602688 0.50616543 0.50616543 0.50611924 0.50577314]\n",
            "Mean CV Accuracy: 0.5060500233052116\n",
            "Test Accuracy: 0.5061315976637427\n",
            "[[    0     0  2121     0     0]\n",
            " [    0     0  8172     0     0]\n",
            " [    0     0 23484     0     1]\n",
            " [    0     0  9859     0     0]\n",
            " [    0     0  2762     0     0]]\n",
            "Accuracy score: 0.5061315976637427\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2121\n",
            "           1       0.00      0.00      0.00      8172\n",
            "           2       0.51      1.00      0.67     23485\n",
            "           3       0.00      0.00      0.00      9859\n",
            "           4       0.00      0.00      0.00      2762\n",
            "\n",
            "    accuracy                           0.51     46399\n",
            "   macro avg       0.10      0.20      0.13     46399\n",
            "weighted avg       0.26      0.51      0.34     46399\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp1tt8eXSRZv",
        "colab_type": "text"
      },
      "source": [
        "## MLP classifier (Fast text feature engineer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh5Lu8o3R1rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(solver='adam', alpha=1e-5, learning_rate='adaptive', early_stopping=True,\n",
        "                    activation = 'relu', hidden_layer_sizes=(512, 512), random_state=42)\n",
        "mlp.fit(avg_ft_train_features, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4R7T2B5R1oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_ft_test_score = mlp.score(avg_ft_test_features, y_test)\n",
        "print('Test Accuracy:', svm_ft_test_score)\n",
        "\n",
        "y_pred = mlp.predict(avg_ft_test_features)\n",
        "print('Accuracy score:',accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QThbrX8GUwA8",
        "colab_type": "text"
      },
      "source": [
        "# Text_analytic_apress Chap9 sentiment analysis- Unsupervised Lexical (two class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmMVbMCi1B_r",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis with textblob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsewilI9R1la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import text_normalizer as tn # specific ebook library\n",
        "#import model_evaluation_utils as meu # specific ebook library\n",
        "import nltk\n",
        "import textblob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8pcL1htVHJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2bae6aa6-e18d-450a-8d69-ace5f85c0a67"
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "data_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(data_df.shape)\n",
        "data_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99FGEGMlztU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "01abc639-77c2-4c3d-f9ce-0c845d736871"
      },
      "source": [
        "data_df_new=data_df.copy()\n",
        "data_df_new['Sentiment'].value_counts()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    79582\n",
              "3    32927\n",
              "1    27273\n",
              "4     9206\n",
              "0     7072\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFAJhpFzzR5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dc8070df-3296-473d-95e4-70a78926a516"
      },
      "source": [
        "data_df_new['Sentiment']=data_df_new['Sentiment'].apply(lambda x: 0 if x<= 2 else 1)\n",
        "data_df_new.head()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          0\n",
              "1         2  ...          0\n",
              "2         3  ...          0\n",
              "3         4  ...          0\n",
              "4         5  ...          0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G2XnrCM0Ent",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "032a350b-ae5e-4306-adb3-ae3ce4ce69de"
      },
      "source": [
        "data_df_new['Sentiment'].value_counts()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    113927\n",
              "1     42133\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSvFvdPubZYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = np.array(data_df_new['Phrase'])\n",
        "sentiments = np.array(data_df_new['Sentiment'])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBf28-qfyexg",
        "colab_type": "text"
      },
      "source": [
        "extract data for model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI6949GSyeLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81f0a1b7-b5af-4064-87fd-2db522728963"
      },
      "source": [
        "split_ratio=0.7\n",
        "split=int(data_df.shape[0]*0.7)\n",
        "split"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyoKH1vfyJRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_reviews = reviews[split:]\n",
        "test_sentiments = sentiments[split:]\n",
        "sample_review_ids = [7626, 3533, 13010]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQgKGQoWyuKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8e6372f5-d382-4338-b0dc-0df4434c8556"
      },
      "source": [
        "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
        "    print('-'*60)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: those sticks\n",
            "Actual Sentiment: 0\n",
            "Predicted Sentiment polarity: 0.0\n",
            "------------------------------------------------------------\n",
            "REVIEW: can admire but is difficult to connect with on any deeper level\n",
            "Actual Sentiment: 0\n",
            "Predicted Sentiment polarity: -0.5\n",
            "------------------------------------------------------------\n",
            "REVIEW: of those moments\n",
            "Actual Sentiment: 0\n",
            "Predicted Sentiment polarity: 0.0\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrXhn5ICzAqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVUr_RQL0XbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentiments = [1 if score >= 0.1 else 0 for score in sentiment_polarity]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5tPt2RY0d3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1297b024-4f6c-4f79-be81-93688531cca9"
      },
      "source": [
        "print(classification_report(test_sentiments,predicted_sentiments))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.82     34073\n",
            "           1       0.52      0.53      0.52     12745\n",
            "\n",
            "    accuracy                           0.74     46818\n",
            "   macro avg       0.67      0.67      0.67     46818\n",
            "weighted avg       0.74      0.74      0.74     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeNTUk0S1HGq",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis with AFINN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZMRxnbN2iFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "6c658559-d422-4858-9126-2967fb491a9f"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp36-none-any.whl size=53452 sha256=1642f0b7505fb178230984e29f2cd0ff49cc51ad711c6e0d8e309cf3657e20e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kysCfMrT0y6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "afn = Afinn(emoticons=True)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__iFsUZk2bnz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6a4a731d-ed6f-4333-c70c-721d2fbceb96"
      },
      "source": [
        "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    print('Predicted Sentiment polarity:', afn.score(review))\n",
        "    print('-'*60)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: those sticks\n",
            "Actual Sentiment: 0\n",
            "Predicted Sentiment polarity: 0.0\n",
            "------------------------------------------------------------\n",
            "REVIEW: can admire but is difficult to connect with on any deeper level\n",
            "Actual Sentiment: 0\n",
            "Predicted Sentiment polarity: 2.0\n",
            "------------------------------------------------------------\n",
            "REVIEW: of those moments\n",
            "Actual Sentiment: 0\n",
            "Predicted Sentiment polarity: 0.0\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o8WSw0X2oj4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f8030d2d-bbc6-4a4a-c908-52bb837c052c"
      },
      "source": [
        "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
        "predicted_sentiments = [1 if score >= 1.0 else 0 for score in sentiment_polarity]\n",
        "print(classification_report(test_sentiments,predicted_sentiments))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83     34073\n",
            "           1       0.54      0.53      0.54     12745\n",
            "\n",
            "    accuracy                           0.75     46818\n",
            "   macro avg       0.68      0.68      0.68     46818\n",
            "weighted avg       0.75      0.75      0.75     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUTSE7LG23gF",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis with SentiWordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWi40zY2xRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e40511f9-7226-4daa-b239-1046e243f935"
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "import nltk\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
        "print('Positive Polarity Score:', awesome.pos_score())\n",
        "print('Negative Polarity Score:', awesome.neg_score())\n",
        "print('Objective Score:', awesome.obj_score())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Positive Polarity Score: 0.875\n",
            "Negative Polarity Score: 0.125\n",
            "Objective Score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WINmcK4X3K2z",
        "colab_type": "text"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDH99rTz29Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_sentiment_sentiwordnet_lexicon(review,\n",
        "                                           verbose=False):\n",
        "\n",
        "    # tokenize and POS tag text tokens\n",
        "    tagged_text = [(token.text, token.tag_) for token in nlp(review)] # from text_normalizer.py\n",
        "    pos_score = neg_score = token_count = obj_score = 0\n",
        "    # get wordnet synsets based on POS tags\n",
        "    # get sentiment scores if synsets are found\n",
        "    for word, tag in tagged_text:\n",
        "        ss_set = None\n",
        "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
        "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
        "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
        "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
        "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
        "        # if senti-synset is found        \n",
        "        if ss_set:\n",
        "            # add scores for all found synsets\n",
        "            pos_score += ss_set.pos_score()\n",
        "            neg_score += ss_set.neg_score()\n",
        "            obj_score += ss_set.obj_score()\n",
        "            token_count += 1\n",
        "    \n",
        "    # aggregate final scores\n",
        "    final_score = pos_score - neg_score\n",
        "    norm_final_score = round(float(final_score) / token_count, 2)\n",
        "    final_sentiment = 1 if norm_final_score >= 0 else 0\n",
        "    if verbose:\n",
        "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
        "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
        "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
        "        # to display results in a nice table\n",
        "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n",
        "                                         norm_neg_score, norm_final_score]],\n",
        "                                       columns = ['Predicted Sentiment', 'Objectivity',\n",
        "                                                     'Positive', 'Negative', 'Overall']\n",
        "                                       #columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
        "                                       #                      ['Predicted Sentiment', 'Objectivity',\n",
        "                                        #                      'Positive', 'Negative', 'Overall']], \n",
        "                                       #                      labels=[[0,0,0,0,0],[0,1,2,3,4]])\n",
        "                                       )\n",
        "        print(sentiment_frame)\n",
        "        \n",
        "    return final_sentiment"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4-rv9X73OAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4de6188e-1341-4a0d-ee39-391da5e663d7"
      },
      "source": [
        "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    \n",
        "    print('-'*60)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: those sticks\n",
            "Actual Sentiment: 0\n",
            "   Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0                    1          1.0       0.0       0.0      0.0\n",
            "------------------------------------------------------------\n",
            "REVIEW: can admire but is difficult to connect with on any deeper level\n",
            "Actual Sentiment: 0\n",
            "   Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0                    0         0.58      0.19      0.23    -0.04\n",
            "------------------------------------------------------------\n",
            "REVIEW: of those moments\n",
            "Actual Sentiment: 0\n",
            "   Predicted Sentiment  Objectivity  Positive  Negative  Overall\n",
            "0                    1          1.0       0.0       0.0      0.0\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEOzFW6Z3Qdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f3d64c5f-b157-4570-bfc9-ed5a1ca8b6af"
      },
      "source": [
        "norm_test_reviews = normalize_corpus(test_reviews)\n",
        "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-17a9bd005996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-17a9bd005996>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-abc0c0544880>\u001b[0m in \u001b[0;36manalyze_sentiment_sentiwordnet_lexicon\u001b[0;34m(review, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# aggregate final scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfinal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mnorm_final_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtoken_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mfinal_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnorm_final_score\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzj0bqYm8xar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(test_sentiments,predicted_sentiments))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Q_EKhX-D9e",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis with VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCKSh-7y-EjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0ab0c072-517d-4665-8bbc-f017add7c33b"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ePTf4gk-HD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_sentiment_vader_lexicon(review, \n",
        "                                    threshold=0.1,\n",
        "                                    verbose=False):\n",
        "    # pre-process text\n",
        "    review = strip_html_tags(review)\n",
        "    review = remove_accented_chars(review)\n",
        "    review = expand_contractions(review)\n",
        "    \n",
        "    # analyze the sentiment for review\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(review)\n",
        "    # get aggregate scores and final sentiment\n",
        "    agg_score = scores['compound']\n",
        "    final_sentiment = 1 if agg_score >= threshold else 0\n",
        "    #1: positive, o: negative\n",
        "    if verbose:\n",
        "        # display detailed sentiment statistics\n",
        "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
        "        final = round(agg_score, 2)\n",
        "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
        "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
        "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
        "                                        negative, neutral]],\n",
        "                                       columns = ['Predicted Sentiment', 'Polarity Score',\n",
        "                                                     'Positive', 'Negative', 'Overall']\n",
        "                                        #columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
        "                                        #                              ['Predicted Sentiment', 'Polarity Score',\n",
        "                                         #                              'Positive', 'Negative', 'Neutral']], \n",
        "                                          #                    labels=[[0,0,0,0,0],[0,1,2,3,4]])\n",
        "                                        )\n",
        "        print(sentiment_frame)\n",
        "    \n",
        "    return final_sentiment"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8aIinxz-f5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b4018b22-9181-44ab-e8d2-335240967f69"
      },
      "source": [
        "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    \n",
        "    print('-'*60)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: those sticks\n",
            "Actual Sentiment: 0\n",
            "   Predicted Sentiment  Polarity Score Positive Negative Overall\n",
            "0                    0             0.0     0.0%     0.0%  100.0%\n",
            "------------------------------------------------------------\n",
            "REVIEW: can admire but is difficult to connect with on any deeper level\n",
            "Actual Sentiment: 0\n",
            "   Predicted Sentiment  Polarity Score Positive Negative Overall\n",
            "0                    0            -0.3    13.0%    21.0%   65.0%\n",
            "------------------------------------------------------------\n",
            "REVIEW: of those moments\n",
            "Actual Sentiment: 0\n",
            "   Predicted Sentiment  Polarity Score Positive Negative Overall\n",
            "0                    0             0.0     0.0%     0.0%  100.0%\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6zBe0pt-f3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WpOjoQx-fz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "29feacdc-94e6-41a6-a3fa-f245e7472155"
      },
      "source": [
        "print(classification_report(test_sentiments,predicted_sentiments))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85     34073\n",
            "           1       0.61      0.44      0.51     12745\n",
            "\n",
            "    accuracy                           0.77     46818\n",
            "   macro avg       0.71      0.67      0.68     46818\n",
            "weighted avg       0.75      0.77      0.76     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zEhAWAy-fxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ql7gu4u-ftu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
