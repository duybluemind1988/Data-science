{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movies_review_sentiment.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_movie_sentiment/Movies_review_sentiment2.ipynb",
      "authorship_tag": "ABX9TyNS5Mm9VCWYkdjCQTABg/Qc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/NLP/Kaggle_movie_sentiment/Movies_review_sentiment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqma_rg6Vlzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jul 31 20:05:23 2017\n",
        "@author: DIP\n",
        "@Copyright: Dipanjan Sarkar\n",
        "\"\"\"\n",
        "\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_curve, auc \n",
        "\n",
        "\n",
        "def get_metrics(true_labels, predicted_labels):\n",
        "    \n",
        "    print('Accuracy:', np.round(\n",
        "                        metrics.accuracy_score(true_labels, \n",
        "                                               predicted_labels),\n",
        "                        4))\n",
        "    print('Precision:', np.round(\n",
        "                        metrics.precision_score(true_labels, \n",
        "                                               predicted_labels,\n",
        "                                               average='weighted'),\n",
        "                        4))\n",
        "    print('Recall:', np.round(\n",
        "                        metrics.recall_score(true_labels, \n",
        "                                               predicted_labels,\n",
        "                                               average='weighted'),\n",
        "                        4))\n",
        "    print('F1 Score:', np.round(\n",
        "                        metrics.f1_score(true_labels, \n",
        "                                               predicted_labels,\n",
        "                                               average='weighted'),\n",
        "                        4))\n",
        "                        \n",
        "\n",
        "def train_predict_model(classifier, \n",
        "                        train_features, train_labels, \n",
        "                        test_features, test_labels):\n",
        "    # build model    \n",
        "    classifier.fit(train_features, train_labels)\n",
        "    # predict using model\n",
        "    predictions = classifier.predict(test_features) \n",
        "    return predictions    \n",
        "\n",
        "\n",
        "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
        "    \n",
        "    total_classes = len(classes)\n",
        "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
        "\n",
        "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
        "                                  labels=classes)\n",
        "    cm_frame = pd.DataFrame(data=cm, \n",
        "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
        "                                                  labels=level_labels), \n",
        "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
        "                                                labels=level_labels)) \n",
        "    print(cm_frame) \n",
        "\n",
        "\n",
        "def display_confusion_matrix_pretty(true_labels, predicted_labels, classes=[1,0]):\n",
        "    \n",
        "    total_classes = len(classes)\n",
        "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
        "\n",
        "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
        "                                  labels=classes)\n",
        "    cm_frame = pd.DataFrame(data=cm, \n",
        "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
        "                                                  labels=level_labels), \n",
        "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
        "                                                labels=level_labels)) \n",
        "    return cm_frame\n",
        "    \n",
        "def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n",
        "\n",
        "    report = metrics.classification_report(y_true=true_labels, \n",
        "                                           y_pred=predicted_labels, \n",
        "                                           labels=classes) \n",
        "    print(report)\n",
        "    \n",
        "    \n",
        "    \n",
        "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n",
        "    print('Model Performance metrics:')\n",
        "    print('-'*30)\n",
        "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
        "    print('\\nModel Classification report:')\n",
        "    print('-'*30)\n",
        "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n",
        "                                  classes=classes)\n",
        "    print('\\nPrediction Confusion Matrix:')\n",
        "    print('-'*30)\n",
        "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n",
        "                             classes=classes)\n",
        "\n",
        "\n",
        "def plot_model_decision_surface(clf, train_features, train_labels,\n",
        "                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n",
        "                                markers=None, alphas=None, colors=None):\n",
        "    \n",
        "    if train_features.shape[1] != 2:\n",
        "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
        "    \n",
        "    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n",
        "    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                         np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "    clf_est = clone(clf)\n",
        "    clf_est.fit(train_features,train_labels)\n",
        "    if hasattr(clf_est, 'predict_proba'):\n",
        "        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
        "    else:\n",
        "        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(train_labels)\n",
        "    n_classes = len(le.classes_)\n",
        "    plot_colors = ''.join(colors) if colors else [None] * n_classes\n",
        "    label_names = le.classes_\n",
        "    markers = markers if markers else [None] * n_classes\n",
        "    alphas = alphas if alphas else [None] * n_classes\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(y_enc == i)\n",
        "        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n",
        "                    label=label_names[i], cmap=cmap, edgecolors='black', \n",
        "                    marker=markers[i], alpha=alphas[i])\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n",
        "    \n",
        "    ## Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    if hasattr(clf, 'classes_'):\n",
        "        class_labels = clf.classes_\n",
        "    elif label_encoder:\n",
        "        class_labels = label_encoder.classes_\n",
        "    elif class_names:\n",
        "        class_labels = class_names\n",
        "    else:\n",
        "        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n",
        "    n_classes = len(class_labels)\n",
        "    y_test = label_binarize(true_labels, classes=class_labels)\n",
        "    if n_classes == 2:\n",
        "        if hasattr(clf, 'predict_proba'):\n",
        "            prob = clf.predict_proba(features)\n",
        "            y_score = prob[:, prob.shape[1]-1] \n",
        "        elif hasattr(clf, 'decision_function'):\n",
        "            prob = clf.decision_function(features)\n",
        "            y_score = prob[:, prob.shape[1]-1]\n",
        "        else:\n",
        "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
        "        \n",
        "        fpr, tpr, _ = roc_curve(y_test, y_score)      \n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
        "                                 ''.format(roc_auc),\n",
        "                 linewidth=2.5)\n",
        "        \n",
        "    elif n_classes > 2:\n",
        "        if hasattr(clf, 'predict_proba'):\n",
        "            y_score = clf.predict_proba(features)\n",
        "        elif hasattr(clf, 'decision_function'):\n",
        "            y_score = clf.decision_function(features)\n",
        "        else:\n",
        "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        ## Compute micro-average ROC curve and ROC area\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "        ## Compute macro-average ROC curve and ROC area\n",
        "        # First aggregate all false positive rates\n",
        "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "        # Then interpolate all ROC curves at this points\n",
        "        mean_tpr = np.zeros_like(all_fpr)\n",
        "        for i in range(n_classes):\n",
        "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "        # Finally average it and compute AUC\n",
        "        mean_tpr /= n_classes\n",
        "        fpr[\"macro\"] = all_fpr\n",
        "        tpr[\"macro\"] = mean_tpr\n",
        "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "        ## Plot ROC curves\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
        "                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n",
        "\n",
        "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
        "                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n",
        "\n",
        "        for i, label in enumerate(class_labels):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(label, roc_auc[i]), \n",
        "                     linewidth=2, linestyle=':')\n",
        "    else:\n",
        "        raise ValueError('Number of classes should be atleast 2 or more')\n",
        "        \n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORWZxYZWnzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "test_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/test.tsv'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1LtJ8a-WuP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_9x2xcHXO8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "983e2342-363e-4645-a4ba-fb57778524b5"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AV4Gg7XX_vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6b84deed-188b-4270-b106-f10ff9baa412"
      },
      "source": [
        "train_df['Sentiment'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    79582\n",
              "3    32927\n",
              "1    27273\n",
              "4     9206\n",
              "0     7072\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiw_EN5lZjVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRxn1HsUaFvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2b222c40-9325-4b87-c13a-c5fac1051994"
      },
      "source": [
        "X"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         A series of escapades demonstrating the adage ...\n",
              "1         A series of escapades demonstrating the adage ...\n",
              "2                                                  A series\n",
              "3                                                         A\n",
              "4                                                    series\n",
              "                                ...                        \n",
              "156055                                            Hearst 's\n",
              "156056                            forced avuncular chortles\n",
              "156057                                   avuncular chortles\n",
              "156058                                            avuncular\n",
              "156059                                             chortles\n",
              "Name: Phrase, Length: 156060, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaT8XZqRaCCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "39cb9877-5099-4874-89e9-0ac16fa6c35f"
      },
      "source": [
        "y"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         1\n",
              "1         2\n",
              "2         2\n",
              "3         2\n",
              "4         2\n",
              "         ..\n",
              "156055    2\n",
              "156056    1\n",
              "156057    3\n",
              "156058    2\n",
              "156059    2\n",
              "Name: Sentiment, Length: 156060, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4196zxaXZ3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e238799b-abe1-40a7-d692-5ab179508043"
      },
      "source": [
        "test_df=pd.read_csv(test_path,sep='\\t')\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(66292, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>8545</td>\n",
              "      <td>An</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase\n",
              "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
              "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
              "2    156063        8545                                                 An\n",
              "3    156064        8545  intermittently pleasing but mostly routine effort\n",
              "4    156065        8545         intermittently pleasing but mostly routine"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcoTIoTZCvT",
        "colab_type": "text"
      },
      "source": [
        "Text have no label, so we split training set to train and text set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRjqjw3YJsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMJZjXxOZXlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4CHcYlYZqu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e1315f83-5c16-4fe6-cfb8-5f28b158b801"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDf2PKyGZyCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "148d3a07-eb6d-4686-8846-65147d56ec93"
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509950\n",
              "3    0.210990\n",
              "1    0.174759\n",
              "4    0.058988\n",
              "0    0.045312\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK9lRF3zZ8MB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "22ee707a-58f8-4fa6-8065-2d17b73056c3"
      },
      "source": [
        "y_test.value_counts(normalize=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509932\n",
              "3    0.210987\n",
              "1    0.174762\n",
              "4    0.058994\n",
              "0    0.045324\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4GK8KGaZVg",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification - I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXtMI2SGafbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install text_normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPP_3GpAa_Gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "361ff0f4-c89f-4a89-ce63-82739f099a97"
      },
      "source": [
        "!pip install demoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/da/0b/d008f26ebbfd86d21117267e627f2f7359c76e5ecbeba08d8f631f4092c4/demoji-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji) (49.2.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.3 demoji-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ODLlPqFqL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ebb8ddc7-0deb-4d43-f46a-e59e57155afb"
      },
      "source": [
        "'''\n",
        "Contains helper funtions for preprocessing of twitter documents before getting their corresponding vectors\n",
        "from word2vec_twitter model\n",
        "Author: Anuj Gupta\n",
        "'''\n",
        "\n",
        "'''\n",
        "functionality:\n",
        "Discard non-english tweets\n",
        "Discard Replies\n",
        "Discard RTs\n",
        "For now this is handle while fetch tweet text from mongo documents\n",
        "process constantbrands mentions\n",
        "process any other mentions\n",
        "process constant brand name if any \n",
        "process hanstags\n",
        "process URLs\n",
        "process websites\n",
        "process process_EmailIds\n",
        "process \n",
        "process alphanums\n",
        "'''\n",
        "\n",
        "import re\n",
        "import string\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "#gobal\n",
        "PunctChars = r'''[`'â€œ\".?!,:;]'''\n",
        "Punct = '%s+' % PunctChars\n",
        "Entity = '&(amp|lt|gt|quot);'\n",
        "printable = set(string.printable)\n",
        "\n",
        "# helper functoins\n",
        "def regex_or(*items):\n",
        "\tr = '|'.join(items)\n",
        "\tr = '(' + r + ')'\n",
        "\treturn r\n",
        "\n",
        "def pos_lookahead(r):\n",
        "\treturn '(?=' + r + ')'\n",
        "\n",
        "def neg_lookahead(r):\n",
        "\treturn '(?!' + r + ')'\n",
        "\n",
        "def optional(r):\n",
        "\treturn '(%s)?' % r\n",
        "\n",
        "def trim(transient_tweet_text):\n",
        "\t''' \n",
        "\ttrim leading and trailing spaces in the tweet text\n",
        "\t'''\n",
        "\treturn transient_tweet_text.strip()\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def to_LowerCase(transient_tweet_text):\n",
        "\t'''\n",
        "\tConvert tweet text to lower to lower case alphabets\n",
        "\t'''\n",
        "\ttransient_tweet_text = transient_tweet_text.lower()\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_unicode(transient_tweet_text):\n",
        "    '''\n",
        "    Strip all unicode characters from a tweet\n",
        "    '''\n",
        "    tweet = ''.join(i for i in transient_tweet_text if ord(i)<128)\n",
        "    return tweet \n",
        "\n",
        "def process_URLs(transient_tweet_text):\n",
        "\t'''\n",
        "\treplace all URLs in the tweet text\n",
        "\t'''\n",
        "\tUrlStart1 = regex_or('https?://', r'www\\.',r'bit.ly/')\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tUrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + pos_lookahead(r'[/ \\W\\b]')\n",
        "\tUrlBody = r'[^ \\t\\r\\n<>]*?'  # * not + for case of:  \"go to bla.com.\" -- don't want period\n",
        "\tUrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n",
        "\tUrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n",
        "\tUrl = \t(optional(r'\\b') + \n",
        "    \t\tregex_or(UrlStart1, UrlStart2) + \n",
        "    \t\tUrlBody + \n",
        "    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n",
        "\n",
        "\tUrl_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(Url_RE, \" constanturl \", transient_tweet_text)\n",
        "\n",
        "\t# fix to handle unicodes in URL\n",
        "\tURL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*'\n",
        "\ttransient_tweet_text = re.sub(URL_regex2, \" constanturl \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Websites(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify website mentioned if any \n",
        "\t'''\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tsep = r'[.]'\n",
        "\twebsite_regex = r'(?<!#)?(\\b)?[a-zA-Z0-9.]+' + sep + CommonTLDs\n",
        "\twebsite_RE = re.compile(\"(%s)\" % website_regex, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(website_RE, ' constantwebsite ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_EmailIds(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify email mentioned if any\n",
        "\t'''\n",
        "\temail_regex = r'(\\b)?[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(\\b)?'\n",
        "\ttransient_tweet_text = re.sub(email_regex, ' constantemailid ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Mentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify mentions if any\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", \" constantnonbrandmention \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "def process_HashTags(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all Hashtags from a tweet\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"#(\\w+)\\b\", ' constanthashtag ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Dates(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify date and convert it to constant\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'(\\d+/\\d+/\\d+)', \" constantdate \" , transient_tweet_text)\n",
        "\t#transient_tweet_text = re.sub(r'constantnum[\\s]?(/|-)[\\s]?constantnum[\\s]?(/|-)[\\s]?constantnum', \" constantdate \" , transient_tweet_text)\n",
        "\t#date_regex = r'(constantnum)[\\s]*(st|nd|rd|th)[\\s]*(january|jan|february|feb|march|mar|april|may|june|jun|july|august|aug|september|sep|october|oct|november|nov|december|dec)'\n",
        "\tdate_regex1 = r'\\b((0|1|2|3)?[0-9][\\s]*)[-./]([\\s]*([012]?[0-9])[\\s]*)([-./]([\\s]*(19|20)[0-9][0-9]))?\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex1, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex2 = r'\\b((19|20)[0-9][0-9][\\s]*[-./]?)?[\\s]*([012]?[0-9])[\\s]*[-./][\\s]*(0|1|2|3)?[0-9]\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex2, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tMonths = regex_or('january','jan','february','feb','march','mar','april','may','june','jun','july','jul','august','aug','september','sep','october','oct','november','nov','december','dec')\n",
        "\tdate_regex3 = r'\\d+[\\s]*(st|nd|rd|th)[\\s]*' + Months \n",
        "\ttransient_tweet_text = re.sub(date_regex3, ' constantdate ', transient_tweet_text)\n",
        "\tdate_regex4 = Months + r'[\\s]*\\d+[\\s]*(st|nd|rd|th)*\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex4, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex5 = r'[\\s]?\\b(19|20)[0-9][0-9]\\b[\\s]?'\n",
        "\ttransient_tweet_text = re.sub(date_regex5, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tdate_regex6 = r'([\\s]*(constantdate))+'\n",
        "\ttransient_tweet_text = re.sub(date_regex6, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Times(transient_tweet_text):\n",
        "\t'''\n",
        "\tIndentify time and convert it to constant\n",
        "\t'''\n",
        "\ttime_regex1 = r'([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9][\\s]*(am|pm)?[\\s]*([iescm](st)|gmt|utc|[pmce](dt))?'\n",
        "\ttransient_tweet_text = re.sub(time_regex1, ' constanttime ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com','shopohelp','shopo','mobikwikswat',\n",
        "\t\t'mobikwik','taxiforsure','tfscares','zoomcarindia','freshmenuindia','freshmenucares','grofers','jetairways',\n",
        "\t\t'cleartrip','olacabs','support_ola','makemytrip','makemytripcare','chai_point')\n",
        "\n",
        "\tBrandMentionRegex = r'@(\\b)*' + constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandMentionRegex, ' constantbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_NonBrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all Mentions left, if any, in tweet text\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", ' constantnonbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandName(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com', 'bluestone','shopohelp','shopo',\n",
        "\t\t'mobikwikswat','mobikwik','taxiforsure','tfscares', 'tfs','zoomcarindia', 'zoomcar','freshmenuindia','freshmenucares', \n",
        "\t\t'freshmenu','grofers','jetairways', 'jet','cleartrip','olacabs','support_ola', 'olacab', 'ola' ,'makemytrip',\n",
        "\t\t'makemytripcare', 'chai_point')\n",
        "\n",
        "\tBrandNameRegex = constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandNameRegex, ' constantbrandname ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_Savings(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify sale/save offers\n",
        "\t'''\n",
        "\t#sale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*(constantnum)*[\\s]*[%]?[\\s]*(-|~)?[\\s]*(constantnum)*[\\s]*[%]?'\n",
        "\tsale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(-|~|or)?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?'\n",
        "\ttransient_tweet_text = re.sub(sale_regex, \" constantdiscount \", transient_tweet_text)\n",
        "\t#discount_List = []\n",
        "\t#discount_List = re.findall(r'constantdiscount', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Offers(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify cashbacks and off / substrings of the form \"30% off\" or \"30% cashback\" or \"$30 off\"\n",
        "\tReplace them by constantOFFER\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'[rs|$]?[ ]*[constantnum][ ]*[%]?[ ]?[off|cashback|offer]', \"constantoffer\", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:(up[\\s]?to)?((rs|\\$)*[\\s]*(constantnum))[\\s]*[%]?)?[\\s]*[-|~|or]?[\\.]?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(offer|off|cashback|cash|cash back)', \" constantoffer \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:cashback|cash back|cash)\\b', \" constantoffer \", transient_tweet_text)\n",
        "\t#Offer_List = []\n",
        "\t#Offer_List = re.findall(r'constantoffer', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Promos(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify coupons/promos with promo codes\n",
        "\tAssumption - promo code can be alphanumeric. But it immediately follows text of promo/code/promocode etc\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'\\b(promocode|promo code|promo|code)[s]?[\\s]*[a-z]*(constantnum)*[a-z]*[\\s]+', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*(constantalphanum)\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[a-z]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[0-9]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code|coupon)[s]?\\b', \" constantpromo \", transient_tweet_text)\n",
        "\t#Promo_List = []\n",
        "\t#Promo_List = re.findall(r'constantpromo', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Money(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify money in the tweet text but outside offers. This includes $,Rs, pound, Euro\n",
        "\t'''\n",
        "\tmoney_regex1 = r'\\b(rs|\\$)[\\s]*(constantnum)?[\\.]?[\\s]*constantnum\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex1, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex2 = r'[\\s]*[\\.]?[\\s]*constantnum(cent[s]?|\\$|c)\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex2, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex3 = r'(\\$|rs)[\\s]*constantalphanum'\n",
        "\ttransient_tweet_text = re.sub(money_regex3, \" constantmoney \", transient_tweet_text)\n",
        "\t#Money_List = []\n",
        "\t#Money_List = re.findall(r'constantmoney', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_freebies(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify freebies in tweets if any - free offers, free shipping, free trial, \n",
        "\t'''\n",
        "\tfreebies_regex1 = r'(?<!#)\\b(?:free)[\\s]+[a-z]+\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex1, \" constantfreebies \", transient_tweet_text)\n",
        "\tfreebies_regex2 = r'(?<!#)\\b(?:free)[\\s]+[a-z]*\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex2, \" constantfreebies \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def replace_numbers(transient_tweet_text):\n",
        "\t'''\n",
        "\tGiven any number/interger in tweet text, we want it to be replaced by constantnum\n",
        "\t'''\n",
        "\t# we want to process only those numbers that are not in a hashtag - below logic does this\n",
        "\tnum_regex = r'(?<!#)\\b(?:[-+]?[\\d,]*[\\.]?[\\d,]*[\\d]+|\\d+)\\b'\n",
        "\ttransient_tweet_text = re.sub(num_regex, \" constantnum \" , transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_AlphaNumerics(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify alpha numerics - this helps in identifying product codes/models, promocodes, Order IDs\n",
        "\t'''\n",
        "\tAlphaNumeric_regex = r'(?<!#)\\b(?:([a-z]+[0-9]+[a-z]*|[a-z]*[0-9]+[a-z]+)[a-z,0-9]*)\\b'\n",
        "\ttransient_tweet_text = re.sub(AlphaNumeric_regex, \" constantalphanum \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_emoji(transient_tweet_text):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokenized_tweet = tweet_tokenizer.tokenize(transient_tweet_text)\n",
        "    emojis_present = demoji.findall(transient_tweet_text)\n",
        "    tweet_no_emoji=''\n",
        "    for i,s in enumerate(tokenized_tweet):\n",
        "        if s in emojis_present.keys():\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + emojis_present[s]\n",
        "        else:\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + s\n",
        "    return tweet_no_emoji\n",
        "\n",
        "def deEmojify(transient_tweet_text):\n",
        "    return transient_tweet_text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "# def print_test():\n",
        "    \n",
        "#     test_tweet = \"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"\n",
        "#     #General Proprocessing\n",
        "#     test_tweet = test_tweet.lower()\n",
        "#     test_tweet = strip_unicode(test_tweet)\n",
        "    \n",
        "#     #function tests\n",
        "#     print(\"Process URLS:\\n\",process_URLs(test_tweet))\n",
        "#     print(\"Remove websites:\\n\",process_Websites(test_tweet))\n",
        "#     print(\"Remove mentions:\\n\",process_Mentions(test_tweet))\n",
        "#     print('Remove Emailid:\\n',process_EmailIds(test_tweet))\n",
        "#     print(\"Remove Hashtags:\\n\",process_HashTags(test_tweet))\n",
        "#     print(\"Remove Dates:\\n\",process_Dates(test_tweet))\n",
        "#     print(\"Process Time:\\n\",process_Times(test_tweet))\n",
        "#     print(\"Process Brand Mention:\\n\",process_BrandMentions(test_tweet))\n",
        "#     print(\"Process non Brand Mention:\\n\",process_NonBrandMentions(test_tweet))\n",
        "#     print(\"Process Brand Name:\\n\",process_BrandName(test_tweet))\n",
        "#     print(\"Process Savings:\\n\",identify_Savings(test_tweet))\n",
        "#     print(\"Process Offers:\\n\",indentify_Offers(test_tweet))\n",
        "#     print(\"Identiy Promos:\\n\",indentify_Promos(test_tweet))\n",
        "# ############\n",
        "# print_test()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_TweetText(tweet_text):\n",
        "\t'''\n",
        "\tTakes tweet_text and preprocesses it \n",
        "\tOrder of preprocessing:\n",
        "\t'''\n",
        "\n",
        "\t# get utf-8 encoding, lowercase, trim and remove multiple white spaces\n",
        "\ttransient_tweet_text = tweet_text\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\t#print \"PROCESSED: \", transient_tweet_text\n",
        "\n",
        "\ttransient_tweet_text = to_LowerCase(transient_tweet_text)\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\ttransient_tweet_text = remove_spl_words(transient_tweet_text)\n",
        "\n",
        " \n",
        "\t#emoji\n",
        "\ttransient_tweet_text = remove_emoji(transient_tweet_text) \n",
        "\ttransient_tweet_text = deEmojify(transient_tweet_text)\n",
        "\t# process Hastags, URLs, Websites, process_EmailIds\n",
        "\t# Give precedence to url over hashtag\n",
        "\ttransient_tweet_text = process_URLs(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_HashTags(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_Websites(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_EmailIds(transient_tweet_text)\n",
        "\n",
        "\t# process for brand mention, any other mention and brand Name\n",
        "\t#transient_tweet_text = process_BrandMentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_NonBrandMentions(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Mentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_BrandName(transient_tweet_text)\n",
        "\n",
        "\t# remove any unicodes\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\n",
        "\t# identify Date / Time if any\n",
        "\ttransient_tweet_text = process_Times(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Dates(transient_tweet_text)\n",
        "\n",
        "\t# indentify alphanums and nums\n",
        "\ttransient_tweet_text = identify_AlphaNumerics(transient_tweet_text)\n",
        "\ttransient_tweet_text = replace_numbers(transient_tweet_text)\n",
        "\t\n",
        "\t# identify promos, savings, offers, money and freebies\n",
        "\ttransient_tweet_text = indentify_Promos(transient_tweet_text)\n",
        "\ttransient_tweet_text = identify_Savings(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Offers(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Money(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_freebies(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = prune_multple_consecutive_same_char(transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# print(process_TweetText(\"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.37 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTAVLjkqFtNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f79feef2-2901-4ead-afba-0fc1a00faa81"
      },
      "source": [
        "#Making the necessary imports\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#preprocessing_path = \"/home/etherealenvy/Downloads/practical-nlp/Ch8/O5_smtd_preprocessing.py\"\n",
        "#sys.path.append(os.path.abspath(preprocessing_path))\n",
        "\n",
        "#import O5_smtd_preprocessing\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrtahTLTG5MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "62efa171-c663-4057-b12a-60f10dce89d1"
      },
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "X_train_df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142974</th>\n",
              "      <td>Laced with liberal doses of dark humor , gorge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78231</th>\n",
              "      <td>Offers absolutely nothing I had n't already se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7375</th>\n",
              "      <td>Ivan is a prince of a fellow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115288</th>\n",
              "      <td>the stomach-knotting suspense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>Quirky</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>109242 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase\n",
              "150238  make you wish you were at home watching that m...\n",
              "133360  , the tale has turned from sweet to bitterswee...\n",
              "49191   can say that about most of the flicks moving i...\n",
              "137709                   Does n't deliver a great story ,\n",
              "73297                                  unrecoverable life\n",
              "...                                                   ...\n",
              "142974  Laced with liberal doses of dark humor , gorge...\n",
              "78231   Offers absolutely nothing I had n't already se...\n",
              "7375                         Ivan is a prince of a fellow\n",
              "115288                      the stomach-knotting suspense\n",
              "2467                                               Quirky\n",
              "\n",
              "[109242 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_FCZjmtGGQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "954447ce-8bbd-4bd2-bea5-f9ede8f72b23"
      },
      "source": [
        "X_train_df['Phrase'].values"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['make you wish you were at home watching that movie instead of in the theater watching this one',\n",
              "       ', the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .',\n",
              "       'can say that about most of the flicks moving in and out of the multiplex',\n",
              "       ..., 'Ivan is a prince of a fellow',\n",
              "       'the stomach-knotting suspense', 'Quirky'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T75lcJa9GiOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "350a3e8a-1e1a-44af-a186-f0f97ec4267d"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150238    1\n",
              "133360    4\n",
              "49191     2\n",
              "137709    1\n",
              "73297     1\n",
              "         ..\n",
              "142974    4\n",
              "78231     1\n",
              "7375      3\n",
              "115288    3\n",
              "2467      2\n",
              "Name: Sentiment, Length: 109242, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3216ea7FvXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_df['Phrase_process'] = X_train_df['Phrase'].apply(lambda x: process_TweetText(x))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EV5FOdGIDNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "04c894c8-1ff2-4768-8207-25073466e7cf"
      },
      "source": [
        "X_train_df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "      <td>does n't deliver a great story ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>unrecoverable life</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase                                     Phrase_process\n",
              "150238  make you wish you were at home watching that m...  make you wish you were at home watching that m...\n",
              "133360  , the tale has turned from sweet to bitterswee...  , the tale has turned from sweet to bitterswee...\n",
              "49191   can say that about most of the flicks moving i...  can say that about most of the flicks moving i...\n",
              "137709                   Does n't deliver a great story ,                   does n't deliver a great story ,\n",
              "73297                                  unrecoverable life                                 unrecoverable life"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPdGgTRuHz4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_df['phrase_tokens'] = X_train_df['Phrase_process'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
        "X_train_df['phrase_no_stopwords'] = X_train_df['phrase_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCWK_obIcHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bd227208-3d0a-4faf-9651-72fe7ef8bc18"
      },
      "source": [
        "X_train_df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "      <th>phrase_tokens</th>\n",
              "      <th>phrase_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>[make, you, wish, you, were, at, home, watchin...</td>\n",
              "      <td>[make, wish, home, watching, movie, instead, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>[,, the, tale, has, turned, from, sweet, to, b...</td>\n",
              "      <td>[,, tale, turned, sweet, bittersweet, ,, tears...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>[can, say, that, about, most, of, the, flicks,...</td>\n",
              "      <td>[say, flicks, moving, multiplex]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "      <td>does n't deliver a great story ,</td>\n",
              "      <td>[does, n't, deliver, a, great, story, ,]</td>\n",
              "      <td>[n't, deliver, great, story, ,]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>[unrecoverable, life]</td>\n",
              "      <td>[unrecoverable, life]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase  ...                                phrase_no_stopwords\n",
              "150238  make you wish you were at home watching that m...  ...  [make, wish, home, watching, movie, instead, t...\n",
              "133360  , the tale has turned from sweet to bitterswee...  ...  [,, tale, turned, sweet, bittersweet, ,, tears...\n",
              "49191   can say that about most of the flicks moving i...  ...                   [say, flicks, moving, multiplex]\n",
              "137709                   Does n't deliver a great story ,  ...                    [n't, deliver, great, story, ,]\n",
              "73297                                  unrecoverable life  ...                              [unrecoverable, life]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQJi0JZKNmPt",
        "colab_type": "text"
      },
      "source": [
        "## Train your own Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCM4cUNjNtjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase_processed=X_train_df['Phrase_process'].values"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zBH1EprIaym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0023cd61-a00e-4a56-df68-b8028bca96c7"
      },
      "source": [
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(phrase_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW Model Training Complete.\n",
            "Time taken for training is:7.45960 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y78qQja0N2-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba210c79-0da4-42cb-afb8-3694126b7aec"
      },
      "source": [
        "#Create document vectors by averaging word vectors.\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 100\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this/count_for_this if count_for_this > 0 else feat_for_this)        \n",
        "    return feats\n",
        "\n",
        "train_vectors = embedding_feats(X_train_df['phrase_no_stopwords'].values)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT1q2Z8INxV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1aadb69e-7079-49f9-d294-78595bd858fb"
      },
      "source": [
        "#Take any classifier (LogisticRegression here)\n",
        "classifier = LogisticRegression(random_state=2020)\n",
        "classifier.fit(train_vectors, y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=2020, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soCmXeipOSal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}