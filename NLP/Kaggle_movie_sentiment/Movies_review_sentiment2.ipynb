{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movies_review_sentiment.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/duybluemind1988/Data-science/blob/master/NLP/Kaggle_movie_sentiment/Movies_review_sentiment2.ipynb",
      "authorship_tag": "ABX9TyP9Ui2CP5wOSpEEVS4ZQwlj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/NLP/Kaggle_movie_sentiment/Movies_review_sentiment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pZdaGiJRDCE",
        "colab_type": "text"
      },
      "source": [
        "# Prepare X_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1LtJ8a-WuP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORWZxYZWnzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "test_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/test.tsv'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_9x2xcHXO8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "983e2342-363e-4645-a4ba-fb57778524b5"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AV4Gg7XX_vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6b84deed-188b-4270-b106-f10ff9baa412"
      },
      "source": [
        "train_df['Sentiment'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    79582\n",
              "3    32927\n",
              "1    27273\n",
              "4     9206\n",
              "0     7072\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiw_EN5lZjVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRxn1HsUaFvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2b222c40-9325-4b87-c13a-c5fac1051994"
      },
      "source": [
        "X"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         A series of escapades demonstrating the adage ...\n",
              "1         A series of escapades demonstrating the adage ...\n",
              "2                                                  A series\n",
              "3                                                         A\n",
              "4                                                    series\n",
              "                                ...                        \n",
              "156055                                            Hearst 's\n",
              "156056                            forced avuncular chortles\n",
              "156057                                   avuncular chortles\n",
              "156058                                            avuncular\n",
              "156059                                             chortles\n",
              "Name: Phrase, Length: 156060, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaT8XZqRaCCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "39cb9877-5099-4874-89e9-0ac16fa6c35f"
      },
      "source": [
        "y"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         1\n",
              "1         2\n",
              "2         2\n",
              "3         2\n",
              "4         2\n",
              "         ..\n",
              "156055    2\n",
              "156056    1\n",
              "156057    3\n",
              "156058    2\n",
              "156059    2\n",
              "Name: Sentiment, Length: 156060, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4196zxaXZ3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e238799b-abe1-40a7-d692-5ab179508043"
      },
      "source": [
        "test_df=pd.read_csv(test_path,sep='\\t')\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(66292, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>8545</td>\n",
              "      <td>An</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase\n",
              "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
              "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
              "2    156063        8545                                                 An\n",
              "3    156064        8545  intermittently pleasing but mostly routine effort\n",
              "4    156065        8545         intermittently pleasing but mostly routine"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcoTIoTZCvT",
        "colab_type": "text"
      },
      "source": [
        "Text have no label, so we split training set to train and text set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRjqjw3YJsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMJZjXxOZXlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4CHcYlYZqu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e1315f83-5c16-4fe6-cfb8-5f28b158b801"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDf2PKyGZyCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "148d3a07-eb6d-4686-8846-65147d56ec93"
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509950\n",
              "3    0.210990\n",
              "1    0.174759\n",
              "4    0.058988\n",
              "0    0.045312\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK9lRF3zZ8MB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "22ee707a-58f8-4fa6-8065-2d17b73056c3"
      },
      "source": [
        "y_test.value_counts(normalize=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.509932\n",
              "3    0.210987\n",
              "1    0.174762\n",
              "4    0.058994\n",
              "0    0.045324\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4GK8KGaZVg",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification chap 8 Twitter_Sentiment analysis Orelilly NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXtMI2SGafbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install text_normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPP_3GpAa_Gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "361ff0f4-c89f-4a89-ce63-82739f099a97"
      },
      "source": [
        "!pip install demoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/da/0b/d008f26ebbfd86d21117267e627f2f7359c76e5ecbeba08d8f631f4092c4/demoji-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji) (49.2.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.3 demoji-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ODLlPqFqL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ebb8ddc7-0deb-4d43-f46a-e59e57155afb"
      },
      "source": [
        "'''\n",
        "Contains helper funtions for preprocessing of twitter documents before getting their corresponding vectors\n",
        "from word2vec_twitter model\n",
        "Author: Anuj Gupta\n",
        "'''\n",
        "\n",
        "'''\n",
        "functionality:\n",
        "Discard non-english tweets\n",
        "Discard Replies\n",
        "Discard RTs\n",
        "For now this is handle while fetch tweet text from mongo documents\n",
        "process constantbrands mentions\n",
        "process any other mentions\n",
        "process constant brand name if any \n",
        "process hanstags\n",
        "process URLs\n",
        "process websites\n",
        "process process_EmailIds\n",
        "process \n",
        "process alphanums\n",
        "'''\n",
        "\n",
        "import re\n",
        "import string\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "#gobal\n",
        "PunctChars = r'''[`'“\".?!,:;]'''\n",
        "Punct = '%s+' % PunctChars\n",
        "Entity = '&(amp|lt|gt|quot);'\n",
        "printable = set(string.printable)\n",
        "\n",
        "# helper functoins\n",
        "def regex_or(*items):\n",
        "\tr = '|'.join(items)\n",
        "\tr = '(' + r + ')'\n",
        "\treturn r\n",
        "\n",
        "def pos_lookahead(r):\n",
        "\treturn '(?=' + r + ')'\n",
        "\n",
        "def neg_lookahead(r):\n",
        "\treturn '(?!' + r + ')'\n",
        "\n",
        "def optional(r):\n",
        "\treturn '(%s)?' % r\n",
        "\n",
        "def trim(transient_tweet_text):\n",
        "\t''' \n",
        "\ttrim leading and trailing spaces in the tweet text\n",
        "\t'''\n",
        "\treturn transient_tweet_text.strip()\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def to_LowerCase(transient_tweet_text):\n",
        "\t'''\n",
        "\tConvert tweet text to lower to lower case alphabets\n",
        "\t'''\n",
        "\ttransient_tweet_text = transient_tweet_text.lower()\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_unicode(transient_tweet_text):\n",
        "    '''\n",
        "    Strip all unicode characters from a tweet\n",
        "    '''\n",
        "    tweet = ''.join(i for i in transient_tweet_text if ord(i)<128)\n",
        "    return tweet \n",
        "\n",
        "def process_URLs(transient_tweet_text):\n",
        "\t'''\n",
        "\treplace all URLs in the tweet text\n",
        "\t'''\n",
        "\tUrlStart1 = regex_or('https?://', r'www\\.',r'bit.ly/')\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tUrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + pos_lookahead(r'[/ \\W\\b]')\n",
        "\tUrlBody = r'[^ \\t\\r\\n<>]*?'  # * not + for case of:  \"go to bla.com.\" -- don't want period\n",
        "\tUrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n",
        "\tUrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n",
        "\tUrl = \t(optional(r'\\b') + \n",
        "    \t\tregex_or(UrlStart1, UrlStart2) + \n",
        "    \t\tUrlBody + \n",
        "    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n",
        "\n",
        "\tUrl_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(Url_RE, \" constanturl \", transient_tweet_text)\n",
        "\n",
        "\t# fix to handle unicodes in URL\n",
        "\tURL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*'\n",
        "\ttransient_tweet_text = re.sub(URL_regex2, \" constanturl \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Websites(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify website mentioned if any \n",
        "\t'''\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tsep = r'[.]'\n",
        "\twebsite_regex = r'(?<!#)?(\\b)?[a-zA-Z0-9.]+' + sep + CommonTLDs\n",
        "\twebsite_RE = re.compile(\"(%s)\" % website_regex, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(website_RE, ' constantwebsite ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_EmailIds(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify email mentioned if any\n",
        "\t'''\n",
        "\temail_regex = r'(\\b)?[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(\\b)?'\n",
        "\ttransient_tweet_text = re.sub(email_regex, ' constantemailid ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Mentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify mentions if any\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", \" constantnonbrandmention \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "def process_HashTags(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all Hashtags from a tweet\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"#(\\w+)\\b\", ' constanthashtag ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Dates(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify date and convert it to constant\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'(\\d+/\\d+/\\d+)', \" constantdate \" , transient_tweet_text)\n",
        "\t#transient_tweet_text = re.sub(r'constantnum[\\s]?(/|-)[\\s]?constantnum[\\s]?(/|-)[\\s]?constantnum', \" constantdate \" , transient_tweet_text)\n",
        "\t#date_regex = r'(constantnum)[\\s]*(st|nd|rd|th)[\\s]*(january|jan|february|feb|march|mar|april|may|june|jun|july|august|aug|september|sep|october|oct|november|nov|december|dec)'\n",
        "\tdate_regex1 = r'\\b((0|1|2|3)?[0-9][\\s]*)[-./]([\\s]*([012]?[0-9])[\\s]*)([-./]([\\s]*(19|20)[0-9][0-9]))?\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex1, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex2 = r'\\b((19|20)[0-9][0-9][\\s]*[-./]?)?[\\s]*([012]?[0-9])[\\s]*[-./][\\s]*(0|1|2|3)?[0-9]\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex2, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tMonths = regex_or('january','jan','february','feb','march','mar','april','may','june','jun','july','jul','august','aug','september','sep','october','oct','november','nov','december','dec')\n",
        "\tdate_regex3 = r'\\d+[\\s]*(st|nd|rd|th)[\\s]*' + Months \n",
        "\ttransient_tweet_text = re.sub(date_regex3, ' constantdate ', transient_tweet_text)\n",
        "\tdate_regex4 = Months + r'[\\s]*\\d+[\\s]*(st|nd|rd|th)*\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex4, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex5 = r'[\\s]?\\b(19|20)[0-9][0-9]\\b[\\s]?'\n",
        "\ttransient_tweet_text = re.sub(date_regex5, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tdate_regex6 = r'([\\s]*(constantdate))+'\n",
        "\ttransient_tweet_text = re.sub(date_regex6, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Times(transient_tweet_text):\n",
        "\t'''\n",
        "\tIndentify time and convert it to constant\n",
        "\t'''\n",
        "\ttime_regex1 = r'([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9][\\s]*(am|pm)?[\\s]*([iescm](st)|gmt|utc|[pmce](dt))?'\n",
        "\ttransient_tweet_text = re.sub(time_regex1, ' constanttime ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com','shopohelp','shopo','mobikwikswat',\n",
        "\t\t'mobikwik','taxiforsure','tfscares','zoomcarindia','freshmenuindia','freshmenucares','grofers','jetairways',\n",
        "\t\t'cleartrip','olacabs','support_ola','makemytrip','makemytripcare','chai_point')\n",
        "\n",
        "\tBrandMentionRegex = r'@(\\b)*' + constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandMentionRegex, ' constantbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_NonBrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all Mentions left, if any, in tweet text\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", ' constantnonbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandName(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com', 'bluestone','shopohelp','shopo',\n",
        "\t\t'mobikwikswat','mobikwik','taxiforsure','tfscares', 'tfs','zoomcarindia', 'zoomcar','freshmenuindia','freshmenucares', \n",
        "\t\t'freshmenu','grofers','jetairways', 'jet','cleartrip','olacabs','support_ola', 'olacab', 'ola' ,'makemytrip',\n",
        "\t\t'makemytripcare', 'chai_point')\n",
        "\n",
        "\tBrandNameRegex = constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandNameRegex, ' constantbrandname ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_Savings(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify sale/save offers\n",
        "\t'''\n",
        "\t#sale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*(constantnum)*[\\s]*[%]?[\\s]*(-|~)?[\\s]*(constantnum)*[\\s]*[%]?'\n",
        "\tsale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(-|~|or)?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?'\n",
        "\ttransient_tweet_text = re.sub(sale_regex, \" constantdiscount \", transient_tweet_text)\n",
        "\t#discount_List = []\n",
        "\t#discount_List = re.findall(r'constantdiscount', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Offers(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify cashbacks and off / substrings of the form \"30% off\" or \"30% cashback\" or \"$30 off\"\n",
        "\tReplace them by constantOFFER\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'[rs|$]?[ ]*[constantnum][ ]*[%]?[ ]?[off|cashback|offer]', \"constantoffer\", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:(up[\\s]?to)?((rs|\\$)*[\\s]*(constantnum))[\\s]*[%]?)?[\\s]*[-|~|or]?[\\.]?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(offer|off|cashback|cash|cash back)', \" constantoffer \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:cashback|cash back|cash)\\b', \" constantoffer \", transient_tweet_text)\n",
        "\t#Offer_List = []\n",
        "\t#Offer_List = re.findall(r'constantoffer', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Promos(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify coupons/promos with promo codes\n",
        "\tAssumption - promo code can be alphanumeric. But it immediately follows text of promo/code/promocode etc\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'\\b(promocode|promo code|promo|code)[s]?[\\s]*[a-z]*(constantnum)*[a-z]*[\\s]+', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*(constantalphanum)\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[a-z]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[0-9]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code|coupon)[s]?\\b', \" constantpromo \", transient_tweet_text)\n",
        "\t#Promo_List = []\n",
        "\t#Promo_List = re.findall(r'constantpromo', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Money(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify money in the tweet text but outside offers. This includes $,Rs, pound, Euro\n",
        "\t'''\n",
        "\tmoney_regex1 = r'\\b(rs|\\$)[\\s]*(constantnum)?[\\.]?[\\s]*constantnum\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex1, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex2 = r'[\\s]*[\\.]?[\\s]*constantnum(cent[s]?|\\$|c)\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex2, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex3 = r'(\\$|rs)[\\s]*constantalphanum'\n",
        "\ttransient_tweet_text = re.sub(money_regex3, \" constantmoney \", transient_tweet_text)\n",
        "\t#Money_List = []\n",
        "\t#Money_List = re.findall(r'constantmoney', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_freebies(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify freebies in tweets if any - free offers, free shipping, free trial, \n",
        "\t'''\n",
        "\tfreebies_regex1 = r'(?<!#)\\b(?:free)[\\s]+[a-z]+\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex1, \" constantfreebies \", transient_tweet_text)\n",
        "\tfreebies_regex2 = r'(?<!#)\\b(?:free)[\\s]+[a-z]*\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex2, \" constantfreebies \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def replace_numbers(transient_tweet_text):\n",
        "\t'''\n",
        "\tGiven any number/interger in tweet text, we want it to be replaced by constantnum\n",
        "\t'''\n",
        "\t# we want to process only those numbers that are not in a hashtag - below logic does this\n",
        "\tnum_regex = r'(?<!#)\\b(?:[-+]?[\\d,]*[\\.]?[\\d,]*[\\d]+|\\d+)\\b'\n",
        "\ttransient_tweet_text = re.sub(num_regex, \" constantnum \" , transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_AlphaNumerics(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify alpha numerics - this helps in identifying product codes/models, promocodes, Order IDs\n",
        "\t'''\n",
        "\tAlphaNumeric_regex = r'(?<!#)\\b(?:([a-z]+[0-9]+[a-z]*|[a-z]*[0-9]+[a-z]+)[a-z,0-9]*)\\b'\n",
        "\ttransient_tweet_text = re.sub(AlphaNumeric_regex, \" constantalphanum \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_emoji(transient_tweet_text):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokenized_tweet = tweet_tokenizer.tokenize(transient_tweet_text)\n",
        "    emojis_present = demoji.findall(transient_tweet_text)\n",
        "    tweet_no_emoji=''\n",
        "    for i,s in enumerate(tokenized_tweet):\n",
        "        if s in emojis_present.keys():\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + emojis_present[s]\n",
        "        else:\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + s\n",
        "    return tweet_no_emoji\n",
        "\n",
        "def deEmojify(transient_tweet_text):\n",
        "    return transient_tweet_text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "# def print_test():\n",
        "    \n",
        "#     test_tweet = \"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"\n",
        "#     #General Proprocessing\n",
        "#     test_tweet = test_tweet.lower()\n",
        "#     test_tweet = strip_unicode(test_tweet)\n",
        "    \n",
        "#     #function tests\n",
        "#     print(\"Process URLS:\\n\",process_URLs(test_tweet))\n",
        "#     print(\"Remove websites:\\n\",process_Websites(test_tweet))\n",
        "#     print(\"Remove mentions:\\n\",process_Mentions(test_tweet))\n",
        "#     print('Remove Emailid:\\n',process_EmailIds(test_tweet))\n",
        "#     print(\"Remove Hashtags:\\n\",process_HashTags(test_tweet))\n",
        "#     print(\"Remove Dates:\\n\",process_Dates(test_tweet))\n",
        "#     print(\"Process Time:\\n\",process_Times(test_tweet))\n",
        "#     print(\"Process Brand Mention:\\n\",process_BrandMentions(test_tweet))\n",
        "#     print(\"Process non Brand Mention:\\n\",process_NonBrandMentions(test_tweet))\n",
        "#     print(\"Process Brand Name:\\n\",process_BrandName(test_tweet))\n",
        "#     print(\"Process Savings:\\n\",identify_Savings(test_tweet))\n",
        "#     print(\"Process Offers:\\n\",indentify_Offers(test_tweet))\n",
        "#     print(\"Identiy Promos:\\n\",indentify_Promos(test_tweet))\n",
        "# ############\n",
        "# print_test()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_TweetText(tweet_text):\n",
        "\t'''\n",
        "\tTakes tweet_text and preprocesses it \n",
        "\tOrder of preprocessing:\n",
        "\t'''\n",
        "\n",
        "\t# get utf-8 encoding, lowercase, trim and remove multiple white spaces\n",
        "\ttransient_tweet_text = tweet_text\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\t#print \"PROCESSED: \", transient_tweet_text\n",
        "\n",
        "\ttransient_tweet_text = to_LowerCase(transient_tweet_text)\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\ttransient_tweet_text = remove_spl_words(transient_tweet_text)\n",
        "\n",
        " \n",
        "\t#emoji\n",
        "\ttransient_tweet_text = remove_emoji(transient_tweet_text) \n",
        "\ttransient_tweet_text = deEmojify(transient_tweet_text)\n",
        "\t# process Hastags, URLs, Websites, process_EmailIds\n",
        "\t# Give precedence to url over hashtag\n",
        "\ttransient_tweet_text = process_URLs(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_HashTags(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_Websites(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_EmailIds(transient_tweet_text)\n",
        "\n",
        "\t# process for brand mention, any other mention and brand Name\n",
        "\t#transient_tweet_text = process_BrandMentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_NonBrandMentions(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Mentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_BrandName(transient_tweet_text)\n",
        "\n",
        "\t# remove any unicodes\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\n",
        "\t# identify Date / Time if any\n",
        "\ttransient_tweet_text = process_Times(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Dates(transient_tweet_text)\n",
        "\n",
        "\t# indentify alphanums and nums\n",
        "\ttransient_tweet_text = identify_AlphaNumerics(transient_tweet_text)\n",
        "\ttransient_tweet_text = replace_numbers(transient_tweet_text)\n",
        "\t\n",
        "\t# identify promos, savings, offers, money and freebies\n",
        "\ttransient_tweet_text = indentify_Promos(transient_tweet_text)\n",
        "\ttransient_tweet_text = identify_Savings(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Offers(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Money(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_freebies(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = prune_multple_consecutive_same_char(transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# print(process_TweetText(\"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.37 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTAVLjkqFtNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f79feef2-2901-4ead-afba-0fc1a00faa81"
      },
      "source": [
        "#Making the necessary imports\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#preprocessing_path = \"/home/etherealenvy/Downloads/practical-nlp/Ch8/O5_smtd_preprocessing.py\"\n",
        "#sys.path.append(os.path.abspath(preprocessing_path))\n",
        "\n",
        "#import O5_smtd_preprocessing\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrtahTLTG5MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "62efa171-c663-4057-b12a-60f10dce89d1"
      },
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "X_train_df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142974</th>\n",
              "      <td>Laced with liberal doses of dark humor , gorge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78231</th>\n",
              "      <td>Offers absolutely nothing I had n't already se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7375</th>\n",
              "      <td>Ivan is a prince of a fellow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115288</th>\n",
              "      <td>the stomach-knotting suspense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>Quirky</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>109242 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase\n",
              "150238  make you wish you were at home watching that m...\n",
              "133360  , the tale has turned from sweet to bitterswee...\n",
              "49191   can say that about most of the flicks moving i...\n",
              "137709                   Does n't deliver a great story ,\n",
              "73297                                  unrecoverable life\n",
              "...                                                   ...\n",
              "142974  Laced with liberal doses of dark humor , gorge...\n",
              "78231   Offers absolutely nothing I had n't already se...\n",
              "7375                         Ivan is a prince of a fellow\n",
              "115288                      the stomach-knotting suspense\n",
              "2467                                               Quirky\n",
              "\n",
              "[109242 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_FCZjmtGGQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "954447ce-8bbd-4bd2-bea5-f9ede8f72b23"
      },
      "source": [
        "X_train_df['Phrase'].values"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['make you wish you were at home watching that movie instead of in the theater watching this one',\n",
              "       ', the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .',\n",
              "       'can say that about most of the flicks moving in and out of the multiplex',\n",
              "       ..., 'Ivan is a prince of a fellow',\n",
              "       'the stomach-knotting suspense', 'Quirky'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T75lcJa9GiOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "350a3e8a-1e1a-44af-a186-f0f97ec4267d"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150238    1\n",
              "133360    4\n",
              "49191     2\n",
              "137709    1\n",
              "73297     1\n",
              "         ..\n",
              "142974    4\n",
              "78231     1\n",
              "7375      3\n",
              "115288    3\n",
              "2467      2\n",
              "Name: Sentiment, Length: 109242, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3216ea7FvXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_df['Phrase_process'] = X_train_df['Phrase'].apply(lambda x: process_TweetText(x))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EV5FOdGIDNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "04c894c8-1ff2-4768-8207-25073466e7cf"
      },
      "source": [
        "X_train_df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "      <td>does n't deliver a great story ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>unrecoverable life</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase                                     Phrase_process\n",
              "150238  make you wish you were at home watching that m...  make you wish you were at home watching that m...\n",
              "133360  , the tale has turned from sweet to bitterswee...  , the tale has turned from sweet to bitterswee...\n",
              "49191   can say that about most of the flicks moving i...  can say that about most of the flicks moving i...\n",
              "137709                   Does n't deliver a great story ,                   does n't deliver a great story ,\n",
              "73297                                  unrecoverable life                                 unrecoverable life"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPdGgTRuHz4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_df['phrase_tokens'] = X_train_df['Phrase_process'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
        "X_train_df['phrase_no_stopwords'] = X_train_df['phrase_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCWK_obIcHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bd227208-3d0a-4faf-9651-72fe7ef8bc18"
      },
      "source": [
        "X_train_df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "      <th>phrase_tokens</th>\n",
              "      <th>phrase_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150238</th>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>make you wish you were at home watching that m...</td>\n",
              "      <td>[make, you, wish, you, were, at, home, watchin...</td>\n",
              "      <td>[make, wish, home, watching, movie, instead, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133360</th>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>, the tale has turned from sweet to bitterswee...</td>\n",
              "      <td>[,, the, tale, has, turned, from, sweet, to, b...</td>\n",
              "      <td>[,, tale, turned, sweet, bittersweet, ,, tears...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49191</th>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>can say that about most of the flicks moving i...</td>\n",
              "      <td>[can, say, that, about, most, of, the, flicks,...</td>\n",
              "      <td>[say, flicks, moving, multiplex]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137709</th>\n",
              "      <td>Does n't deliver a great story ,</td>\n",
              "      <td>does n't deliver a great story ,</td>\n",
              "      <td>[does, n't, deliver, a, great, story, ,]</td>\n",
              "      <td>[n't, deliver, great, story, ,]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73297</th>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>unrecoverable life</td>\n",
              "      <td>[unrecoverable, life]</td>\n",
              "      <td>[unrecoverable, life]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase  ...                                phrase_no_stopwords\n",
              "150238  make you wish you were at home watching that m...  ...  [make, wish, home, watching, movie, instead, t...\n",
              "133360  , the tale has turned from sweet to bitterswee...  ...  [,, tale, turned, sweet, bittersweet, ,, tears...\n",
              "49191   can say that about most of the flicks moving i...  ...                   [say, flicks, moving, multiplex]\n",
              "137709                   Does n't deliver a great story ,  ...                    [n't, deliver, great, story, ,]\n",
              "73297                                  unrecoverable life  ...                              [unrecoverable, life]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQJi0JZKNmPt",
        "colab_type": "text"
      },
      "source": [
        "## Train your own Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCM4cUNjNtjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase_processed=X_train_df['Phrase_process'].values"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zBH1EprIaym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0023cd61-a00e-4a56-df68-b8028bca96c7"
      },
      "source": [
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(phrase_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW Model Training Complete.\n",
            "Time taken for training is:7.45960 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y78qQja0N2-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba210c79-0da4-42cb-afb8-3694126b7aec"
      },
      "source": [
        "#Create document vectors by averaging word vectors.\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 100\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this/count_for_this if count_for_this > 0 else feat_for_this)        \n",
        "    return feats\n",
        "\n",
        "train_vectors = embedding_feats(X_train_df['phrase_no_stopwords'].values)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT1q2Z8INxV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1aadb69e-7079-49f9-d294-78595bd858fb"
      },
      "source": [
        "#Take any classifier (LogisticRegression here)\n",
        "classifier = LogisticRegression(random_state=2020)\n",
        "classifier.fit(train_vectors, y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=2020, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soCmXeipOSal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "18423103-7124-4c82-bd26-3541df3aed0d"
      },
      "source": [
        "X_test_df=pd.DataFrame(X_test)\n",
        "X_test_df['Phrase_process'] = X_test_df['Phrase'].apply(lambda x: process_TweetText(x))\n",
        "X_test_df['phrase_tokens'] = X_test_df['Phrase_process'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
        "X_test_df['phrase_no_stopwords'] = X_test_df['phrase_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
        "X_test_df.head()\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Phrase_process</th>\n",
              "      <th>phrase_tokens</th>\n",
              "      <th>phrase_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105687</th>\n",
              "      <td>seems just as expectant of an adoring , wide-s...</td>\n",
              "      <td>seems just as expectant of an adoring , wide-s...</td>\n",
              "      <td>[seems, just, as, expectant, of, an, adoring, ...</td>\n",
              "      <td>[seems, expectant, adoring, ,, wide-smiling, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133291</th>\n",
              "      <td>the dream world of teen life ,</td>\n",
              "      <td>the dream world of teen life ,</td>\n",
              "      <td>[the, dream, world, of, teen, life, ,]</td>\n",
              "      <td>[dream, world, teen, life, ,]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36948</th>\n",
              "      <td>purpose and finesse</td>\n",
              "      <td>purpose and finesse</td>\n",
              "      <td>[purpose, and, finesse]</td>\n",
              "      <td>[purpose, finesse]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101915</th>\n",
              "      <td>Argento , at only 26</td>\n",
              "      <td>argento , at only constantnum</td>\n",
              "      <td>[argento, ,, at, only, constantnum]</td>\n",
              "      <td>[argento, ,, constantnum]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150567</th>\n",
              "      <td>you may ask</td>\n",
              "      <td>you may ask</td>\n",
              "      <td>[you, may, ask]</td>\n",
              "      <td>[may, ask]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Phrase  ...                                phrase_no_stopwords\n",
              "105687  seems just as expectant of an adoring , wide-s...  ...  [seems, expectant, adoring, ,, wide-smiling, r...\n",
              "133291                     the dream world of teen life ,  ...                      [dream, world, teen, life, ,]\n",
              "36948                                 purpose and finesse  ...                                 [purpose, finesse]\n",
              "101915                               Argento , at only 26  ...                          [argento, ,, constantnum]\n",
              "150567                                        you may ask  ...                                         [may, ask]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw_Lz-BNPig8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dfb542f5-3940-47c1-e3a1-a34b906ba13d"
      },
      "source": [
        "phrase_processed=X_test_df['Phrase_process'].values\n",
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(phrase_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW Model Training Complete.\n",
            "Time taken for training is:3.24631 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9qoVDD0PW6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2164b72-403d-45f2-c5a1-027b989b23a7"
      },
      "source": [
        "test_vectors = embedding_feats(X_test_df['phrase_no_stopwords'].values)\n",
        "print(len(test_vectors))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2QtcKeRPrSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "605d1991-0764-47c4-bbad-efd3e81f3764"
      },
      "source": [
        "preds = classifier.predict(test_vectors)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.13      0.12      2122\n",
            "           1       0.19      0.13      0.15      8182\n",
            "           2       0.53      0.84      0.65     23874\n",
            "           3       0.18      0.02      0.03      9878\n",
            "           4       0.00      0.00      0.00      2762\n",
            "\n",
            "    accuracy                           0.46     46818\n",
            "   macro avg       0.20      0.22      0.19     46818\n",
            "weighted avg       0.35      0.46      0.37     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsYInHiTSUBl",
        "colab_type": "text"
      },
      "source": [
        "# Deep learning (Chap 4 classification Oreillly NLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ksS0pLDQAw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_58AP4rSg35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXF3sxVPSzN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/train.tsv'\n",
        "test_path='/content/drive/My Drive/Data/NLP/Kaggle_movie_review_sentiment/test.tsv'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYCNgHLrS-0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "45e4a52a-5f8c-45b5-8c54-464f48a62030"
      },
      "source": [
        "train_df=pd.read_csv(train_path,sep='\\t')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FWeIEmXTAqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c4a6089c-7099-457b-e7a9-78c70b03a649"
      },
      "source": [
        "X=train_df['Phrase']\n",
        "y=train_df['Sentiment']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,\n",
        "                                               stratify=y,random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242,)\n",
            "(109242,)\n",
            "(46818,)\n",
            "(46818,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n36erOnFTScR",
        "colab_type": "text"
      },
      "source": [
        "## 1. Tokenize the texts and convert them into word index vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjot__KCTbvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b01e07f5-cea1-43ca-e85c-e0bb86e22533"
      },
      "source": [
        "X_train.values"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['make you wish you were at home watching that movie instead of in the theater watching this one',\n",
              "       ', the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .',\n",
              "       'can say that about most of the flicks moving in and out of the multiplex',\n",
              "       ..., 'Ivan is a prince of a fellow',\n",
              "       'the stomach-knotting suspense', 'Quirky'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv-TZ9w0TBmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43964c0a-80bb-404b-da26-77a2377130a1"
      },
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) #20000\n",
        "tokenizer.fit_on_texts(X_train.values)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train.values) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test.values)\n",
        "word_index = tokenizer.word_index #(X_train, luc nay chua phan ra X_train_new va X_valid)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "# max 20000 words keep in sequences, word index is just a show case, not use (> max words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15264 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPhIB9eCTkk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e25170d5-fc27-4da5-9198-8f0b9e229b68"
      },
      "source": [
        "print(train_sequences[0]) # max 10000 words in sequences\n",
        "print(len(train_sequences[0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[77, 22, 970, 22, 175, 30, 448, 227, 9, 17, 378, 3, 7, 1, 317, 227, 18, 26]\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xmcdp8Ty6I",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 2. Pad the text sequences so that all text vectors are of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_NNjyMiTtRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "train_sequences_padding = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) #1000\n",
        "test_sequences_padding = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) #1000"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_-i79_mUB7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c68c4a61-ddb0-4bdc-d13c-7f744520d907"
      },
      "source": [
        "#Before transform\n",
        "print(len(train_sequences)) # train_texts --> train_sequences\n",
        "print(len(train_sequences[0]))\n",
        "print(train_sequences[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109242\n",
            "18\n",
            "[77, 22, 970, 22, 175, 30, 448, 227, 9, 17, 378, 3, 7, 1, 317, 227, 18, 26]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDu5J-nJUDTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b5cbb5b0-095a-4cc6-f8b2-b498f8fab5f4"
      },
      "source": [
        "#After transform\n",
        "print(train_sequences_padding.shape)\n",
        "train_sequences_padding"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(109242, 1000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  227,   18,   26],\n",
              "       [   0,    0,    0, ...,  140, 1487, 3657],\n",
              "       [   0,    0,    0, ...,    3,    1, 4473],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    3,    2, 2915],\n",
              "       [   0,    0,    0, ..., 1308, 9589,  436],\n",
              "       [   0,    0,    0, ...,    0,    0,  429]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-dcDpkvUV4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c4325817-688f-4be1-a8c8-8828c48c2b94"
      },
      "source": [
        "train_labels = to_categorical(np.asarray(y_train))\n",
        "test_labels = to_categorical(np.asarray(y_test))\n",
        "\n",
        "print(y_train[:3])\n",
        "print(train_labels.shape)\n",
        "train_labels"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150238    1\n",
            "133360    4\n",
            "49191     2\n",
            "Name: Sentiment, dtype: int64\n",
            "(109242, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4eO9vYuU7BT",
        "colab_type": "text"
      },
      "source": [
        "## Split the training data into a training set and a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxzb9ztBUn21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d88ebf3d-5d97-4cbd-821a-0970699c571c"
      },
      "source": [
        "X_train_new,X_valid,y_train_new,y_valid=train_test_split(train_sequences_padding,train_labels,\n",
        "                                                         test_size=0.3,\n",
        "                                               stratify=train_labels,random_state=42)\n",
        "print(X_train_new.shape)\n",
        "print(y_train_new.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76469, 1000)\n",
            "(76469, 5)\n",
            "(32773, 1000)\n",
            "(32773, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBfYWJRYZ0O2",
        "colab_type": "text"
      },
      "source": [
        "##3. Map every word index to an embedding vector (for pre-train model| can skip if no need pre train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9R4jMVZouf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5b1ef79b-807c-4b27-f6e2-1afe88e0c122"
      },
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/Data/NLP/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # vocabulary name\n",
        "        coefs = np.asarray(values[1:], dtype='float32') # embedding 100 dim\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "print(embeddings_index[\"google\"])\n",
        "print(len(embeddings_index[\"google\"]))\n",
        "print(MAX_NUM_WORDS)\n",
        "print(EMBEDDING_DIM)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "[ 0.22575  -0.56253  -0.05156  -0.079389  1.1876   -0.48397  -0.23342\n",
            " -0.85278   0.97495  -0.33344   0.71692   0.12644   0.31962  -1.4136\n",
            " -0.57903  -0.037286 -0.0164    0.45155  -0.29005   0.52599  -0.22534\n",
            " -0.29556  -0.032407  1.5608   -0.013499 -0.064558  0.26625   0.78595\n",
            " -0.71693  -0.93025   0.80461   1.6035   -0.30602  -0.34764   0.93872\n",
            "  0.38137  -0.26743  -0.56519   0.58899  -0.14554  -0.34324   0.21291\n",
            " -0.39887   0.090042 -0.8495    0.38803  -0.5045   -0.22488   1.0644\n",
            " -0.2624    1.0334    0.06348  -0.39989   0.24236  -0.65636  -1.8107\n",
            " -0.061801  0.13795   1.1658   -0.30046  -0.50143   0.16509   0.039835\n",
            "  0.62541   0.56935   0.64125   0.21308   0.30276   0.39673   0.38973\n",
            "  0.28183   0.79481  -0.11962  -0.49598  -0.53195  -0.14897   0.51254\n",
            " -0.39208  -0.58535  -0.078509  0.81721  -0.73497  -0.68131   0.099243\n",
            " -0.87608   0.029632  0.33402  -0.14305   0.16964  -0.035178  0.39777\n",
            "  0.71769   0.25867  -0.36201   0.45698  -0.39156  -0.49343  -0.11224\n",
            "  0.29046   0.73216 ]\n",
            "100\n",
            "20000\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8uhTlrDaDHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c3157bb7-ae39-4f6e-dea5-4cd068534c78"
      },
      "source": [
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "print(num_words)\n",
        "print(embedding_matrix.shape)\n",
        "print(len(word_index)) # max 20000 tu hay gap nhat thoi, index nay khong xai het\n",
        "print(len(embeddings_index))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15265\n",
            "(15265, 100)\n",
            "15264\n",
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNv0IKplaQZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS: # chi thuc hien get embedding word < max_num_words(20000)\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word) #lay 100d embedding cho tung tu\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstnO3lKaZOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ea8ddfa6-62b9-4827-e184-acdcfe115dd6"
      },
      "source": [
        "print(embedding_matrix.shape)\n",
        "embedding_matrix[1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15265, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
              "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
              "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
              "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
              "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
              "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
              "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
              "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
              "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
              "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
              "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
              "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
              "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
              "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
              "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
              "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
              "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
              "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
              "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
              "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPCsv-Upcp44",
        "colab_type": "text"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQX8E0XHvtpb",
        "colab_type": "text"
      },
      "source": [
        "### Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcaa_Qkpy0ZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_test = test_sequences_padding"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp3NOeAKcs5y",
        "colab_type": "text"
      },
      "source": [
        "1D CNN Model with pre-trained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znReffWAaajm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78d10e12-6be9-4086-edcd-16c5c1973508"
      },
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(input_dim=num_words,#15265 (max 20000)\n",
        "                            output_dim=EMBEDDING_DIM, #100\n",
        "                            embeddings_initializer=Constant(embedding_matrix), #(15265, 100)\n",
        "                            input_length=MAX_SEQUENCE_LENGTH, #1000\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSqhIrVduB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b31dd077-8c41-48d0-f9b6-bef85155563f"
      },
      "source": [
        "labels_index=len(np.unique(y))\n",
        "labels_index"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bRn1CR5afRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "ff46a351-9520-4cb9-f42a-f2f0c9755518"
      },
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(labels_index, activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "cnnmodel.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Define a 1D CNN model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 100)         1526500   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 996, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 195, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 39, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 35, 128)           82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,771,881\n",
            "Trainable params: 245,381\n",
            "Non-trainable params: 1,526,500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r2JxL9wc0rL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "49f4c43f-6865-4e4f-ca79-991d82c23978"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "cnnmodel.fit(X_train_new, y_train_new,\n",
        "          batch_size=200,\n",
        "          epochs=1, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 22s 59ms/step - loss: 1.1060 - acc: 0.5578 - val_loss: 1.0369 - val_acc: 0.5784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8b9d18d780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zCciAlIer-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c54b3fd7-e48a-43f7-e62d-1237a7d6d166"
      },
      "source": [
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_sequences_padding, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1464/1464 [==============================] - 10s 7ms/step - loss: 1.0420 - acc: 0.5764\n",
            "Test accuracy with CNN: 0.5764449834823608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUJeNgcAlkZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcd8d411-f8c8-4fb2-ace4-79928bf4f139"
      },
      "source": [
        "y_pred=cnnmodel.predict_classes(test_sequences_padding)\n",
        "y_pred"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 1, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNx6ytuPlzAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d73b88-4075-4045-a3a5-6385cd1170a9"
      },
      "source": [
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "test_labels_reverse"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, ..., 3, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZYFJsYjt-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8aa43407-fa1d-4983-b6fe-eff9b5645cd1"
      },
      "source": [
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.35      0.31      2122\n",
            "           1       0.39      0.26      0.31      8182\n",
            "           2       0.65      0.85      0.74     23874\n",
            "           3       0.51      0.36      0.42      9878\n",
            "           4       0.66      0.09      0.15      2762\n",
            "\n",
            "    accuracy                           0.58     46818\n",
            "   macro avg       0.50      0.38      0.39     46818\n",
            "weighted avg       0.56      0.58      0.54     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNAfM91cgRwP",
        "colab_type": "text"
      },
      "source": [
        "1D CNN model with training your own embedding¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qVjvVghgSSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn4fIB6UgT0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "ff04101f-1d28-452b-9355-cf44c472f42c"
      },
      "source": [
        "\n",
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel2 = Sequential()\n",
        "cnnmodel2.add(Embedding(MAX_NUM_WORDS, 128)) # co the thieu input length = MAX_SEQUENCE_LENGTH (1000)\n",
        "cnnmodel2.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel2.add(MaxPooling1D(5))\n",
        "cnnmodel2.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel2.add(MaxPooling1D(5))\n",
        "cnnmodel2.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel2.add(GlobalMaxPooling1D())\n",
        "cnnmodel2.add(Dense(128, activation='relu'))\n",
        "cnnmodel2.add(Dense(labels_index, activation='softmax'))\n",
        "\n",
        "cnnmodel2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "print(cnnmodel2.summary())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 2,823,301\n",
            "Trainable params: 2,823,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4K_DQ7igcQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c23706ba-0782-4a03-f4f1-fc453e2b1c49"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "cnnmodel2.fit(X_train_new, y_train_new,\n",
        "          batch_size=200, #75000 X_train shape\n",
        "          epochs=1, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 41s 107ms/step - loss: 1.1858 - acc: 0.5329 - val_loss: 1.0873 - val_acc: 0.5795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8bacdb2828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoNdDcD7mv2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "0ad304e8-d487-40e9-90d6-5dbea4545fd7"
      },
      "source": [
        "#Evaluate on test set:\n",
        "y_pred=cnnmodel2.predict_classes(test_sequences_padding)\n",
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      2122\n",
            "           1       0.40      0.22      0.29      8182\n",
            "           2       0.63      0.87      0.73     23874\n",
            "           3       0.47      0.40      0.43      9878\n",
            "           4       0.49      0.15      0.23      2762\n",
            "\n",
            "    accuracy                           0.58     46818\n",
            "   macro avg       0.40      0.33      0.34     46818\n",
            "weighted avg       0.52      0.58      0.53     46818\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhbiiiM4gy5G",
        "colab_type": "text"
      },
      "source": [
        "LSTM Model with training your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i43pq6begzUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "8225ea77-c840-4e6a-872a-8540028c7004"
      },
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#model\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(labels_index, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "rnnmodel.summary()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 2,692,229\n",
            "Trainable params: 2,692,229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BNsvkOg4A-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "8af11ca4-9791-4316-b2c6-081af808d687"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "rnnmodel.fit(X_train_new, y_train_new,\n",
        "          batch_size=200, #75000 X_train shape\n",
        "          epochs=1, validation_data=(X_valid, y_valid))\n",
        "#  1472s 4s/step - loss: 0.3740 - accuracy: 0.5546 - val_loss: 0.3232 - val_accuracy: 0.6187\n",
        "#Evaluate on test set:\n",
        "y_pred=rnnmodel.predict_classes(test_sequences_padding)\n",
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 1472s 4s/step - loss: 0.3740 - accuracy: 0.5546 - val_loss: 0.3232 - val_accuracy: 0.6187\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.01      0.01      2122\n",
            "           1       0.50      0.38      0.43      8182\n",
            "           2       0.70      0.83      0.76     23874\n",
            "           3       0.49      0.59      0.53      9878\n",
            "           4       0.59      0.05      0.09      2762\n",
            "\n",
            "    accuracy                           0.62     46818\n",
            "   macro avg       0.54      0.37      0.37     46818\n",
            "weighted avg       0.60      0.62      0.58     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kde4min4g7IA",
        "colab_type": "text"
      },
      "source": [
        "LSTM Model using pre-trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AURWp4UKg8IA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "e80ccfe8-01e4-45fd-d622-7788c542d1cd"
      },
      "source": [
        "\n",
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(labels_index, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print(rnnmodel2.summary())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 100)         1526500   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,644,393\n",
            "Trainable params: 117,893\n",
            "Non-trainable params: 1,526,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr1TypbahDWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "1ee40ce9-8f55-4919-a7ab-c93ade3a65e2"
      },
      "source": [
        "#Train the model. Tune to validation set. \n",
        "rnnmodel2.fit(X_train_new, y_train_new,\n",
        "          batch_size=200, #75000 X_train shape\n",
        "          epochs=1, validation_data=(X_valid, y_valid))\n",
        "#Evaluate on test set:\n",
        "y_pred=rnnmodel2.predict_classes(test_sequences_padding)\n",
        "test_labels_reverse=np.argmax(test_labels, axis =1)\n",
        "print(classification_report(test_labels_reverse,y_pred))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 1357s 4s/step - loss: 0.3639 - accuracy: 0.5577 - val_loss: 0.3349 - val_accuracy: 0.5898\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.01      0.02      2122\n",
            "           1       0.44      0.27      0.33      8182\n",
            "           2       0.67      0.84      0.74     23874\n",
            "           3       0.46      0.53      0.49      9878\n",
            "           4       0.52      0.06      0.11      2762\n",
            "\n",
            "    accuracy                           0.59     46818\n",
            "   macro avg       0.49      0.34      0.34     46818\n",
            "weighted avg       0.56      0.59      0.55     46818\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kajyJPe3wgbn",
        "colab_type": "text"
      },
      "source": [
        "- DL model is better than ML model. (improve 20% macro and weighted avg, 10% accuracy)\n",
        "- RNN model is better than CNN model, but very long time training (~ 20 times lower), accuracy is not imporve so much (just 4%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKy3AW_7vkGW",
        "colab_type": "text"
      },
      "source": [
        "### Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHL03IoPnE05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(input_dim=num_words,#15265 (max 20000)\n",
        "                            output_dim=EMBEDDING_DIM, #100\n",
        "                            embeddings_initializer=Constant(embedding_matrix), #(15265, 100)\n",
        "                            input_length=MAX_SEQUENCE_LENGTH, #1000\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}