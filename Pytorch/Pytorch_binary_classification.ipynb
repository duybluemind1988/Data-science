{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch binary classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/duybluemind1988/Data-science/blob/master/Pytorch/Pytorch_binary_classification.ipynb",
      "authorship_tag": "ABX9TyPNTP8207hhXN1Fx0STtdFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1e93ee9c16e4d6f9bd898418374b5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_66b762c435804e48a3575cd1b37c0c2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_74fd7f375e984835b5a5eb8bda068af0",
              "IPY_MODEL_090a49da710b421cbb90bdd3d9c6672d"
            ]
          }
        },
        "66b762c435804e48a3575cd1b37c0c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74fd7f375e984835b5a5eb8bda068af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6521596a98e84bb693e18f9fa337ff2a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_394fab414ef34376a093ffb0c0ae6c72"
          }
        },
        "090a49da710b421cbb90bdd3d9c6672d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a238b4c2e8564a6992a57f8f8261dd8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/50 [00:30&lt;00:00,  1.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d82b941790b74651bb5b22f96e7a94c2"
          }
        },
        "6521596a98e84bb693e18f9fa337ff2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "394fab414ef34376a093ffb0c0ae6c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a238b4c2e8564a6992a57f8f8261dd8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d82b941790b74651bb5b22f96e7a94c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/Pytorch/Pytorch_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXyCEd0qMl2D"
      },
      "source": [
        "# Lower back pain symptoms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPIjOS5JCzap"
      },
      "source": [
        "https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtfJ2VUyEJ5d"
      },
      "source": [
        "https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ1bIqoHMpHy"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_cq20eB-y0",
        "outputId": "0c0e4839-8828-45de-de3d-a2b347813c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import balanced_accuracy_score,matthews_corrcoef"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sorKOe-HDevD"
      },
      "source": [
        "310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\n",
        "\n",
        "Lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. Typical sources of low back pain include:\n",
        "\n",
        "- The large nerve roots in the low back that go to the legs may be irritated\n",
        "- The smaller nerves that supply the low back may be irritated\n",
        "- The large paired lower back muscles (erector spinae) may be strained\n",
        "- The bones, ligaments or joints may be damaged\n",
        "- An intervertebral disc may be degenerating\n",
        "\n",
        "An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.\n",
        "\n",
        "While lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\n",
        "\n",
        "This data set is about to identify a person is abnormal or normal using collected physical spine details/data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iBwviBKC1R_",
        "outputId": "0a03444a-b11b-4b68-9a64-b82f5dfe4397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Data/Dataset_spine.csv\")\n",
        "df=df.iloc[:,:-1]\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(310, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Col1</th>\n",
              "      <th>Col2</th>\n",
              "      <th>Col3</th>\n",
              "      <th>Col4</th>\n",
              "      <th>Col5</th>\n",
              "      <th>Col6</th>\n",
              "      <th>Col7</th>\n",
              "      <th>Col8</th>\n",
              "      <th>Col9</th>\n",
              "      <th>Col10</th>\n",
              "      <th>Col11</th>\n",
              "      <th>Col12</th>\n",
              "      <th>Class_att</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63.027818</td>\n",
              "      <td>22.552586</td>\n",
              "      <td>39.609117</td>\n",
              "      <td>40.475232</td>\n",
              "      <td>98.672917</td>\n",
              "      <td>-0.254400</td>\n",
              "      <td>0.744503</td>\n",
              "      <td>12.5661</td>\n",
              "      <td>14.5386</td>\n",
              "      <td>15.30468</td>\n",
              "      <td>-28.658501</td>\n",
              "      <td>43.5123</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39.056951</td>\n",
              "      <td>10.060991</td>\n",
              "      <td>25.015378</td>\n",
              "      <td>28.995960</td>\n",
              "      <td>114.405425</td>\n",
              "      <td>4.564259</td>\n",
              "      <td>0.415186</td>\n",
              "      <td>12.8874</td>\n",
              "      <td>17.5323</td>\n",
              "      <td>16.78486</td>\n",
              "      <td>-25.530607</td>\n",
              "      <td>16.1102</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>68.832021</td>\n",
              "      <td>22.218482</td>\n",
              "      <td>50.092194</td>\n",
              "      <td>46.613539</td>\n",
              "      <td>105.985135</td>\n",
              "      <td>-3.530317</td>\n",
              "      <td>0.474889</td>\n",
              "      <td>26.8343</td>\n",
              "      <td>17.4861</td>\n",
              "      <td>16.65897</td>\n",
              "      <td>-29.031888</td>\n",
              "      <td>19.2221</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>69.297008</td>\n",
              "      <td>24.652878</td>\n",
              "      <td>44.311238</td>\n",
              "      <td>44.644130</td>\n",
              "      <td>101.868495</td>\n",
              "      <td>11.211523</td>\n",
              "      <td>0.369345</td>\n",
              "      <td>23.5603</td>\n",
              "      <td>12.7074</td>\n",
              "      <td>11.42447</td>\n",
              "      <td>-30.470246</td>\n",
              "      <td>18.8329</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49.712859</td>\n",
              "      <td>9.652075</td>\n",
              "      <td>28.317406</td>\n",
              "      <td>40.060784</td>\n",
              "      <td>108.168725</td>\n",
              "      <td>7.918501</td>\n",
              "      <td>0.543360</td>\n",
              "      <td>35.4940</td>\n",
              "      <td>15.9546</td>\n",
              "      <td>8.87237</td>\n",
              "      <td>-16.378376</td>\n",
              "      <td>24.9171</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Col1       Col2       Col3  ...      Col11    Col12  Class_att\n",
              "0  63.027818  22.552586  39.609117  ... -28.658501  43.5123   Abnormal\n",
              "1  39.056951  10.060991  25.015378  ... -25.530607  16.1102   Abnormal\n",
              "2  68.832021  22.218482  50.092194  ... -29.031888  19.2221   Abnormal\n",
              "3  69.297008  24.652878  44.311238  ... -30.470246  18.8329   Abnormal\n",
              "4  49.712859   9.652075  28.317406  ... -16.378376  24.9171   Abnormal\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUHKizrGDJQv",
        "outputId": "6c86eec0-c7ad-4888-8bc1-96231b6b83e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "sns.countplot(x = 'Class_att', data=df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa87da24588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MjsODTHK9Hl"
      },
      "source": [
        "df['Class_att'] = df['Class_att'].astype('category')\n",
        "encode_map = {\n",
        "    'Abnormal': 1,\n",
        "    'Normal': 0\n",
        "}\n",
        "\n",
        "df['Class_att'].replace(encode_map, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjJZ02ORK_lv"
      },
      "source": [
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:, -1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxcfmxhOLB1t"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-aBJGJ5LCzs"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tOTnWrM0PL"
      },
      "source": [
        "## Define Custom Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr9BTe2YLGXK"
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRsYdQzGLHUr"
      },
      "source": [
        "## train data\n",
        "class trainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "train_data = trainData(torch.FloatTensor(X_train), \n",
        "                       torch.FloatTensor(y_train))\n",
        "## test data    \n",
        "class testData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "    \n",
        "\n",
        "test_data = testData(torch.FloatTensor(X_test))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBKD9y9QLKI8"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBvCk4XgMvnL"
      },
      "source": [
        "## Define Neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5qEajHgMOIt"
      },
      "source": [
        "Once weâ€™ve defined all these layers, itâ€™s time to use them. In the forward() function, we take variable inputs as our input. We pass this input through the different layers we initialized.\n",
        "\n",
        "The first line of the forward() functions takes the input, passes it through our first linear layer and then applies the ReLU activation on it. Then we apply BatchNorm on the output. Look at the following code to understand it better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7UY7l_ELN4L"
      },
      "source": [
        "class binaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(binaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(12, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtIn5w8AMMC1"
      },
      "source": [
        "Once, weâ€™ve defined our architecture, we check if our GPU is active. The amazing thing about PyTorch is that itâ€™s super easy to use the GPU.\n",
        "The variable device will either say cuda:0 if we have the GPU. If not, itâ€™ll say cpu . You can follow along this tutorial even if you do not have a GPU without any change in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSY9ILZlL7nz",
        "outputId": "1fd91046-6b3f-4a8a-c218-a7c3fe365654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBFld7NMG6j"
      },
      "source": [
        "Note that we did not use the Sigmoid activation in our final layer during training. Thatâ€™s because, we use the nn.BCEWithLogitsLoss() loss function which automatically applies the the Sigmoid activation. We however, need to use Sigmoid manually during inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5JyXyZcMYxb"
      },
      "source": [
        "Next, we need to initialize our model. After initializing it, we move it to device . Now, this device is a GPU if you have one or itâ€™s CPU if you donâ€™t. The network weâ€™ve used is fairly small. So, it will not take a lot of time to train on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3oFa9dAL7xV",
        "outputId": "2c2b23f7-26cb-449b-b48d-f2f1bf6a3f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGISxMIxMr-H"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihGl7H0ZM8bh"
      },
      "source": [
        "Before we start the actual training, letâ€™s define a function to calculate accuracy.\n",
        "\n",
        "In the function below, we take the predicted and actual output as the input. The predicted value(a probability) is rounded off to convert it into either a 0 or a 1.\n",
        "\n",
        "Once that is done, we simply compare the number of 1/0 we predicted to the number of 1/0 actually present and calculate the accuracy.\n",
        "\n",
        "Note that the inputs y_pred and y_test are for a batch. Our batch_size was 64. So, this accuracy is being calculated for 64 predictions(tensors) at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6blbm9EwMERh"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUpK6ivKNwbp"
      },
      "source": [
        "You can see weâ€™ve put a model.train() at the before the loop. model.train() tells PyTorch that youâ€™re in training mode.\n",
        "\n",
        "Well, why do we need to do that? If youâ€™re using layers such as Dropout or BatchNorm which behave differently during training and evaluation, you need to tell PyTorch to act accordingly. While the default mode in PyTorch is the train, so, you donâ€™t explicitly have to write that. But itâ€™s good practice.\n",
        "\n",
        "Similarly, weâ€™ll call model.eval() when we test our model. Weâ€™ll see that below.\n",
        "\n",
        "Back to training; we start a for-loop. At the top of this for-loop, we initialize our loss and accuracy per epoch to 0. After every epoch, weâ€™ll print out the loss/accuracy and reset it back to 0.\n",
        "\n",
        "Then we have another for-loop. This for-loop is used to get our data in batches from the train_loader.\n",
        "\n",
        "We do optimizer.zero_grad() before we make any predictions. Since the backward() function accumulates gradients, we need to set it to 0 manually per mini-batch.\n",
        "\n",
        "From our defined model, we then obtain a prediction, get the loss(and accuracy) for that mini-batch, perform backpropagation using loss.backward() and optimizer.step() . Finally, we add all the mini-batch losses (and accuracies) to obtain the average loss (and accuracy) for that epoch.\n",
        "\n",
        "This loss and accuracy is printed out in the outer for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZGxYSnAMEUD",
        "outputId": "246a41ba-c0e4-4323-9690-5411b1da7be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.train()\n",
        "for e in range(1, EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        #Get the input, data is a list of [inputs, labels], transfer to GPU\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        #1. Initialise gradients \n",
        "        optimizer.zero_grad()\n",
        "        #2 Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        #3 Backward\n",
        "        loss.backward()\n",
        "        #4 Copute the loss and update the weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item() # Loss each batch\n",
        "        epoch_acc += acc.item() # acc each batch\n",
        "        \n",
        "    # Loss and acc per each Eposhs\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 001: | Loss: 0.73404 | Acc: 49.500\n",
            "Epoch 002: | Loss: 0.62179 | Acc: 65.500\n",
            "Epoch 003: | Loss: 0.56483 | Acc: 74.750\n",
            "Epoch 004: | Loss: 0.51332 | Acc: 80.000\n",
            "Epoch 005: | Loss: 0.50564 | Acc: 79.250\n",
            "Epoch 006: | Loss: 0.48180 | Acc: 83.250\n",
            "Epoch 007: | Loss: 0.45540 | Acc: 82.750\n",
            "Epoch 008: | Loss: 0.44128 | Acc: 81.000\n",
            "Epoch 009: | Loss: 0.42676 | Acc: 85.500\n",
            "Epoch 010: | Loss: 0.40659 | Acc: 86.500\n",
            "Epoch 011: | Loss: 0.40709 | Acc: 83.250\n",
            "Epoch 012: | Loss: 0.40034 | Acc: 86.000\n",
            "Epoch 013: | Loss: 0.36650 | Acc: 86.250\n",
            "Epoch 014: | Loss: 0.35865 | Acc: 88.000\n",
            "Epoch 015: | Loss: 0.35159 | Acc: 88.250\n",
            "Epoch 016: | Loss: 0.34127 | Acc: 88.000\n",
            "Epoch 017: | Loss: 0.32761 | Acc: 88.500\n",
            "Epoch 018: | Loss: 0.31594 | Acc: 91.250\n",
            "Epoch 019: | Loss: 0.30948 | Acc: 90.000\n",
            "Epoch 020: | Loss: 0.30402 | Acc: 90.250\n",
            "Epoch 021: | Loss: 0.28991 | Acc: 89.250\n",
            "Epoch 022: | Loss: 0.27839 | Acc: 92.000\n",
            "Epoch 023: | Loss: 0.25964 | Acc: 91.750\n",
            "Epoch 024: | Loss: 0.24015 | Acc: 92.500\n",
            "Epoch 025: | Loss: 0.24978 | Acc: 92.000\n",
            "Epoch 026: | Loss: 0.23495 | Acc: 92.000\n",
            "Epoch 027: | Loss: 0.22540 | Acc: 93.750\n",
            "Epoch 028: | Loss: 0.21602 | Acc: 95.750\n",
            "Epoch 029: | Loss: 0.20996 | Acc: 94.500\n",
            "Epoch 030: | Loss: 0.19583 | Acc: 94.500\n",
            "Epoch 031: | Loss: 0.17723 | Acc: 96.750\n",
            "Epoch 032: | Loss: 0.16939 | Acc: 96.750\n",
            "Epoch 033: | Loss: 0.16099 | Acc: 95.250\n",
            "Epoch 034: | Loss: 0.14242 | Acc: 97.250\n",
            "Epoch 035: | Loss: 0.12721 | Acc: 98.750\n",
            "Epoch 036: | Loss: 0.13416 | Acc: 96.500\n",
            "Epoch 037: | Loss: 0.13513 | Acc: 96.750\n",
            "Epoch 038: | Loss: 0.13897 | Acc: 95.750\n",
            "Epoch 039: | Loss: 0.11559 | Acc: 98.000\n",
            "Epoch 040: | Loss: 0.10481 | Acc: 97.750\n",
            "Epoch 041: | Loss: 0.10964 | Acc: 98.500\n",
            "Epoch 042: | Loss: 0.11238 | Acc: 98.250\n",
            "Epoch 043: | Loss: 0.10858 | Acc: 98.000\n",
            "Epoch 044: | Loss: 0.08821 | Acc: 99.500\n",
            "Epoch 045: | Loss: 0.08937 | Acc: 99.000\n",
            "Epoch 046: | Loss: 0.09734 | Acc: 97.500\n",
            "Epoch 047: | Loss: 0.08090 | Acc: 99.500\n",
            "Epoch 048: | Loss: 0.07229 | Acc: 98.250\n",
            "Epoch 049: | Loss: 0.07549 | Acc: 99.250\n",
            "Epoch 050: | Loss: 0.07319 | Acc: 99.250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0bN3LynOnCw"
      },
      "source": [
        "After training is done, we need to test how our model fared. Note that weâ€™ve used model.eval() before we run our testing code. To tell PyTorch that we do not want to perform back-propagation during inference, we use torch.no_grad() which reduces memory usage and speeds up computation.\n",
        "\n",
        "We start by defining a list that will hold our predictions. Then we loop through our batches using the test_loader. For each batch â€”\n",
        "\n",
        "- We make the predictions using our trained model.\n",
        "- Round off the probabilities to 1 or 0.\n",
        "- Move the batch to the GPU from the CPU.\n",
        "- Convert the tensor to a numpy object and append it to our list.\n",
        "- Flatten out the list so that we can use it as an input to confusion_matrix and classification_report ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-xHPNZaMEWn"
      },
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch) # make predictions\n",
        "        y_test_pred = torch.sigmoid(y_test_pred) # convert to propability 0-1\n",
        "        y_pred_tag = torch.round(y_test_pred) # round to 0,1\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())# move batch to GPU, convert to numpy object and append to list\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list] #Flatten out the list for confusion matrix and classification report"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHh77-vNoLH",
        "outputId": "aa62f375-2bc4-4c70-8f79-e262531c3a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred_list))\n",
        "print('balanced_accuracy_score: ',balanced_accuracy_score(y_test, y_pred_list))\n",
        "print('matthews_corrcoef: ',matthews_corrcoef(y_test, y_pred_list))\n",
        "print(classification_report(y_test, y_pred_list))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12  6]\n",
            " [ 7 37]]\n",
            "balanced_accuracy_score:  0.7537878787878788\n",
            "matthews_corrcoef:  0.49974957565417644\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.67      0.65        18\n",
            "           1       0.86      0.84      0.85        44\n",
            "\n",
            "    accuracy                           0.79        62\n",
            "   macro avg       0.75      0.75      0.75        62\n",
            "weighted avg       0.79      0.79      0.79        62\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1GMAiaxvEVk",
        "outputId": "6279da30-cbb6-4fe1-8136-a3771753a2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "model=XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('Balanced accuracy score:',balanced_accuracy_score(y_test,y_pred))\n",
        "print('Matthews_corrcoef accuracy score:',matthews_corrcoef(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Balanced accuracy score: 0.7424242424242424\n",
            "Matthews_corrcoef accuracy score: 0.47079190906919977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63        18\n",
            "           1       0.86      0.82      0.84        44\n",
            "\n",
            "    accuracy                           0.77        62\n",
            "   macro avg       0.73      0.74      0.73        62\n",
            "weighted avg       0.78      0.77      0.78        62\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvO-08OWlRmT"
      },
      "source": [
        "# Churn prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR7CC_QOwJLQ"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns ; sns.set()\n",
        "#pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#Load sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report ,balanced_accuracy_score,matthews_corrcoef\n",
        "\n",
        "#Special\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS-4tYHnwdt4"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4-7xUJ7NpJg",
        "outputId": "9ea4ca0a-5a83-407e-e6ef-e5913579f2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "path='https://raw.githubusercontent.com/rstudio/keras-customer-churn/master/data/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "df=pd.read_csv(path)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7043, 21)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customerID</th>\n",
              "      <th>gender</th>\n",
              "      <th>SeniorCitizen</th>\n",
              "      <th>Partner</th>\n",
              "      <th>Dependents</th>\n",
              "      <th>tenure</th>\n",
              "      <th>PhoneService</th>\n",
              "      <th>MultipleLines</th>\n",
              "      <th>InternetService</th>\n",
              "      <th>OnlineSecurity</th>\n",
              "      <th>OnlineBackup</th>\n",
              "      <th>DeviceProtection</th>\n",
              "      <th>TechSupport</th>\n",
              "      <th>StreamingTV</th>\n",
              "      <th>StreamingMovies</th>\n",
              "      <th>Contract</th>\n",
              "      <th>PaperlessBilling</th>\n",
              "      <th>PaymentMethod</th>\n",
              "      <th>MonthlyCharges</th>\n",
              "      <th>TotalCharges</th>\n",
              "      <th>Churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7590-VHVEG</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>1</td>\n",
              "      <td>No</td>\n",
              "      <td>No phone service</td>\n",
              "      <td>DSL</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Electronic check</td>\n",
              "      <td>29.85</td>\n",
              "      <td>29.85</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5575-GNVDE</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>34</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>One year</td>\n",
              "      <td>No</td>\n",
              "      <td>Mailed check</td>\n",
              "      <td>56.95</td>\n",
              "      <td>1889.5</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3668-QPYBK</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>2</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Mailed check</td>\n",
              "      <td>53.85</td>\n",
              "      <td>108.15</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7795-CFOCW</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>45</td>\n",
              "      <td>No</td>\n",
              "      <td>No phone service</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>One year</td>\n",
              "      <td>No</td>\n",
              "      <td>Bank transfer (automatic)</td>\n",
              "      <td>42.30</td>\n",
              "      <td>1840.75</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9237-HQITU</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>2</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Fiber optic</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Electronic check</td>\n",
              "      <td>70.70</td>\n",
              "      <td>151.65</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   customerID  gender  SeniorCitizen  ... MonthlyCharges TotalCharges  Churn\n",
              "0  7590-VHVEG  Female              0  ...          29.85        29.85     No\n",
              "1  5575-GNVDE    Male              0  ...          56.95       1889.5     No\n",
              "2  3668-QPYBK    Male              0  ...          53.85       108.15    Yes\n",
              "3  7795-CFOCW    Male              0  ...          42.30      1840.75     No\n",
              "4  9237-HQITU  Female              0  ...          70.70       151.65    Yes\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upjFKnOgvSFQ",
        "outputId": "99c3dbc5-355e-4d69-8bc3-fed545cea23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "df['Churn'].value_counts(normalize=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.73463\n",
              "1    0.26537\n",
              "Name: Churn, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ALkpC1_vSZP"
      },
      "source": [
        "df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan).astype(float)\n",
        "df = df.dropna()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-xf0dgWv2Mf",
        "outputId": "9267f6a6-debe-4ea0-f054-dee4da3170a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "#One hot encoding\n",
        "dummy_cols = []\n",
        "# column with value\n",
        "sample_set = df[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']].copy(deep=True)\n",
        "# for other column with category, only one hot with column have nuique < 5\n",
        "for col in list(df.columns):\n",
        "    if col not in ['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn'] and df[col].nunique() < 5:\n",
        "        dummy_vars = pd.get_dummies(df[col])\n",
        "        dummy_vars.columns = [col+str(x) for x in dummy_vars.columns]        \n",
        "        sample_set = pd.concat([sample_set, dummy_vars], axis=1)\n",
        "sample_set.head(10)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tenure</th>\n",
              "      <th>MonthlyCharges</th>\n",
              "      <th>TotalCharges</th>\n",
              "      <th>Churn</th>\n",
              "      <th>genderFemale</th>\n",
              "      <th>genderMale</th>\n",
              "      <th>SeniorCitizen0</th>\n",
              "      <th>SeniorCitizen1</th>\n",
              "      <th>PartnerNo</th>\n",
              "      <th>PartnerYes</th>\n",
              "      <th>DependentsNo</th>\n",
              "      <th>DependentsYes</th>\n",
              "      <th>PhoneServiceNo</th>\n",
              "      <th>PhoneServiceYes</th>\n",
              "      <th>MultipleLinesNo</th>\n",
              "      <th>MultipleLinesNo phone service</th>\n",
              "      <th>MultipleLinesYes</th>\n",
              "      <th>InternetServiceDSL</th>\n",
              "      <th>InternetServiceFiber optic</th>\n",
              "      <th>InternetServiceNo</th>\n",
              "      <th>OnlineSecurityNo</th>\n",
              "      <th>OnlineSecurityNo internet service</th>\n",
              "      <th>OnlineSecurityYes</th>\n",
              "      <th>OnlineBackupNo</th>\n",
              "      <th>OnlineBackupNo internet service</th>\n",
              "      <th>OnlineBackupYes</th>\n",
              "      <th>DeviceProtectionNo</th>\n",
              "      <th>DeviceProtectionNo internet service</th>\n",
              "      <th>DeviceProtectionYes</th>\n",
              "      <th>TechSupportNo</th>\n",
              "      <th>TechSupportNo internet service</th>\n",
              "      <th>TechSupportYes</th>\n",
              "      <th>StreamingTVNo</th>\n",
              "      <th>StreamingTVNo internet service</th>\n",
              "      <th>StreamingTVYes</th>\n",
              "      <th>StreamingMoviesNo</th>\n",
              "      <th>StreamingMoviesNo internet service</th>\n",
              "      <th>StreamingMoviesYes</th>\n",
              "      <th>ContractMonth-to-month</th>\n",
              "      <th>ContractOne year</th>\n",
              "      <th>ContractTwo year</th>\n",
              "      <th>PaperlessBillingNo</th>\n",
              "      <th>PaperlessBillingYes</th>\n",
              "      <th>PaymentMethodBank transfer (automatic)</th>\n",
              "      <th>PaymentMethodCredit card (automatic)</th>\n",
              "      <th>PaymentMethodElectronic check</th>\n",
              "      <th>PaymentMethodMailed check</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>29.85</td>\n",
              "      <td>29.85</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34</td>\n",
              "      <td>56.95</td>\n",
              "      <td>1889.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>53.85</td>\n",
              "      <td>108.15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>42.30</td>\n",
              "      <td>1840.75</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>70.70</td>\n",
              "      <td>151.65</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>99.65</td>\n",
              "      <td>820.50</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>22</td>\n",
              "      <td>89.10</td>\n",
              "      <td>1949.40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>29.75</td>\n",
              "      <td>301.90</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>28</td>\n",
              "      <td>104.80</td>\n",
              "      <td>3046.05</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>62</td>\n",
              "      <td>56.15</td>\n",
              "      <td>3487.95</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tenure  ...  PaymentMethodMailed check\n",
              "0       1  ...                          0\n",
              "1      34  ...                          1\n",
              "2       2  ...                          1\n",
              "3      45  ...                          0\n",
              "4       2  ...                          0\n",
              "5       8  ...                          0\n",
              "6      22  ...                          0\n",
              "7      10  ...                          1\n",
              "8      28  ...                          0\n",
              "9      62  ...                          0\n",
              "\n",
              "[10 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWqdZzsev7Tm",
        "outputId": "6100bf42-d177-43d8-ec46-5f63e7314c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X = sample_set.drop(columns='Churn')\n",
        "y= sample_set['Churn']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "X_train,  X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, \n",
        "                                                     random_state = 42,stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7032, 46)\n",
            "(7032,)\n",
            "(5625, 46) (5625,)\n",
            "(1407, 46) (1407,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ3sF1UZv-g-",
        "outputId": "492a3b5c-01ba-46a9-b8c5-8a61cf4b3141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model=XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('Balanced accuracy score:',balanced_accuracy_score(y_test,y_pred))\n",
        "print('Matthews_corrcoef accuracy score:',matthews_corrcoef(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Balanced accuracy score: 0.6944287703640816\n",
            "Matthews_corrcoef accuracy score: 0.4285486325890265\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86      1033\n",
            "           1       0.64      0.49      0.55       374\n",
            "\n",
            "    accuracy                           0.79      1407\n",
            "   macro avg       0.74      0.69      0.71      1407\n",
            "weighted avg       0.78      0.79      0.78      1407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW2Qd65kwaym"
      },
      "source": [
        "## Pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A59PA1RzwA4f"
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb3mBh5Xw7qt"
      },
      "source": [
        "## train data\n",
        "class trainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "## test data    \n",
        "class testData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBaEf9rs48U7"
      },
      "source": [
        "train_data = trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
        "test_data = testData(torch.FloatTensor(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj7ss93q5Hl7",
        "outputId": "f6203411-00cd-4ce2-f8db-3302b4b85143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db9qkjhdw-nk"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrAzDsSLw_13"
      },
      "source": [
        "class binaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(binaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(X.shape[1], 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIo7VY3rxBFM",
        "outputId": "92a26ff3-cb90-47cc-8250-298411212835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtIw68fKxB07",
        "outputId": "53f4bf00-f6a6-4334-bdb1-080bd213462e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=46, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmKRMXA3xC7c"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSbk6EgxEZJ",
        "outputId": "227930a6-1608-4393-ed81-58f5f40bbabc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.train()\n",
        "for e in range(1, EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        #Get the input, data is a list of [inputs, labels], transfer to GPU\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        #1. Initialise gradients \n",
        "        optimizer.zero_grad()\n",
        "        #2 Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        #3 Backward\n",
        "        loss.backward()\n",
        "        #4 Copute the loss and update the weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item() # Loss each batch\n",
        "        epoch_acc += acc.item() # acc each batch\n",
        "        \n",
        "    # Loss and acc per each Eposhs\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 001: | Loss: 0.53652 | Acc: 75.205\n",
            "Epoch 002: | Loss: 0.45769 | Acc: 79.307\n",
            "Epoch 003: | Loss: 0.42025 | Acc: 80.341\n",
            "Epoch 004: | Loss: 0.40758 | Acc: 81.034\n",
            "Epoch 005: | Loss: 0.40132 | Acc: 81.045\n",
            "Epoch 006: | Loss: 0.39310 | Acc: 81.159\n",
            "Epoch 007: | Loss: 0.39445 | Acc: 81.375\n",
            "Epoch 008: | Loss: 0.39100 | Acc: 81.727\n",
            "Epoch 009: | Loss: 0.38122 | Acc: 82.398\n",
            "Epoch 010: | Loss: 0.38077 | Acc: 82.080\n",
            "Epoch 011: | Loss: 0.37267 | Acc: 82.466\n",
            "Epoch 012: | Loss: 0.37510 | Acc: 82.114\n",
            "Epoch 013: | Loss: 0.36919 | Acc: 82.568\n",
            "Epoch 014: | Loss: 0.36843 | Acc: 82.443\n",
            "Epoch 015: | Loss: 0.36131 | Acc: 83.239\n",
            "Epoch 016: | Loss: 0.35704 | Acc: 83.284\n",
            "Epoch 017: | Loss: 0.35807 | Acc: 83.091\n",
            "Epoch 018: | Loss: 0.35137 | Acc: 83.420\n",
            "Epoch 019: | Loss: 0.35336 | Acc: 83.432\n",
            "Epoch 020: | Loss: 0.34553 | Acc: 83.727\n",
            "Epoch 021: | Loss: 0.33794 | Acc: 84.420\n",
            "Epoch 022: | Loss: 0.34286 | Acc: 84.216\n",
            "Epoch 023: | Loss: 0.33959 | Acc: 84.500\n",
            "Epoch 024: | Loss: 0.33001 | Acc: 84.750\n",
            "Epoch 025: | Loss: 0.32645 | Acc: 84.898\n",
            "Epoch 026: | Loss: 0.32398 | Acc: 85.239\n",
            "Epoch 027: | Loss: 0.32881 | Acc: 84.682\n",
            "Epoch 028: | Loss: 0.31983 | Acc: 85.818\n",
            "Epoch 029: | Loss: 0.32362 | Acc: 84.761\n",
            "Epoch 030: | Loss: 0.31933 | Acc: 85.511\n",
            "Epoch 031: | Loss: 0.31400 | Acc: 85.636\n",
            "Epoch 032: | Loss: 0.31115 | Acc: 85.398\n",
            "Epoch 033: | Loss: 0.31174 | Acc: 85.807\n",
            "Epoch 034: | Loss: 0.30321 | Acc: 86.091\n",
            "Epoch 035: | Loss: 0.30104 | Acc: 86.295\n",
            "Epoch 036: | Loss: 0.30104 | Acc: 86.784\n",
            "Epoch 037: | Loss: 0.29706 | Acc: 86.273\n",
            "Epoch 038: | Loss: 0.30097 | Acc: 86.455\n",
            "Epoch 039: | Loss: 0.29094 | Acc: 86.864\n",
            "Epoch 040: | Loss: 0.28620 | Acc: 86.307\n",
            "Epoch 041: | Loss: 0.28658 | Acc: 87.034\n",
            "Epoch 042: | Loss: 0.28096 | Acc: 87.318\n",
            "Epoch 043: | Loss: 0.28593 | Acc: 87.125\n",
            "Epoch 044: | Loss: 0.27860 | Acc: 87.443\n",
            "Epoch 045: | Loss: 0.27651 | Acc: 87.739\n",
            "Epoch 046: | Loss: 0.27448 | Acc: 87.523\n",
            "Epoch 047: | Loss: 0.27788 | Acc: 87.466\n",
            "Epoch 048: | Loss: 0.27391 | Acc: 87.193\n",
            "Epoch 049: | Loss: 0.27142 | Acc: 87.818\n",
            "Epoch 050: | Loss: 0.26985 | Acc: 87.864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHHpQBC4xFR6"
      },
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch) # make predictions\n",
        "        y_test_pred = torch.sigmoid(y_test_pred) # convert to propability 0-1\n",
        "        y_pred_tag = torch.round(y_test_pred) # round to 0,1\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())# move batch to GPU, convert to numpy object and append to list\n",
        "\n",
        "y_pred = [a.squeeze().tolist() for a in y_pred_list] #Flatten out the list for confusion matrix and classification report"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "871fXYHdxIUI",
        "outputId": "a80922f8-ce99-4d83-fa6d-f39753a07f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print('balanced_accuracy_score: ',balanced_accuracy_score(y_test, y_pred))\n",
        "print('matthews_corrcoef: ',matthews_corrcoef(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "balanced_accuracy_score:  0.6943834737098218\n",
            "matthews_corrcoef:  0.40448462897704845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85      1033\n",
            "           1       0.59      0.52      0.55       374\n",
            "\n",
            "    accuracy                           0.78      1407\n",
            "   macro avg       0.71      0.69      0.70      1407\n",
            "weighted avg       0.77      0.78      0.77      1407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL9FBNkl50H5"
      },
      "source": [
        "## Pytorch model 2 (with train,val and test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU9C34Z1AVHF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns ; sns.set()\n",
        "#pytorch\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#Load sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report ,balanced_accuracy_score,matthews_corrcoef\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnHA1NUp6cj_",
        "outputId": "76ae1209-ece4-42e3-a2c8-1039aad191c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Train - Test\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2,stratify=y,\n",
        "                                                          random_state=2020)\n",
        "# Split train into train-val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, stratify=y_trainval,\n",
        "                                                  test_size=0.1, random_state=2020)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.fit_transform(X_val)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7032, 46) (7032,)\n",
            "(5062, 46) (5062,)\n",
            "(563, 46) (563,)\n",
            "(1407, 46) (1407,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1WO4ssn5Tck"
      },
      "source": [
        "class BinaryData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7CLHRS66Wk-",
        "outputId": "a0bc9c02-874b-4cc0-cbcb-04fedacf1c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = BinaryData(torch.FloatTensor(X_train),torch.FloatTensor(y_train.values))\n",
        "val_data   = BinaryData(torch.FloatTensor(X_val),torch.FloatTensor(y_val.values))\n",
        "test_data = BinaryData(torch.FloatTensor(X_test),torch.FloatTensor(y_test.values))\n",
        "print(train_data.X_data.shape,train_data.y_data.shape)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5625, 46]) torch.Size([5625])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTt9Jsv98mDA"
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPzePeQ9wE9"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=1)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VX-GHFi9wPD"
      },
      "source": [
        "class binaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(binaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(X.shape[1], 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31G1GgrRA-0i"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEIXZnPE9wTF",
        "outputId": "81e970b8-946a-4cdc-ef77-01063e681814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=46, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsGmzPUrBmOd"
      },
      "source": [
        "loss_stats = {'train': [],'val': []}\n",
        "acc_stats={'train': [], 'val': []}"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LXrpXGG9wM-",
        "outputId": "7018b35b-5725-4617-bdbb-25d2a9288ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916,
          "referenced_widgets": [
            "e1e93ee9c16e4d6f9bd898418374b5e0",
            "66b762c435804e48a3575cd1b37c0c2f",
            "74fd7f375e984835b5a5eb8bda068af0",
            "090a49da710b421cbb90bdd3d9c6672d",
            "6521596a98e84bb693e18f9fa337ff2a",
            "394fab414ef34376a093ffb0c0ae6c72",
            "a238b4c2e8564a6992a57f8f8261dd8c",
            "d82b941790b74651bb5b22f96e7a94c2"
          ]
        }
      },
      "source": [
        "model.train()\n",
        "for e in tqdm(range(1, EPOCHS+1)):\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "  model.train() # model.train() tells PyTorch that youâ€™re in training mode.\n",
        "  for X_batch, y_batch in train_loader:\n",
        "      #Get the input, data is a list of [inputs, labels], transfer to GPU\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      #1. Initialise gradients \n",
        "      optimizer.zero_grad()\n",
        "      #2 Forward pass\n",
        "      y_pred = model(X_batch)\n",
        "      train_loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "      train_acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "      #3 Backward\n",
        "      train_loss.backward()\n",
        "      #4 Copute the loss and update the weights\n",
        "      optimizer.step()\n",
        "      \n",
        "      train_epoch_loss += train_loss.item() # Loss each batch\n",
        "      train_epoch_acc += train_acc.item() # acc each batch\n",
        "         \n",
        "  # VALIDATION \n",
        "  #tells PyTorch that we do not want to perform back-propagation, which reduces memory usage and speeds up computation.   \n",
        "  with torch.no_grad():\n",
        "\n",
        "      val_epoch_loss = 0\n",
        "      val_epoch_acc = 0\n",
        "      model.eval() # weâ€™ll call model.eval() when we test our model\n",
        "      for X_val_batch, y_val_batch in val_loader:\n",
        "          X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "          y_val_pred = model(X_val_batch)    \n",
        "          val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n",
        "          val_acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "\n",
        "          val_epoch_loss += val_loss.item()\n",
        "          val_epoch_acc += val_acc.item() # acc each batch\n",
        "\n",
        "  train_loss_final=train_epoch_loss/len(train_loader) # loss each epoch\n",
        "  train_acc_final =train_epoch_acc/len(train_loader) # acc each eopch\n",
        "  val_loss_final  =val_epoch_loss/len(val_loader)\n",
        "  val_acc_final   =val_epoch_acc/len(val_loader)\n",
        "\n",
        "  loss_stats['train'].append(train_loss_final) # list of lost each epoch\n",
        "  acc_stats['train'].append(train_acc_final)\n",
        "  loss_stats['val'].append(val_loss_final)    \n",
        "  acc_stats['val'].append(val_acc_final)     \n",
        "  # Loss and acc per each Eposhs\n",
        "  print(f'Epoch {e+0:03}: | Train Loss: {train_loss_final:.5f} | Train Acc: {train_acc_final:.3f}| Val Loss: {val_loss_final:.5f} | Val Acc: {val_acc_final:.3f}')"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1e93ee9c16e4d6f9bd898418374b5e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 001: | Train Loss: 0.53801 | Train Acc: 74.670| Val Loss: 0.48482 | Acc: 72.000\n",
            "Epoch 002: | Train Loss: 0.46116 | Train Acc: 79.330| Val Loss: 0.44368 | Acc: 77.000\n",
            "Epoch 003: | Train Loss: 0.42395 | Train Acc: 80.182| Val Loss: 0.40593 | Acc: 93.000\n",
            "Epoch 004: | Train Loss: 0.40767 | Train Acc: 80.761| Val Loss: 0.40592 | Acc: 72.000\n",
            "Epoch 005: | Train Loss: 0.40021 | Train Acc: 80.818| Val Loss: 0.39738 | Acc: 82.000\n",
            "Epoch 006: | Train Loss: 0.39607 | Train Acc: 81.409| Val Loss: 0.39845 | Acc: 70.000\n",
            "Epoch 007: | Train Loss: 0.39293 | Train Acc: 81.693| Val Loss: 0.39410 | Acc: 81.000\n",
            "Epoch 008: | Train Loss: 0.39101 | Train Acc: 81.545| Val Loss: 0.38803 | Acc: 88.000\n",
            "Epoch 009: | Train Loss: 0.38583 | Train Acc: 81.568| Val Loss: 0.39856 | Acc: 89.000\n",
            "Epoch 010: | Train Loss: 0.37813 | Train Acc: 81.352| Val Loss: 0.39114 | Acc: 84.000\n",
            "Epoch 011: | Train Loss: 0.37842 | Train Acc: 82.420| Val Loss: 0.38844 | Acc: 86.000\n",
            "Epoch 012: | Train Loss: 0.37375 | Train Acc: 82.500| Val Loss: 0.38114 | Acc: 84.000\n",
            "Epoch 013: | Train Loss: 0.37381 | Train Acc: 82.011| Val Loss: 0.37920 | Acc: 79.000\n",
            "Epoch 014: | Train Loss: 0.36873 | Train Acc: 82.739| Val Loss: 0.38333 | Acc: 82.000\n",
            "Epoch 015: | Train Loss: 0.36434 | Train Acc: 82.568| Val Loss: 0.38058 | Acc: 79.000\n",
            "Epoch 016: | Train Loss: 0.36026 | Train Acc: 83.091| Val Loss: 0.37052 | Acc: 77.000\n",
            "Epoch 017: | Train Loss: 0.36013 | Train Acc: 82.773| Val Loss: 0.37053 | Acc: 81.000\n",
            "Epoch 018: | Train Loss: 0.35514 | Train Acc: 83.114| Val Loss: 0.36318 | Acc: 88.000\n",
            "Epoch 019: | Train Loss: 0.35156 | Train Acc: 83.205| Val Loss: 0.35636 | Acc: 79.000\n",
            "Epoch 020: | Train Loss: 0.35153 | Train Acc: 83.170| Val Loss: 0.35332 | Acc: 84.000\n",
            "Epoch 021: | Train Loss: 0.34547 | Train Acc: 83.670| Val Loss: 0.36222 | Acc: 82.000\n",
            "Epoch 022: | Train Loss: 0.34238 | Train Acc: 84.170| Val Loss: 0.36652 | Acc: 86.000\n",
            "Epoch 023: | Train Loss: 0.34623 | Train Acc: 83.750| Val Loss: 0.35447 | Acc: 88.000\n",
            "Epoch 024: | Train Loss: 0.33793 | Train Acc: 84.216| Val Loss: 0.36493 | Acc: 75.000\n",
            "Epoch 025: | Train Loss: 0.33475 | Train Acc: 83.898| Val Loss: 0.34965 | Acc: 81.000\n",
            "Epoch 026: | Train Loss: 0.33208 | Train Acc: 84.398| Val Loss: 0.35186 | Acc: 91.000\n",
            "Epoch 027: | Train Loss: 0.32902 | Train Acc: 84.898| Val Loss: 0.35893 | Acc: 95.000\n",
            "Epoch 028: | Train Loss: 0.32758 | Train Acc: 84.273| Val Loss: 0.34820 | Acc: 79.000\n",
            "Epoch 029: | Train Loss: 0.32680 | Train Acc: 85.500| Val Loss: 0.35804 | Acc: 88.000\n",
            "Epoch 030: | Train Loss: 0.32016 | Train Acc: 84.886| Val Loss: 0.35469 | Acc: 84.000\n",
            "Epoch 031: | Train Loss: 0.31838 | Train Acc: 85.170| Val Loss: 0.34103 | Acc: 75.000\n",
            "Epoch 032: | Train Loss: 0.31357 | Train Acc: 85.648| Val Loss: 0.35354 | Acc: 84.000\n",
            "Epoch 033: | Train Loss: 0.31726 | Train Acc: 85.432| Val Loss: 0.34653 | Acc: 84.000\n",
            "Epoch 034: | Train Loss: 0.30780 | Train Acc: 85.455| Val Loss: 0.34397 | Acc: 81.000\n",
            "Epoch 035: | Train Loss: 0.30600 | Train Acc: 85.670| Val Loss: 0.33773 | Acc: 88.000\n",
            "Epoch 036: | Train Loss: 0.30258 | Train Acc: 85.989| Val Loss: 0.34090 | Acc: 79.000\n",
            "Epoch 037: | Train Loss: 0.30372 | Train Acc: 85.739| Val Loss: 0.34521 | Acc: 79.000\n",
            "Epoch 038: | Train Loss: 0.29905 | Train Acc: 86.420| Val Loss: 0.32828 | Acc: 75.000\n",
            "Epoch 039: | Train Loss: 0.30035 | Train Acc: 86.375| Val Loss: 0.33389 | Acc: 88.000\n",
            "Epoch 040: | Train Loss: 0.29362 | Train Acc: 86.955| Val Loss: 0.33175 | Acc: 79.000\n",
            "Epoch 041: | Train Loss: 0.29644 | Train Acc: 86.614| Val Loss: 0.35284 | Acc: 82.000\n",
            "Epoch 042: | Train Loss: 0.28739 | Train Acc: 87.125| Val Loss: 0.33557 | Acc: 88.000\n",
            "Epoch 043: | Train Loss: 0.28377 | Train Acc: 87.591| Val Loss: 0.33321 | Acc: 84.000\n",
            "Epoch 044: | Train Loss: 0.29103 | Train Acc: 86.614| Val Loss: 0.33259 | Acc: 88.000\n",
            "Epoch 045: | Train Loss: 0.28809 | Train Acc: 87.159| Val Loss: 0.33854 | Acc: 89.000\n",
            "Epoch 046: | Train Loss: 0.28357 | Train Acc: 86.784| Val Loss: 0.33028 | Acc: 82.000\n",
            "Epoch 047: | Train Loss: 0.28635 | Train Acc: 86.511| Val Loss: 0.33326 | Acc: 91.000\n",
            "Epoch 048: | Train Loss: 0.27803 | Train Acc: 87.170| Val Loss: 0.33385 | Acc: 82.000\n",
            "Epoch 049: | Train Loss: 0.28123 | Train Acc: 87.159| Val Loss: 0.33117 | Acc: 84.000\n",
            "Epoch 050: | Train Loss: 0.26753 | Train Acc: 88.250| Val Loss: 0.34025 | Acc: 88.000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtYWDwrO9wK-",
        "outputId": "7c14d11b-2b2c-4453-e171-5d368069f968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\").set_title('Train-Val Loss/Epoch')"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Train-Val Loss/Epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKY0SNzS9wH9",
        "outputId": "f61f912e-ccfb-43bb-d83e-39d59c5bc875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for X_batch,_ in test_loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_test_pred = model(X_batch) # make predictions\n",
        "    y_test_pred = torch.sigmoid(y_test_pred) # convert to propability 0-1\n",
        "    y_pred_tag = torch.round(y_test_pred) # round to 0,1\n",
        "    y_pred_list.append(y_pred_tag.cpu().numpy())# move batch to GPU, convert to numpy object and append to list\n",
        "\n",
        "y_pred = [a.squeeze().tolist() for a in y_pred_list] #Flatten out the list for confusion matrix and classification report\n",
        "print('balanced_accuracy_score: ',balanced_accuracy_score(y_test, y_pred))\n",
        "print('matthews_corrcoef: ',matthews_corrcoef(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "balanced_accuracy_score:  0.6541380434951416\n",
            "matthews_corrcoef:  0.3362383205880171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1033\n",
            "           1       0.56      0.43      0.49       374\n",
            "\n",
            "    accuracy                           0.76      1407\n",
            "   macro avg       0.68      0.65      0.66      1407\n",
            "weighted avg       0.74      0.76      0.75      1407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWiJ5At6OYAX"
      },
      "source": [
        "# Pytorch model 3 (call back)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYKPP7D_GbYP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}