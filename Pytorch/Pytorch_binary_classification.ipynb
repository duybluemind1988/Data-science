{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch binary classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/duybluemind1988/Data-science/blob/master/Pytorch/Pytorch_binary_classification.ipynb",
      "authorship_tag": "ABX9TyNwkNqz1+6DJy6SGVcp4UK5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/Pytorch/Pytorch_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXyCEd0qMl2D"
      },
      "source": [
        "# Lower back pain symptoms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPIjOS5JCzap"
      },
      "source": [
        "https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtfJ2VUyEJ5d"
      },
      "source": [
        "https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ1bIqoHMpHy"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_cq20eB-y0",
        "outputId": "0c0e4839-8828-45de-de3d-a2b347813c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import balanced_accuracy_score,matthews_corrcoef"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sorKOe-HDevD"
      },
      "source": [
        "310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\n",
        "\n",
        "Lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. Typical sources of low back pain include:\n",
        "\n",
        "- The large nerve roots in the low back that go to the legs may be irritated\n",
        "- The smaller nerves that supply the low back may be irritated\n",
        "- The large paired lower back muscles (erector spinae) may be strained\n",
        "- The bones, ligaments or joints may be damaged\n",
        "- An intervertebral disc may be degenerating\n",
        "\n",
        "An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.\n",
        "\n",
        "While lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\n",
        "\n",
        "This data set is about to identify a person is abnormal or normal using collected physical spine details/data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iBwviBKC1R_",
        "outputId": "0a03444a-b11b-4b68-9a64-b82f5dfe4397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Data/Dataset_spine.csv\")\n",
        "df=df.iloc[:,:-1]\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(310, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Col1</th>\n",
              "      <th>Col2</th>\n",
              "      <th>Col3</th>\n",
              "      <th>Col4</th>\n",
              "      <th>Col5</th>\n",
              "      <th>Col6</th>\n",
              "      <th>Col7</th>\n",
              "      <th>Col8</th>\n",
              "      <th>Col9</th>\n",
              "      <th>Col10</th>\n",
              "      <th>Col11</th>\n",
              "      <th>Col12</th>\n",
              "      <th>Class_att</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63.027818</td>\n",
              "      <td>22.552586</td>\n",
              "      <td>39.609117</td>\n",
              "      <td>40.475232</td>\n",
              "      <td>98.672917</td>\n",
              "      <td>-0.254400</td>\n",
              "      <td>0.744503</td>\n",
              "      <td>12.5661</td>\n",
              "      <td>14.5386</td>\n",
              "      <td>15.30468</td>\n",
              "      <td>-28.658501</td>\n",
              "      <td>43.5123</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39.056951</td>\n",
              "      <td>10.060991</td>\n",
              "      <td>25.015378</td>\n",
              "      <td>28.995960</td>\n",
              "      <td>114.405425</td>\n",
              "      <td>4.564259</td>\n",
              "      <td>0.415186</td>\n",
              "      <td>12.8874</td>\n",
              "      <td>17.5323</td>\n",
              "      <td>16.78486</td>\n",
              "      <td>-25.530607</td>\n",
              "      <td>16.1102</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>68.832021</td>\n",
              "      <td>22.218482</td>\n",
              "      <td>50.092194</td>\n",
              "      <td>46.613539</td>\n",
              "      <td>105.985135</td>\n",
              "      <td>-3.530317</td>\n",
              "      <td>0.474889</td>\n",
              "      <td>26.8343</td>\n",
              "      <td>17.4861</td>\n",
              "      <td>16.65897</td>\n",
              "      <td>-29.031888</td>\n",
              "      <td>19.2221</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>69.297008</td>\n",
              "      <td>24.652878</td>\n",
              "      <td>44.311238</td>\n",
              "      <td>44.644130</td>\n",
              "      <td>101.868495</td>\n",
              "      <td>11.211523</td>\n",
              "      <td>0.369345</td>\n",
              "      <td>23.5603</td>\n",
              "      <td>12.7074</td>\n",
              "      <td>11.42447</td>\n",
              "      <td>-30.470246</td>\n",
              "      <td>18.8329</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49.712859</td>\n",
              "      <td>9.652075</td>\n",
              "      <td>28.317406</td>\n",
              "      <td>40.060784</td>\n",
              "      <td>108.168725</td>\n",
              "      <td>7.918501</td>\n",
              "      <td>0.543360</td>\n",
              "      <td>35.4940</td>\n",
              "      <td>15.9546</td>\n",
              "      <td>8.87237</td>\n",
              "      <td>-16.378376</td>\n",
              "      <td>24.9171</td>\n",
              "      <td>Abnormal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Col1       Col2       Col3  ...      Col11    Col12  Class_att\n",
              "0  63.027818  22.552586  39.609117  ... -28.658501  43.5123   Abnormal\n",
              "1  39.056951  10.060991  25.015378  ... -25.530607  16.1102   Abnormal\n",
              "2  68.832021  22.218482  50.092194  ... -29.031888  19.2221   Abnormal\n",
              "3  69.297008  24.652878  44.311238  ... -30.470246  18.8329   Abnormal\n",
              "4  49.712859   9.652075  28.317406  ... -16.378376  24.9171   Abnormal\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUHKizrGDJQv",
        "outputId": "6c86eec0-c7ad-4888-8bc1-96231b6b83e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "sns.countplot(x = 'Class_att', data=df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa87da24588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEHCAYAAABBW1qbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATUklEQVR4nO3dfbRddX3n8fcHxLY+UHC4UgRikIl2oWKQW8Zqcai2U3B1DLJmqBnLg7KMrhFHbe2MDzPVdo1rukYpg2hxhREBB1E0otgyVRajUlsfSCCG8GAFCoWsGFJwCfWBNvCdP86+Pw7Xm+Qkcs6+4bxfa5119v7uvc/53uTcfLL32Xv/UlVIkgSwV98NSJIWD0NBktQYCpKkxlCQJDWGgiSpeULfDfwsDjjggFq6dGnfbUjSHmXdunX/UFUzCy3bo0Nh6dKlrF27tu82JGmPkuTO7S3z8JEkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSp2aOvaH4sHP0HF/fdghahde8/te8WpF64pyBJagwFSVJjKEiSGkNBktQYCpKkZmyhkOTQJF9OclOSG5O8pas/LclVSb7bPe/f1ZPkg0luTbIhyQvH1ZskaWHj3FPYBvx+VR0BvAh4U5IjgHcAV1fVMuDqbh7gBGBZ91gFnDfG3iRJCxhbKFTV5qq6rpt+ALgZOBhYAVzUrXYRcGI3vQK4uAa+AeyX5KBx9SdJ+mkT+U4hyVLgKOCbwIFVtblb9D3gwG76YOCuoc3u7mqSpAkZeygkeQqwBnhrVd0/vKyqCqhdfL1VSdYmWbt169bHsFNJ0lhDIck+DALhkqr6bFfeMndYqHu+p6tvAg4d2vyQrvYoVbW6qmaranZmZmZ8zUvSFBrn2UcBPgrcXFV/OrToCuC0bvo04PND9VO7s5BeBPxg6DCTJGkCxnlDvJcApwA3JFnf1d4F/AlwWZIzgDuBk7tlVwKvAG4FfgS8doy9SZIWMLZQqKqvAdnO4pcvsH4BbxpXP5KknfOKZklSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqxjkc5wVJ7kmycaj2qSTru8cdcyOyJVma5MdDyz4yrr4kSds3zuE4LwQ+BFw8V6iq35mbTnIW8IOh9W+rquVj7EeStBPjHI7zmiRLF1qWJAzGZn7ZuN5fkrTr+vpO4VhgS1V9d6h2WJLrk3w1ybHb2zDJqiRrk6zdunXr+DuVpCnSVyisBC4dmt8MLKmqo4DfAz6RZN+FNqyq1VU1W1WzMzMzE2hVkqbHxEMhyROAk4BPzdWq6sGqurebXgfcBjx70r1J0rTrY0/hN4BbquruuUKSmSR7d9PPApYBt/fQmyRNtXGeknop8HXgOUnuTnJGt+jVPPrQEcBLgQ3dKaqfAd5YVfeNqzdJ0sLGefbRyu3UT1+gtgZYM65eJEmj8YpmSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWrGOfLaBUnuSbJxqPbeJJuSrO8erxha9s4ktyb5TpLfGldfkqTtG+eewoXA8QvUz66q5d3jSoAkRzAYpvO53TZ/NjdmsyRpcsYWClV1DTDqOMsrgE9W1YNV9XfArcAx4+pNkrSwPr5TODPJhu7w0v5d7WDgrqF17u5qPyXJqiRrk6zdunXruHuVpKky6VA4DzgcWA5sBs7a1ReoqtVVNVtVszMzM491f5I01SYaClW1paoeqqqHgfN55BDRJuDQoVUP6WqSpAmaaCgkOWho9lXA3JlJVwCvTvJzSQ4DlgHfmmRvkiR4wrheOMmlwHHAAUnuBt4DHJdkOVDAHcAbAKrqxiSXATcB24A3VdVD4+pNkrSwsYVCVa1coPzRHaz/PuB94+pHkrRzXtEsSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpGVsoJLkgyT1JNg7V3p/kliQbklyeZL+uvjTJj5Os7x4fGVdfkqTtG+eewoXA8fNqVwHPq6ojgb8F3jm07LaqWt493jjGviRJ2zG2UKiqa4D75tW+VFXbutlvAIeM6/0lSbuuz+8UXgf836H5w5Jcn+SrSY7tqylJmmZjG6N5R5K8G9gGXNKVNgNLqureJEcDn0vy3Kq6f4FtVwGrAJYsWTKpliVpKkx8TyHJ6cBvA6+pqgKoqger6t5ueh1wG/DshbavqtVVNVtVszMzMxPqWpKmw0RDIcnxwH8GXllVPxqqzyTZu5t+FrAMuH2SvUmSRgyFJFePUpu3/FLg68Bzktyd5AzgQ8BTgavmnXr6UmBDkvXAZ4A3VtV9C76wJGlsdvidQpKfB54EHJBkfyDdon2Bg3e0bVWtXKD80e2suwZYs9NuJUljtbMvmt8AvBV4BrCOR0Lhfgb/65ckPY7sMBSq6hzgnCRvrqpzJ9STJKknI52SWlXnJnkxsHR4m6q6eEx9SZJ6MFIoJPk4cDiwHnioKxdgKEjS48ioF6/NAkfMXVcgSXp8GvU6hY3AL42zEUlS/0bdUzgAuCnJt4AH54pV9cqxdCVJ6sWoofDecTYhSVocRj376KvjbkSS1L9Rzz56gMHZRgBPBPYBflhV+46rMUnS5I26p/DUuekkAVYALxpXU5KkfuzyXVJr4HPAb42hH0lSj0Y9fHTS0OxeDK5b+MlYOpIk9WbUs4/+7dD0NuAOBoeQJEmPI6N+p/DacTciSerfqIPsHJLk8iT3dI81SQ4Zd3OSpMka9YvmjwFXMBhX4RnAF7raDiW5oAuRjUO1pyW5Ksl3u+f9u3qSfDDJrUk2JHnhrv84kqSfxaihMFNVH6uqbd3jQmBmhO0uBI6fV3sHcHVVLQOu7uYBTmAwNvMyYBVw3oi9SZIeI6OGwr1JfjfJ3t3jd4F7d7ZRVV0DzB9reQVwUTd9EXDiUP3i7pTXbwD7JTloxP4kSY+BUc8+eh1wLnA2gyub/wY4fTff88Cq2txNfw84sJs+GLhraL27u9rmoRpJVjHYk2DJkiW72YK0+P39Hz+/7xa0CC35wxvG+vqj7in8MXBaVc1U1dMZhMQf/axv3o3PsEtjNFTV6qqararZmZlRjmBJkkY1aigcWVXfn5upqvuAo3bzPbfMHRbqnu/p6puAQ4fWO6SrSZImZNRQ2GvuLCEYnEHE6Iee5rsCOK2bPg34/FD91O4spBcBPxg6zCRJmoBR/2E/C/h6kk938/8eeN/ONkpyKXAccECSu4H3AH8CXJbkDOBO4ORu9SuBVwC3Aj8CvGBOkiZs1CuaL06yFnhZVzqpqm4aYbuV21n08gXWLeBNo/QjSRqPkQ8BdSGw0yCQJO25dvnW2ZKkxy9DQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqdndIzd2W5DnAp4ZKzwL+ENgPeD2wtau/q6qunHB7kjTVJh4KVfUdYDlAkr2BTcDlDIbfPLuqPjDpniRJA30fPno5cFtV3dlzH5Ik+g+FVwOXDs2fmWRDkguS7L/QBklWJVmbZO3WrVsXWkWStJt6C4UkTwReCXy6K50HHM7g0NJm4KyFtquq1VU1W1WzMzMzE+lVkqZFn3sKJwDXVdUWgKraUlUPVdXDwPnAMT32JklTqc9QWMnQoaMkBw0texWwceIdSdKUm/jZRwBJngz8JvCGofL/TLIcKOCOecskSRPQSyhU1Q+BfzGvdkofvUiSHtH32UeSpEXEUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSml7GUwBIcgfwAPAQsK2qZpM8DfgUsJTBQDsnV9X3++pRkqZN33sKv15Vy6tqtpt/B3B1VS0Dru7mJUkT0ncozLcCuKibvgg4scdeJGnq9BkKBXwpybokq7ragVW1uZv+HnBgP61J0nTq7TsF4NeqalOSpwNXJblleGFVVZKav1EXIKsAlixZMplOJWlK9LanUFWbuud7gMuBY4AtSQ4C6J7vWWC71VU1W1WzMzMzk2xZkh73egmFJE9O8tS5aeDfABuBK4DTutVOAz7fR3+SNK36Onx0IHB5krkePlFVf5nkWuCyJGcAdwIn99SfJE2lXkKhqm4HXrBA/V7g5ZPvSJIEi++UVElSjwwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJzcRDIcmhSb6c5KYkNyZ5S1d/b5JNSdZ3j1dMujdJmnZ9jLy2Dfj9qrquG6d5XZKrumVnV9UHeuhJkkQPoVBVm4HN3fQDSW4GDp50H5Kkn9brdwpJlgJHAd/sSmcm2ZDkgiT7b2ebVUnWJlm7devWCXUqSdOht1BI8hRgDfDWqrofOA84HFjOYE/irIW2q6rVVTVbVbMzMzMT61eSpkEvoZBkHwaBcElVfRagqrZU1UNV9TBwPnBMH71J0jTr4+yjAB8Fbq6qPx2qHzS02quAjZPuTZKmXR9nH70EOAW4Icn6rvYuYGWS5UABdwBv6KE3SZpqfZx99DUgCyy6ctK9SJIezSuaJUmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKlZdKGQ5Pgk30lya5J39N2PJE2TRRUKSfYGPgycABzBYIjOI/rtSpKmx6IKBeAY4Naqur2q/gn4JLCi554kaWpMfIzmnTgYuGto/m7gXw2vkGQVsKqb/cck35lQb9PgAOAf+m5iMcgHTuu7BT2an80571loiPtd9sztLVhsobBTVbUaWN13H49HSdZW1WzffUjz+dmcnMV2+GgTcOjQ/CFdTZI0AYstFK4FliU5LMkTgVcDV/TckyRNjUV1+KiqtiU5E/gisDdwQVXd2HNb08TDclqs/GxOSKqq7x4kSYvEYjt8JEnqkaEgSWoMhT1QkhOTVJJf7uaPS/Lnffe1kCRfSeKphFOs+6yeNTT/9iTvnXAPfg5HZCjsmVYCX+uexybJojoRQXusB4GTkhywOxv7OZws/7D3MEmeAvwa8OvAF4D3dIv2TfIXwL8Evgz8x6p6OMk/AucAvw38GFhRVVuSLAUuYHCl6FbgtVX190kuBH4CHAX8dZKnddsdBTwdeB1wKvCrwDer6vSur/OAXwF+AfhMVc31JW1jcPbQ24B3Dy/wc7j4uKew51kB/GVV/S1wb5Kju/oxwJsZ3EjwcOCkrv5k4BtV9QLgGuD1Xf1c4KKqOhK4BPjg0HscAry4qn6vm9+fwS/f2xhcN3I28Fzg+UmWd+u8u7vi9EjgXyc58jH8mbXn+zDwmiS/OK/u53CRMRT2PCsZ3CiQ7nnuENK3uhsJPgRcymBvAuCfgLnvG9YBS7vpXwU+0U1/fGh9gE93rzPnCzU4d/kGYEtV3VBVDwM3Dr3eyUmuA65n8Ivq3W3VVNX9wMXAf5q3yM/hIuPhoz1Itwv9Mgb/MyoGF/gV8Bfd87C5+X+uRy5GeYjR/s5/OG/+we754aHpufknJDkMeDvwK1X1/W7X/+dHeB9Nl/8FXAd8bMT1/Rz2wD2FPcu/Az5eVc+sqqVVdSjwd8CxwDHd7UH2An6HwRfRO/I3DG4jAvAa4K9+hr72ZfAL/IMkBzIYD0N6lKq6D7gMOGOo7OdwkTEU9iwrgcvn1dZ09WuBDwE3MwiK+evN92bgtUk2AKcAb9ndpqrq2wx2129hcCjgr3f3tfS4dxaDL5Xn+DlcZLzNhSSpcU9BktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAUJSPJLST6Z5LYk65JcmeTZSTb21M+JSY4Ymj89yTP66EXTxVDQ1EsSBhf7faWqDq+qo4F3Agf22NaJPPq+PacDhoLGzlCQBrch/+eq+shcobs69q65+SRLk/xVkuu6x4u7+kFJrkmyPsnGJMcm2TvJhd38DUnetr03TvL6JNcm+XaSNUme1L32K4H3d6/7X4BZ4JJu/hfG9QcheUM8CZ7H4A6yO3IP8JtV9ZMkyxjciXYW+A/AF6vqfUn2Bp4ELAcOrqrnASTZbwev+9mqOr9b778DZ1TVuUmuAP68qj7TLTsBeHtVrd39H1PaOUNBGs0+wIe6+/Y/BDy7q18LXJBkH+BzVbU+ye3As5Kcy+AOtl/awes+rwuD/YCnAF8c208gjcDDR9LgfvxH72SdtwFbgBcw2EN4IkBVXQO8FNgEXJjk1Kr6frfeV4A3Av97B697IXBmVT0f+CO81bN6ZihI8P+An0uyaq7Qjdh16NA6vwhs7gZ1OYXBWBYkeSaDAV/OZ/CP/wu7sYj3qqo1wH8FXriD934qsLnb03jNUP2Bbtn25qWxMBQ09bpBiF4F/EZ3SuqNwP8Avje02p8BpyX5NvDLPDIAzHHAt5Ncz2Aci3OAg4GvJFkP/B8GZzJtz38DvsngNs+3DNU/CfxBkuuTHM5gj+IjftGscfPW2ZKkxj0FSVLj2UfSBCT5MPCSeeVzqmrU8YqlifDwkSSp8fCRJKkxFCRJjaEgSWoMBUlS8/8Bxehmn5EaHpEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MjsODTHK9Hl"
      },
      "source": [
        "df['Class_att'] = df['Class_att'].astype('category')\n",
        "encode_map = {\n",
        "    'Abnormal': 1,\n",
        "    'Normal': 0\n",
        "}\n",
        "\n",
        "df['Class_att'].replace(encode_map, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjJZ02ORK_lv"
      },
      "source": [
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:, -1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxcfmxhOLB1t"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-aBJGJ5LCzs"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tOTnWrM0PL"
      },
      "source": [
        "## Define Custom Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr9BTe2YLGXK"
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRsYdQzGLHUr"
      },
      "source": [
        "## train data\n",
        "class trainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "train_data = trainData(torch.FloatTensor(X_train), \n",
        "                       torch.FloatTensor(y_train))\n",
        "## test data    \n",
        "class testData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "    \n",
        "\n",
        "test_data = testData(torch.FloatTensor(X_test))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBKD9y9QLKI8"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBvCk4XgMvnL"
      },
      "source": [
        "## Define Neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5qEajHgMOIt"
      },
      "source": [
        "Once we’ve defined all these layers, it’s time to use them. In the forward() function, we take variable inputs as our input. We pass this input through the different layers we initialized.\n",
        "\n",
        "The first line of the forward() functions takes the input, passes it through our first linear layer and then applies the ReLU activation on it. Then we apply BatchNorm on the output. Look at the following code to understand it better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7UY7l_ELN4L"
      },
      "source": [
        "class binaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(binaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(12, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtIn5w8AMMC1"
      },
      "source": [
        "Once, we’ve defined our architecture, we check if our GPU is active. The amazing thing about PyTorch is that it’s super easy to use the GPU.\n",
        "The variable device will either say cuda:0 if we have the GPU. If not, it’ll say cpu . You can follow along this tutorial even if you do not have a GPU without any change in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSY9ILZlL7nz",
        "outputId": "1fd91046-6b3f-4a8a-c218-a7c3fe365654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBFld7NMG6j"
      },
      "source": [
        "Note that we did not use the Sigmoid activation in our final layer during training. That’s because, we use the nn.BCEWithLogitsLoss() loss function which automatically applies the the Sigmoid activation. We however, need to use Sigmoid manually during inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5JyXyZcMYxb"
      },
      "source": [
        "Next, we need to initialize our model. After initializing it, we move it to device . Now, this device is a GPU if you have one or it’s CPU if you don’t. The network we’ve used is fairly small. So, it will not take a lot of time to train on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3oFa9dAL7xV",
        "outputId": "2c2b23f7-26cb-449b-b48d-f2f1bf6a3f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGISxMIxMr-H"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihGl7H0ZM8bh"
      },
      "source": [
        "Before we start the actual training, let’s define a function to calculate accuracy.\n",
        "\n",
        "In the function below, we take the predicted and actual output as the input. The predicted value(a probability) is rounded off to convert it into either a 0 or a 1.\n",
        "\n",
        "Once that is done, we simply compare the number of 1/0 we predicted to the number of 1/0 actually present and calculate the accuracy.\n",
        "\n",
        "Note that the inputs y_pred and y_test are for a batch. Our batch_size was 64. So, this accuracy is being calculated for 64 predictions(tensors) at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6blbm9EwMERh"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUpK6ivKNwbp"
      },
      "source": [
        "You can see we’ve put a model.train() at the before the loop. model.train() tells PyTorch that you’re in training mode.\n",
        "\n",
        "Well, why do we need to do that? If you’re using layers such as Dropout or BatchNorm which behave differently during training and evaluation, you need to tell PyTorch to act accordingly. While the default mode in PyTorch is the train, so, you don’t explicitly have to write that. But it’s good practice.\n",
        "\n",
        "Similarly, we’ll call model.eval() when we test our model. We’ll see that below.\n",
        "\n",
        "Back to training; we start a for-loop. At the top of this for-loop, we initialize our loss and accuracy per epoch to 0. After every epoch, we’ll print out the loss/accuracy and reset it back to 0.\n",
        "\n",
        "Then we have another for-loop. This for-loop is used to get our data in batches from the train_loader.\n",
        "\n",
        "We do optimizer.zero_grad() before we make any predictions. Since the backward() function accumulates gradients, we need to set it to 0 manually per mini-batch.\n",
        "\n",
        "From our defined model, we then obtain a prediction, get the loss(and accuracy) for that mini-batch, perform backpropagation using loss.backward() and optimizer.step() . Finally, we add all the mini-batch losses (and accuracies) to obtain the average loss (and accuracy) for that epoch.\n",
        "\n",
        "This loss and accuracy is printed out in the outer for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZGxYSnAMEUD",
        "outputId": "246a41ba-c0e4-4323-9690-5411b1da7be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.train()\n",
        "for e in range(1, EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        #Get the input, data is a list of [inputs, labels], transfer to GPU\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        #1. Initialise gradients \n",
        "        optimizer.zero_grad()\n",
        "        #2 Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        #3 Backward\n",
        "        loss.backward()\n",
        "        #4 Copute the loss and update the weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item() # Loss each batch\n",
        "        epoch_acc += acc.item() # acc each batch\n",
        "        \n",
        "    # Loss and acc per each Eposhs\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 001: | Loss: 0.73404 | Acc: 49.500\n",
            "Epoch 002: | Loss: 0.62179 | Acc: 65.500\n",
            "Epoch 003: | Loss: 0.56483 | Acc: 74.750\n",
            "Epoch 004: | Loss: 0.51332 | Acc: 80.000\n",
            "Epoch 005: | Loss: 0.50564 | Acc: 79.250\n",
            "Epoch 006: | Loss: 0.48180 | Acc: 83.250\n",
            "Epoch 007: | Loss: 0.45540 | Acc: 82.750\n",
            "Epoch 008: | Loss: 0.44128 | Acc: 81.000\n",
            "Epoch 009: | Loss: 0.42676 | Acc: 85.500\n",
            "Epoch 010: | Loss: 0.40659 | Acc: 86.500\n",
            "Epoch 011: | Loss: 0.40709 | Acc: 83.250\n",
            "Epoch 012: | Loss: 0.40034 | Acc: 86.000\n",
            "Epoch 013: | Loss: 0.36650 | Acc: 86.250\n",
            "Epoch 014: | Loss: 0.35865 | Acc: 88.000\n",
            "Epoch 015: | Loss: 0.35159 | Acc: 88.250\n",
            "Epoch 016: | Loss: 0.34127 | Acc: 88.000\n",
            "Epoch 017: | Loss: 0.32761 | Acc: 88.500\n",
            "Epoch 018: | Loss: 0.31594 | Acc: 91.250\n",
            "Epoch 019: | Loss: 0.30948 | Acc: 90.000\n",
            "Epoch 020: | Loss: 0.30402 | Acc: 90.250\n",
            "Epoch 021: | Loss: 0.28991 | Acc: 89.250\n",
            "Epoch 022: | Loss: 0.27839 | Acc: 92.000\n",
            "Epoch 023: | Loss: 0.25964 | Acc: 91.750\n",
            "Epoch 024: | Loss: 0.24015 | Acc: 92.500\n",
            "Epoch 025: | Loss: 0.24978 | Acc: 92.000\n",
            "Epoch 026: | Loss: 0.23495 | Acc: 92.000\n",
            "Epoch 027: | Loss: 0.22540 | Acc: 93.750\n",
            "Epoch 028: | Loss: 0.21602 | Acc: 95.750\n",
            "Epoch 029: | Loss: 0.20996 | Acc: 94.500\n",
            "Epoch 030: | Loss: 0.19583 | Acc: 94.500\n",
            "Epoch 031: | Loss: 0.17723 | Acc: 96.750\n",
            "Epoch 032: | Loss: 0.16939 | Acc: 96.750\n",
            "Epoch 033: | Loss: 0.16099 | Acc: 95.250\n",
            "Epoch 034: | Loss: 0.14242 | Acc: 97.250\n",
            "Epoch 035: | Loss: 0.12721 | Acc: 98.750\n",
            "Epoch 036: | Loss: 0.13416 | Acc: 96.500\n",
            "Epoch 037: | Loss: 0.13513 | Acc: 96.750\n",
            "Epoch 038: | Loss: 0.13897 | Acc: 95.750\n",
            "Epoch 039: | Loss: 0.11559 | Acc: 98.000\n",
            "Epoch 040: | Loss: 0.10481 | Acc: 97.750\n",
            "Epoch 041: | Loss: 0.10964 | Acc: 98.500\n",
            "Epoch 042: | Loss: 0.11238 | Acc: 98.250\n",
            "Epoch 043: | Loss: 0.10858 | Acc: 98.000\n",
            "Epoch 044: | Loss: 0.08821 | Acc: 99.500\n",
            "Epoch 045: | Loss: 0.08937 | Acc: 99.000\n",
            "Epoch 046: | Loss: 0.09734 | Acc: 97.500\n",
            "Epoch 047: | Loss: 0.08090 | Acc: 99.500\n",
            "Epoch 048: | Loss: 0.07229 | Acc: 98.250\n",
            "Epoch 049: | Loss: 0.07549 | Acc: 99.250\n",
            "Epoch 050: | Loss: 0.07319 | Acc: 99.250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0bN3LynOnCw"
      },
      "source": [
        "After training is done, we need to test how our model fared. Note that we’ve used model.eval() before we run our testing code. To tell PyTorch that we do not want to perform back-propagation during inference, we use torch.no_grad() which reduces memory usage and speeds up computation.\n",
        "\n",
        "We start by defining a list that will hold our predictions. Then we loop through our batches using the test_loader. For each batch —\n",
        "\n",
        "- We make the predictions using our trained model.\n",
        "- Round off the probabilities to 1 or 0.\n",
        "- Move the batch to the GPU from the CPU.\n",
        "- Convert the tensor to a numpy object and append it to our list.\n",
        "- Flatten out the list so that we can use it as an input to confusion_matrix and classification_report ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-xHPNZaMEWn"
      },
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch) # make predictions\n",
        "        y_test_pred = torch.sigmoid(y_test_pred) # convert to propability 0-1\n",
        "        y_pred_tag = torch.round(y_test_pred) # round to 0,1\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())# move batch to GPU, convert to numpy object and append to list\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list] #Flatten out the list for confusion matrix and classification report"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHh77-vNoLH",
        "outputId": "aa62f375-2bc4-4c70-8f79-e262531c3a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred_list))\n",
        "print('balanced_accuracy_score: ',balanced_accuracy_score(y_test, y_pred_list))\n",
        "print('matthews_corrcoef: ',matthews_corrcoef(y_test, y_pred_list))\n",
        "print(classification_report(y_test, y_pred_list))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12  6]\n",
            " [ 7 37]]\n",
            "balanced_accuracy_score:  0.7537878787878788\n",
            "matthews_corrcoef:  0.49974957565417644\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.67      0.65        18\n",
            "           1       0.86      0.84      0.85        44\n",
            "\n",
            "    accuracy                           0.79        62\n",
            "   macro avg       0.75      0.75      0.75        62\n",
            "weighted avg       0.79      0.79      0.79        62\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1GMAiaxvEVk",
        "outputId": "6279da30-cbb6-4fe1-8136-a3771753a2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "model=XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('Balanced accuracy score:',balanced_accuracy_score(y_test,y_pred))\n",
        "print('Matthews_corrcoef accuracy score:',matthews_corrcoef(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Balanced accuracy score: 0.7424242424242424\n",
            "Matthews_corrcoef accuracy score: 0.47079190906919977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63        18\n",
            "           1       0.86      0.82      0.84        44\n",
            "\n",
            "    accuracy                           0.77        62\n",
            "   macro avg       0.73      0.74      0.73        62\n",
            "weighted avg       0.78      0.77      0.78        62\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvO-08OWlRmT"
      },
      "source": [
        "# Churn prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR7CC_QOwJLQ"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns ; sns.set()\n",
        "\n",
        "\n",
        "#Load sklearn\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score,cross_validate,cross_val_predict\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.metrics import log_loss ,balanced_accuracy_score,matthews_corrcoef\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import make_scorer,roc_curve, auc,precision_recall_curve\n",
        "from sklearn.metrics import f1_score,balanced_accuracy_score,average_precision_score \n",
        "\n",
        "#ML normal ML\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#ML ensembles\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "#Special\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS-4tYHnwdt4"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4-7xUJ7NpJg",
        "outputId": "9ea4ca0a-5a83-407e-e6ef-e5913579f2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "path='https://raw.githubusercontent.com/rstudio/keras-customer-churn/master/data/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "df=pd.read_csv(path)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7043, 21)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customerID</th>\n",
              "      <th>gender</th>\n",
              "      <th>SeniorCitizen</th>\n",
              "      <th>Partner</th>\n",
              "      <th>Dependents</th>\n",
              "      <th>tenure</th>\n",
              "      <th>PhoneService</th>\n",
              "      <th>MultipleLines</th>\n",
              "      <th>InternetService</th>\n",
              "      <th>OnlineSecurity</th>\n",
              "      <th>OnlineBackup</th>\n",
              "      <th>DeviceProtection</th>\n",
              "      <th>TechSupport</th>\n",
              "      <th>StreamingTV</th>\n",
              "      <th>StreamingMovies</th>\n",
              "      <th>Contract</th>\n",
              "      <th>PaperlessBilling</th>\n",
              "      <th>PaymentMethod</th>\n",
              "      <th>MonthlyCharges</th>\n",
              "      <th>TotalCharges</th>\n",
              "      <th>Churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7590-VHVEG</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>1</td>\n",
              "      <td>No</td>\n",
              "      <td>No phone service</td>\n",
              "      <td>DSL</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Electronic check</td>\n",
              "      <td>29.85</td>\n",
              "      <td>29.85</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5575-GNVDE</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>34</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>One year</td>\n",
              "      <td>No</td>\n",
              "      <td>Mailed check</td>\n",
              "      <td>56.95</td>\n",
              "      <td>1889.5</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3668-QPYBK</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>2</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Mailed check</td>\n",
              "      <td>53.85</td>\n",
              "      <td>108.15</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7795-CFOCW</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>45</td>\n",
              "      <td>No</td>\n",
              "      <td>No phone service</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>One year</td>\n",
              "      <td>No</td>\n",
              "      <td>Bank transfer (automatic)</td>\n",
              "      <td>42.30</td>\n",
              "      <td>1840.75</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9237-HQITU</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>2</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Fiber optic</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Electronic check</td>\n",
              "      <td>70.70</td>\n",
              "      <td>151.65</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   customerID  gender  SeniorCitizen  ... MonthlyCharges TotalCharges  Churn\n",
              "0  7590-VHVEG  Female              0  ...          29.85        29.85     No\n",
              "1  5575-GNVDE    Male              0  ...          56.95       1889.5     No\n",
              "2  3668-QPYBK    Male              0  ...          53.85       108.15    Yes\n",
              "3  7795-CFOCW    Male              0  ...          42.30      1840.75     No\n",
              "4  9237-HQITU  Female              0  ...          70.70       151.65    Yes\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upjFKnOgvSFQ",
        "outputId": "99c3dbc5-355e-4d69-8bc3-fed545cea23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "df['Churn'].value_counts(normalize=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.73463\n",
              "1    0.26537\n",
              "Name: Churn, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ALkpC1_vSZP"
      },
      "source": [
        "df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan).astype(float)\n",
        "df = df.dropna()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-xf0dgWv2Mf",
        "outputId": "9267f6a6-debe-4ea0-f054-dee4da3170a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "#One hot encoding\n",
        "dummy_cols = []\n",
        "# column with value\n",
        "sample_set = df[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']].copy(deep=True)\n",
        "# for other column with category, only one hot with column have nuique < 5\n",
        "for col in list(df.columns):\n",
        "    if col not in ['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn'] and df[col].nunique() < 5:\n",
        "        dummy_vars = pd.get_dummies(df[col])\n",
        "        dummy_vars.columns = [col+str(x) for x in dummy_vars.columns]        \n",
        "        sample_set = pd.concat([sample_set, dummy_vars], axis=1)\n",
        "sample_set.head(10)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tenure</th>\n",
              "      <th>MonthlyCharges</th>\n",
              "      <th>TotalCharges</th>\n",
              "      <th>Churn</th>\n",
              "      <th>genderFemale</th>\n",
              "      <th>genderMale</th>\n",
              "      <th>SeniorCitizen0</th>\n",
              "      <th>SeniorCitizen1</th>\n",
              "      <th>PartnerNo</th>\n",
              "      <th>PartnerYes</th>\n",
              "      <th>DependentsNo</th>\n",
              "      <th>DependentsYes</th>\n",
              "      <th>PhoneServiceNo</th>\n",
              "      <th>PhoneServiceYes</th>\n",
              "      <th>MultipleLinesNo</th>\n",
              "      <th>MultipleLinesNo phone service</th>\n",
              "      <th>MultipleLinesYes</th>\n",
              "      <th>InternetServiceDSL</th>\n",
              "      <th>InternetServiceFiber optic</th>\n",
              "      <th>InternetServiceNo</th>\n",
              "      <th>OnlineSecurityNo</th>\n",
              "      <th>OnlineSecurityNo internet service</th>\n",
              "      <th>OnlineSecurityYes</th>\n",
              "      <th>OnlineBackupNo</th>\n",
              "      <th>OnlineBackupNo internet service</th>\n",
              "      <th>OnlineBackupYes</th>\n",
              "      <th>DeviceProtectionNo</th>\n",
              "      <th>DeviceProtectionNo internet service</th>\n",
              "      <th>DeviceProtectionYes</th>\n",
              "      <th>TechSupportNo</th>\n",
              "      <th>TechSupportNo internet service</th>\n",
              "      <th>TechSupportYes</th>\n",
              "      <th>StreamingTVNo</th>\n",
              "      <th>StreamingTVNo internet service</th>\n",
              "      <th>StreamingTVYes</th>\n",
              "      <th>StreamingMoviesNo</th>\n",
              "      <th>StreamingMoviesNo internet service</th>\n",
              "      <th>StreamingMoviesYes</th>\n",
              "      <th>ContractMonth-to-month</th>\n",
              "      <th>ContractOne year</th>\n",
              "      <th>ContractTwo year</th>\n",
              "      <th>PaperlessBillingNo</th>\n",
              "      <th>PaperlessBillingYes</th>\n",
              "      <th>PaymentMethodBank transfer (automatic)</th>\n",
              "      <th>PaymentMethodCredit card (automatic)</th>\n",
              "      <th>PaymentMethodElectronic check</th>\n",
              "      <th>PaymentMethodMailed check</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>29.85</td>\n",
              "      <td>29.85</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34</td>\n",
              "      <td>56.95</td>\n",
              "      <td>1889.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>53.85</td>\n",
              "      <td>108.15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>42.30</td>\n",
              "      <td>1840.75</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>70.70</td>\n",
              "      <td>151.65</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>99.65</td>\n",
              "      <td>820.50</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>22</td>\n",
              "      <td>89.10</td>\n",
              "      <td>1949.40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>29.75</td>\n",
              "      <td>301.90</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>28</td>\n",
              "      <td>104.80</td>\n",
              "      <td>3046.05</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>62</td>\n",
              "      <td>56.15</td>\n",
              "      <td>3487.95</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tenure  ...  PaymentMethodMailed check\n",
              "0       1  ...                          0\n",
              "1      34  ...                          1\n",
              "2       2  ...                          1\n",
              "3      45  ...                          0\n",
              "4       2  ...                          0\n",
              "5       8  ...                          0\n",
              "6      22  ...                          0\n",
              "7      10  ...                          1\n",
              "8      28  ...                          0\n",
              "9      62  ...                          0\n",
              "\n",
              "[10 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWqdZzsev7Tm",
        "outputId": "6100bf42-d177-43d8-ec46-5f63e7314c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X = sample_set.drop(columns='Churn')\n",
        "y= sample_set['Churn']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "X_train,  X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, \n",
        "                                                     random_state = 42,stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7032, 46)\n",
            "(7032,)\n",
            "(5625, 46) (5625,)\n",
            "(1407, 46) (1407,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ3sF1UZv-g-",
        "outputId": "492a3b5c-01ba-46a9-b8c5-8a61cf4b3141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model=XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('Balanced accuracy score:',balanced_accuracy_score(y_test,y_pred))\n",
        "print('Matthews_corrcoef accuracy score:',matthews_corrcoef(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Balanced accuracy score: 0.6944287703640816\n",
            "Matthews_corrcoef accuracy score: 0.4285486325890265\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86      1033\n",
            "           1       0.64      0.49      0.55       374\n",
            "\n",
            "    accuracy                           0.79      1407\n",
            "   macro avg       0.74      0.69      0.71      1407\n",
            "weighted avg       0.78      0.79      0.78      1407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW2Qd65kwaym"
      },
      "source": [
        "## Pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A59PA1RzwA4f"
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb3mBh5Xw7qt"
      },
      "source": [
        "## train data\n",
        "class trainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "## test data    \n",
        "class testData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBaEf9rs48U7"
      },
      "source": [
        "train_data = trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
        "test_data = testData(torch.FloatTensor(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj7ss93q5Hl7",
        "outputId": "f6203411-00cd-4ce2-f8db-3302b4b85143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db9qkjhdw-nk"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrAzDsSLw_13"
      },
      "source": [
        "class binaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(binaryClassification, self).__init__()\n",
        "        # Number of input features is 12.\n",
        "        self.layer_1 = nn.Linear(X.shape[1], 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIo7VY3rxBFM",
        "outputId": "92a26ff3-cb90-47cc-8250-298411212835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtIw68fKxB07",
        "outputId": "53f4bf00-f6a6-4334-bdb1-080bd213462e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=46, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmKRMXA3xC7c"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSbk6EgxEZJ",
        "outputId": "227930a6-1608-4393-ed81-58f5f40bbabc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.train()\n",
        "for e in range(1, EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        #Get the input, data is a list of [inputs, labels], transfer to GPU\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        #1. Initialise gradients \n",
        "        optimizer.zero_grad()\n",
        "        #2 Forward pass\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        #3 Backward\n",
        "        loss.backward()\n",
        "        #4 Copute the loss and update the weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item() # Loss each batch\n",
        "        epoch_acc += acc.item() # acc each batch\n",
        "        \n",
        "    # Loss and acc per each Eposhs\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 001: | Loss: 0.53652 | Acc: 75.205\n",
            "Epoch 002: | Loss: 0.45769 | Acc: 79.307\n",
            "Epoch 003: | Loss: 0.42025 | Acc: 80.341\n",
            "Epoch 004: | Loss: 0.40758 | Acc: 81.034\n",
            "Epoch 005: | Loss: 0.40132 | Acc: 81.045\n",
            "Epoch 006: | Loss: 0.39310 | Acc: 81.159\n",
            "Epoch 007: | Loss: 0.39445 | Acc: 81.375\n",
            "Epoch 008: | Loss: 0.39100 | Acc: 81.727\n",
            "Epoch 009: | Loss: 0.38122 | Acc: 82.398\n",
            "Epoch 010: | Loss: 0.38077 | Acc: 82.080\n",
            "Epoch 011: | Loss: 0.37267 | Acc: 82.466\n",
            "Epoch 012: | Loss: 0.37510 | Acc: 82.114\n",
            "Epoch 013: | Loss: 0.36919 | Acc: 82.568\n",
            "Epoch 014: | Loss: 0.36843 | Acc: 82.443\n",
            "Epoch 015: | Loss: 0.36131 | Acc: 83.239\n",
            "Epoch 016: | Loss: 0.35704 | Acc: 83.284\n",
            "Epoch 017: | Loss: 0.35807 | Acc: 83.091\n",
            "Epoch 018: | Loss: 0.35137 | Acc: 83.420\n",
            "Epoch 019: | Loss: 0.35336 | Acc: 83.432\n",
            "Epoch 020: | Loss: 0.34553 | Acc: 83.727\n",
            "Epoch 021: | Loss: 0.33794 | Acc: 84.420\n",
            "Epoch 022: | Loss: 0.34286 | Acc: 84.216\n",
            "Epoch 023: | Loss: 0.33959 | Acc: 84.500\n",
            "Epoch 024: | Loss: 0.33001 | Acc: 84.750\n",
            "Epoch 025: | Loss: 0.32645 | Acc: 84.898\n",
            "Epoch 026: | Loss: 0.32398 | Acc: 85.239\n",
            "Epoch 027: | Loss: 0.32881 | Acc: 84.682\n",
            "Epoch 028: | Loss: 0.31983 | Acc: 85.818\n",
            "Epoch 029: | Loss: 0.32362 | Acc: 84.761\n",
            "Epoch 030: | Loss: 0.31933 | Acc: 85.511\n",
            "Epoch 031: | Loss: 0.31400 | Acc: 85.636\n",
            "Epoch 032: | Loss: 0.31115 | Acc: 85.398\n",
            "Epoch 033: | Loss: 0.31174 | Acc: 85.807\n",
            "Epoch 034: | Loss: 0.30321 | Acc: 86.091\n",
            "Epoch 035: | Loss: 0.30104 | Acc: 86.295\n",
            "Epoch 036: | Loss: 0.30104 | Acc: 86.784\n",
            "Epoch 037: | Loss: 0.29706 | Acc: 86.273\n",
            "Epoch 038: | Loss: 0.30097 | Acc: 86.455\n",
            "Epoch 039: | Loss: 0.29094 | Acc: 86.864\n",
            "Epoch 040: | Loss: 0.28620 | Acc: 86.307\n",
            "Epoch 041: | Loss: 0.28658 | Acc: 87.034\n",
            "Epoch 042: | Loss: 0.28096 | Acc: 87.318\n",
            "Epoch 043: | Loss: 0.28593 | Acc: 87.125\n",
            "Epoch 044: | Loss: 0.27860 | Acc: 87.443\n",
            "Epoch 045: | Loss: 0.27651 | Acc: 87.739\n",
            "Epoch 046: | Loss: 0.27448 | Acc: 87.523\n",
            "Epoch 047: | Loss: 0.27788 | Acc: 87.466\n",
            "Epoch 048: | Loss: 0.27391 | Acc: 87.193\n",
            "Epoch 049: | Loss: 0.27142 | Acc: 87.818\n",
            "Epoch 050: | Loss: 0.26985 | Acc: 87.864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHHpQBC4xFR6"
      },
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch) # make predictions\n",
        "        y_test_pred = torch.sigmoid(y_test_pred) # convert to propability 0-1\n",
        "        y_pred_tag = torch.round(y_test_pred) # round to 0,1\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())# move batch to GPU, convert to numpy object and append to list\n",
        "\n",
        "y_pred = [a.squeeze().tolist() for a in y_pred_list] #Flatten out the list for confusion matrix and classification report"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "871fXYHdxIUI",
        "outputId": "a80922f8-ce99-4d83-fa6d-f39753a07f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print('balanced_accuracy_score: ',balanced_accuracy_score(y_test, y_pred))\n",
        "print('matthews_corrcoef: ',matthews_corrcoef(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "balanced_accuracy_score:  0.6943834737098218\n",
            "matthews_corrcoef:  0.40448462897704845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85      1033\n",
            "           1       0.59      0.52      0.55       374\n",
            "\n",
            "    accuracy                           0.78      1407\n",
            "   macro avg       0.71      0.69      0.70      1407\n",
            "weighted avg       0.77      0.78      0.77      1407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL9FBNkl50H5"
      },
      "source": [
        "## Pytorch model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1WO4ssn5Tck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}