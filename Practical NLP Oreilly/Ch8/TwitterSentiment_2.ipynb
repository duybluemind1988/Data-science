{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "TwitterSentiment_2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bazrz-9Ld9_p",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we combine the concepts from previous notebook. We pre-process tweets using our preprocessing pipeline, build embeddings and then classify them using a logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOTY6C7kgQMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "94bee9b7-6c07-4370-ade1-b48e3f1474fd"
      },
      "source": [
        "!!pip install demoji"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting demoji',\n",
              " '  Downloading https://files.pythonhosted.org/packages/da/0b/d008f26ebbfd86d21117267e627f2f7359c76e5ecbeba08d8f631f4092c4/demoji-0.2.1-py2.py3-none-any.whl',\n",
              " 'Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji) (49.2.0)',\n",
              " 'Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)',\n",
              " 'Collecting colorama',\n",
              " '  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl',\n",
              " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.6.20)',\n",
              " 'Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)',\n",
              " 'Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)',\n",
              " 'Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)',\n",
              " 'Installing collected packages: colorama, demoji',\n",
              " 'Successfully installed colorama-0.4.3 demoji-0.2.1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DszxuG0qgJdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d1a73a21-7837-4467-a4af-8612f0e5bc81"
      },
      "source": [
        "'''\n",
        "Contains helper funtions for preprocessing of twitter documents before getting their corresponding vectors\n",
        "from word2vec_twitter model\n",
        "Author: Anuj Gupta\n",
        "'''\n",
        "\n",
        "'''\n",
        "functionality:\n",
        "Discard non-english tweets\n",
        "Discard Replies\n",
        "Discard RTs\n",
        "For now this is handle while fetch tweet text from mongo documents\n",
        "process constantbrands mentions\n",
        "process any other mentions\n",
        "process constant brand name if any \n",
        "process hanstags\n",
        "process URLs\n",
        "process websites\n",
        "process process_EmailIds\n",
        "process \n",
        "process alphanums\n",
        "'''\n",
        "\n",
        "import re\n",
        "import string\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "#gobal\n",
        "PunctChars = r'''[`'â€œ\".?!,:;]'''\n",
        "Punct = '%s+' % PunctChars\n",
        "Entity = '&(amp|lt|gt|quot);'\n",
        "printable = set(string.printable)\n",
        "\n",
        "# helper functoins\n",
        "def regex_or(*items):\n",
        "\tr = '|'.join(items)\n",
        "\tr = '(' + r + ')'\n",
        "\treturn r\n",
        "\n",
        "def pos_lookahead(r):\n",
        "\treturn '(?=' + r + ')'\n",
        "\n",
        "def neg_lookahead(r):\n",
        "\treturn '(?!' + r + ')'\n",
        "\n",
        "def optional(r):\n",
        "\treturn '(%s)?' % r\n",
        "\n",
        "def trim(transient_tweet_text):\n",
        "\t''' \n",
        "\ttrim leading and trailing spaces in the tweet text\n",
        "\t'''\n",
        "\treturn transient_tweet_text.strip()\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def to_LowerCase(transient_tweet_text):\n",
        "\t'''\n",
        "\tConvert tweet text to lower to lower case alphabets\n",
        "\t'''\n",
        "\ttransient_tweet_text = transient_tweet_text.lower()\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_unicode(transient_tweet_text):\n",
        "    '''\n",
        "    Strip all unicode characters from a tweet\n",
        "    '''\n",
        "    tweet = ''.join(i for i in transient_tweet_text if ord(i)<128)\n",
        "    return tweet \n",
        "\n",
        "def process_URLs(transient_tweet_text):\n",
        "\t'''\n",
        "\treplace all URLs in the tweet text\n",
        "\t'''\n",
        "\tUrlStart1 = regex_or('https?://', r'www\\.',r'bit.ly/')\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tUrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + pos_lookahead(r'[/ \\W\\b]')\n",
        "\tUrlBody = r'[^ \\t\\r\\n<>]*?'  # * not + for case of:  \"go to bla.com.\" -- don't want period\n",
        "\tUrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n",
        "\tUrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n",
        "\tUrl = \t(optional(r'\\b') + \n",
        "    \t\tregex_or(UrlStart1, UrlStart2) + \n",
        "    \t\tUrlBody + \n",
        "    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n",
        "\n",
        "\tUrl_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(Url_RE, \" constanturl \", transient_tweet_text)\n",
        "\n",
        "\t# fix to handle unicodes in URL\n",
        "\tURL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*'\n",
        "\ttransient_tweet_text = re.sub(URL_regex2, \" constanturl \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Websites(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify website mentioned if any \n",
        "\t'''\n",
        "\tCommonTLDs = regex_or('com','co\\\\.uk','org','net','info','ca','biz','info','edu','in','au')\n",
        "\tsep = r'[.]'\n",
        "\twebsite_regex = r'(?<!#)?(\\b)?[a-zA-Z0-9.]+' + sep + CommonTLDs\n",
        "\twebsite_RE = re.compile(\"(%s)\" % website_regex, re.U|re.I)\n",
        "\ttransient_tweet_text = re.sub(website_RE, ' constantwebsite ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_EmailIds(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify email mentioned if any\n",
        "\t'''\n",
        "\temail_regex = r'(\\b)?[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(\\b)?'\n",
        "\ttransient_tweet_text = re.sub(email_regex, ' constantemailid ', transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Mentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify mentions if any\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", \" constantnonbrandmention \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "def process_HashTags(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all Hashtags from a tweet\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"#(\\w+)\\b\", ' constanthashtag ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Dates(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify date and convert it to constant\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'(\\d+/\\d+/\\d+)', \" constantdate \" , transient_tweet_text)\n",
        "\t#transient_tweet_text = re.sub(r'constantnum[\\s]?(/|-)[\\s]?constantnum[\\s]?(/|-)[\\s]?constantnum', \" constantdate \" , transient_tweet_text)\n",
        "\t#date_regex = r'(constantnum)[\\s]*(st|nd|rd|th)[\\s]*(january|jan|february|feb|march|mar|april|may|june|jun|july|august|aug|september|sep|october|oct|november|nov|december|dec)'\n",
        "\tdate_regex1 = r'\\b((0|1|2|3)?[0-9][\\s]*)[-./]([\\s]*([012]?[0-9])[\\s]*)([-./]([\\s]*(19|20)[0-9][0-9]))?\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex1, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex2 = r'\\b((19|20)[0-9][0-9][\\s]*[-./]?)?[\\s]*([012]?[0-9])[\\s]*[-./][\\s]*(0|1|2|3)?[0-9]\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex2, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tMonths = regex_or('january','jan','february','feb','march','mar','april','may','june','jun','july','jul','august','aug','september','sep','october','oct','november','nov','december','dec')\n",
        "\tdate_regex3 = r'\\d+[\\s]*(st|nd|rd|th)[\\s]*' + Months \n",
        "\ttransient_tweet_text = re.sub(date_regex3, ' constantdate ', transient_tweet_text)\n",
        "\tdate_regex4 = Months + r'[\\s]*\\d+[\\s]*(st|nd|rd|th)*\\b'\n",
        "\ttransient_tweet_text = re.sub(date_regex4, ' constantdate ' , transient_tweet_text)\n",
        "\tdate_regex5 = r'[\\s]?\\b(19|20)[0-9][0-9]\\b[\\s]?'\n",
        "\ttransient_tweet_text = re.sub(date_regex5, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\tdate_regex6 = r'([\\s]*(constantdate))+'\n",
        "\ttransient_tweet_text = re.sub(date_regex6, ' constantdate ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_Times(transient_tweet_text):\n",
        "\t'''\n",
        "\tIndentify time and convert it to constant\n",
        "\t'''\n",
        "\ttime_regex1 = r'([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9][\\s]*(am|pm)?[\\s]*([iescm](st)|gmt|utc|[pmce](dt))?'\n",
        "\ttransient_tweet_text = re.sub(time_regex1, ' constanttime ' , transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com','shopohelp','shopo','mobikwikswat',\n",
        "\t\t'mobikwik','taxiforsure','tfscares','zoomcarindia','freshmenuindia','freshmenucares','grofers','jetairways',\n",
        "\t\t'cleartrip','olacabs','support_ola','makemytrip','makemytripcare','chai_point')\n",
        "\n",
        "\tBrandMentionRegex = r'@(\\b)*' + constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandMentionRegex, ' constantbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_NonBrandMentions(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all Mentions left, if any, in tweet text\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r\"@(\\w+)\", ' constantnonbrandmention ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def process_BrandName(transient_tweet_text):\n",
        "\t'''\n",
        "\tprocess all airrwoot brands Mentions if any in tweet text\n",
        "\t'''\n",
        "\tconstant_brands = regex_or('paytmcare', 'paytm','snapdeal_help','snapdeal','bluestone_com', 'bluestone','shopohelp','shopo',\n",
        "\t\t'mobikwikswat','mobikwik','taxiforsure','tfscares', 'tfs','zoomcarindia', 'zoomcar','freshmenuindia','freshmenucares', \n",
        "\t\t'freshmenu','grofers','jetairways', 'jet','cleartrip','olacabs','support_ola', 'olacab', 'ola' ,'makemytrip',\n",
        "\t\t'makemytripcare', 'chai_point')\n",
        "\n",
        "\tBrandNameRegex = constant_brands\n",
        "\ttransient_tweet_text = re.sub(BrandNameRegex, ' constantbrandname ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_Savings(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify sale/save offers\n",
        "\t'''\n",
        "\t#sale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*(constantnum)*[\\s]*[%]?[\\s]*(-|~)?[\\s]*(constantnum)*[\\s]*[%]?'\n",
        "\tsale_regex = r'(?<!#)\\b(discount|discounts|sale|save|saver|super[\\s]*saver|super[\\s]*save)\\b[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(-|~|or)?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?'\n",
        "\ttransient_tweet_text = re.sub(sale_regex, \" constantdiscount \", transient_tweet_text)\n",
        "\t#discount_List = []\n",
        "\t#discount_List = re.findall(r'constantdiscount', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Offers(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify cashbacks and off / substrings of the form \"30% off\" or \"30% cashback\" or \"$30 off\"\n",
        "\tReplace them by constantOFFER\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'[rs|$]?[ ]*[constantnum][ ]*[%]?[ ]?[off|cashback|offer]', \"constantoffer\", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:(up[\\s]?to)?((rs|\\$)*[\\s]*(constantnum))[\\s]*[%]?)?[\\s]*[-|~|or]?[\\.]?[\\s]*((rs|\\$)*[\\s]*(constantnum))*[\\s]*[%]?[\\s]*(offer|off|cashback|cash|cash back)', \" constantoffer \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:cashback|cash back|cash)\\b', \" constantoffer \", transient_tweet_text)\n",
        "\t#Offer_List = []\n",
        "\t#Offer_List = re.findall(r'constantoffer', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Promos(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify coupons/promos with promo codes\n",
        "\tAssumption - promo code can be alphanumeric. But it immediately follows text of promo/code/promocode etc\n",
        "\t'''\n",
        "\t#transient_tweet_text = re.sub(r'\\b(promocode|promo code|promo|code)[s]?[\\s]*[a-z]*(constantnum)*[a-z]*[\\s]+', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*(constantalphanum)\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[a-z]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code)\\b[\\s]*[0-9]+\\b', \" constantpromo \", transient_tweet_text)\n",
        "\ttransient_tweet_text = re.sub(r'(?<!#)\\b(?:promocode|promo code|promo|coupon code|code|coupon)[s]?\\b', \" constantpromo \", transient_tweet_text)\n",
        "\t#Promo_List = []\n",
        "\t#Promo_List = re.findall(r'constantpromo', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_Money(transient_tweet_text):\n",
        "\t'''\n",
        "\tidentify money in the tweet text but outside offers. This includes $,Rs, pound, Euro\n",
        "\t'''\n",
        "\tmoney_regex1 = r'\\b(rs|\\$)[\\s]*(constantnum)?[\\.]?[\\s]*constantnum\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex1, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex2 = r'[\\s]*[\\.]?[\\s]*constantnum(cent[s]?|\\$|c)\\b'\n",
        "\ttransient_tweet_text = re.sub(money_regex2, \" constantmoney \", transient_tweet_text)\n",
        "\tmoney_regex3 = r'(\\$|rs)[\\s]*constantalphanum'\n",
        "\ttransient_tweet_text = re.sub(money_regex3, \" constantmoney \", transient_tweet_text)\n",
        "\t#Money_List = []\n",
        "\t#Money_List = re.findall(r'constantmoney', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def indentify_freebies(transient_tweet_text):\n",
        "\t'''\n",
        "\tindentify freebies in tweets if any - free offers, free shipping, free trial, \n",
        "\t'''\n",
        "\tfreebies_regex1 = r'(?<!#)\\b(?:free)[\\s]+[a-z]+\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex1, \" constantfreebies \", transient_tweet_text)\n",
        "\tfreebies_regex2 = r'(?<!#)\\b(?:free)[\\s]+[a-z]*\\b'\n",
        "\ttransient_tweet_text = re.sub(freebies_regex2, \" constantfreebies \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def replace_numbers(transient_tweet_text):\n",
        "\t'''\n",
        "\tGiven any number/interger in tweet text, we want it to be replaced by constantnum\n",
        "\t'''\n",
        "\t# we want to process only those numbers that are not in a hashtag - below logic does this\n",
        "\tnum_regex = r'(?<!#)\\b(?:[-+]?[\\d,]*[\\.]?[\\d,]*[\\d]+|\\d+)\\b'\n",
        "\ttransient_tweet_text = re.sub(num_regex, \" constantnum \" , transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def identify_AlphaNumerics(transient_tweet_text):\n",
        "\t'''\n",
        "\tIdentify alpha numerics - this helps in identifying product codes/models, promocodes, Order IDs\n",
        "\t'''\n",
        "\tAlphaNumeric_regex = r'(?<!#)\\b(?:([a-z]+[0-9]+[a-z]*|[a-z]*[0-9]+[a-z]+)[a-z,0-9]*)\\b'\n",
        "\ttransient_tweet_text = re.sub(AlphaNumeric_regex, \" constantalphanum \", transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def strip_whiteSpaces(transient_tweet_text):\n",
        "\t'''\n",
        "\tStrip all white spaces\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'[\\s]+', ' ', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def prune_multple_consecutive_same_char(transient_tweet_text):\n",
        "\t'''\n",
        "\tyesssssssss  is converted to yess \n",
        "\tssssssssssh is converted to ssh\n",
        "\t'''\n",
        "\ttransient_tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', transient_tweet_text)\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_spl_words(transient_tweet_text):\n",
        "\ttransient_tweet_text = transient_tweet_text.replace('&amp;',' and ')\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "def remove_emoji(transient_tweet_text):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    tokenized_tweet = tweet_tokenizer.tokenize(transient_tweet_text)\n",
        "    emojis_present = demoji.findall(transient_tweet_text)\n",
        "    tweet_no_emoji=''\n",
        "    for i,s in enumerate(tokenized_tweet):\n",
        "        if s in emojis_present.keys():\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + emojis_present[s]\n",
        "        else:\n",
        "            tweet_no_emoji = tweet_no_emoji + ' ' + s\n",
        "    return tweet_no_emoji\n",
        "\n",
        "def deEmojify(transient_tweet_text):\n",
        "    return transient_tweet_text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "# def print_test():\n",
        "    \n",
        "#     test_tweet = \"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"\n",
        "#     #General Proprocessing\n",
        "#     test_tweet = test_tweet.lower()\n",
        "#     test_tweet = strip_unicode(test_tweet)\n",
        "    \n",
        "#     #function tests\n",
        "#     print(\"Process URLS:\\n\",process_URLs(test_tweet))\n",
        "#     print(\"Remove websites:\\n\",process_Websites(test_tweet))\n",
        "#     print(\"Remove mentions:\\n\",process_Mentions(test_tweet))\n",
        "#     print('Remove Emailid:\\n',process_EmailIds(test_tweet))\n",
        "#     print(\"Remove Hashtags:\\n\",process_HashTags(test_tweet))\n",
        "#     print(\"Remove Dates:\\n\",process_Dates(test_tweet))\n",
        "#     print(\"Process Time:\\n\",process_Times(test_tweet))\n",
        "#     print(\"Process Brand Mention:\\n\",process_BrandMentions(test_tweet))\n",
        "#     print(\"Process non Brand Mention:\\n\",process_NonBrandMentions(test_tweet))\n",
        "#     print(\"Process Brand Name:\\n\",process_BrandName(test_tweet))\n",
        "#     print(\"Process Savings:\\n\",identify_Savings(test_tweet))\n",
        "#     print(\"Process Offers:\\n\",indentify_Offers(test_tweet))\n",
        "#     print(\"Identiy Promos:\\n\",indentify_Promos(test_tweet))\n",
        "# ############\n",
        "# print_test()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_TweetText(tweet_text):\n",
        "\t'''\n",
        "\tTakes tweet_text and preprocesses it \n",
        "\tOrder of preprocessing:\n",
        "\t'''\n",
        "\n",
        "\t# get utf-8 encoding, lowercase, trim and remove multiple white spaces\n",
        "\ttransient_tweet_text = tweet_text\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\t#print \"PROCESSED: \", transient_tweet_text\n",
        "\n",
        "\ttransient_tweet_text = to_LowerCase(transient_tweet_text)\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\ttransient_tweet_text = remove_spl_words(transient_tweet_text)\n",
        "\n",
        " \n",
        "\t#emoji\n",
        "\ttransient_tweet_text = remove_emoji(transient_tweet_text) \n",
        "\ttransient_tweet_text = deEmojify(transient_tweet_text)\n",
        "\t# process Hastags, URLs, Websites, process_EmailIds\n",
        "\t# Give precedence to url over hashtag\n",
        "\ttransient_tweet_text = process_URLs(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_HashTags(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_Websites(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_EmailIds(transient_tweet_text)\n",
        "\n",
        "\t# process for brand mention, any other mention and brand Name\n",
        "\t#transient_tweet_text = process_BrandMentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_NonBrandMentions(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Mentions(transient_tweet_text)\n",
        "\t#transient_tweet_text = process_BrandName(transient_tweet_text)\n",
        "\n",
        "\t# remove any unicodes\n",
        "\ttransient_tweet_text = strip_unicode(transient_tweet_text)\n",
        "\n",
        "\t# identify Date / Time if any\n",
        "\ttransient_tweet_text = process_Times(transient_tweet_text)\n",
        "\ttransient_tweet_text = process_Dates(transient_tweet_text)\n",
        "\n",
        "\t# indentify alphanums and nums\n",
        "\ttransient_tweet_text = identify_AlphaNumerics(transient_tweet_text)\n",
        "\ttransient_tweet_text = replace_numbers(transient_tweet_text)\n",
        "\t\n",
        "\t# identify promos, savings, offers, money and freebies\n",
        "\ttransient_tweet_text = indentify_Promos(transient_tweet_text)\n",
        "\ttransient_tweet_text = identify_Savings(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Offers(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_Money(transient_tweet_text)\n",
        "\ttransient_tweet_text = indentify_freebies(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = trim(transient_tweet_text)\n",
        "\ttransient_tweet_text = strip_whiteSpaces(transient_tweet_text)\n",
        "\n",
        "\ttransient_tweet_text = prune_multple_consecutive_same_char(transient_tweet_text)\n",
        "\n",
        "\treturn transient_tweet_text\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# print(process_TweetText(\"Nice @varun paytm @paytm saver abc@gmail.com sizes for the wolf on 20/10/2010 at 10:00PM  grey/deep royal-volt Nike Air Skylon II retro are 40% OFF for a limited time at $59.99 + FREE shipping.BUY HERE -> https://bit.ly/2L2n7rB (promotion - use code MEMDAYSV at checkout)\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.41 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzWAE4Hd9_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98a0310f-db3e-45e3-dc69-724d9fcb91f2"
      },
      "source": [
        "#Making the necessary imports\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#preprocessing_path = \"/home/etherealenvy/Downloads/practical-nlp/Ch8/O5_smtd_preprocessing.py\"\n",
        "#sys.path.append(os.path.abspath(preprocessing_path))\n",
        "\n",
        "#import O5_smtd_preprocessing\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ST7YsMTd9_u",
        "colab_type": "text"
      },
      "source": [
        "## Reading and Preprocessing\n",
        "Let's read the dataset and pre-process them using the pre-processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XOA7NWfggE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "786f7693-6721-4822-ea45-fc438ed91993"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chLBGh79d9_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "52cc16c4-40df-4a2d-8d11-896170de9bb0"
      },
      "source": [
        "\n",
        "datapath = \"/content/drive/My Drive/Data/NLP/Practical_NLP_Oreilly/Chap8/sts_gold_tweet.csv\"\n",
        "df = pd.read_csv(datapath,error_bad_lines=False,delimiter=\";\")\n",
        "df = df.dropna(how='any')\n",
        "df.drop(columns=['id'], inplace=True)\n",
        "display(df.head())\n",
        "\n",
        "#pre-process tweets using our package\n",
        "tweets_no_processed = df['tweet'].values\n",
        "df['tweet'] = df['tweet'].apply(lambda x: process_TweetText(x))\n",
        "df['tweet_tokens'] = df['tweet'].apply(lambda x: tweet_tokenizer.tokenize(x))\n",
        "df['tweet_no_stopwords'] = df['tweet_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
        "tweets_processed = df['tweet_tokens'].values\n",
        "tweets_cat = df['polarity'].values\n",
        "\n",
        "display(df.head())\n",
        "print(\"Number of tweets and categories\")\n",
        "print(len(tweets_processed), len(tweets_cat))\n",
        "print(\"\\nExamle of polarity, processed tweet, processed tweet without stopwords\")\n",
        "print(tweets_cat[0],',',tweets_processed[0],',',df['tweet_no_stopwords'].values[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the angel is going to miss the athlete this we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>It looks as though Shaq is getting traded to C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>drinking a McDonalds coffee and not understand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity                                              tweet\n",
              "0         0  the angel is going to miss the athlete this we...\n",
              "1         0  It looks as though Shaq is getting traded to C...\n",
              "2         0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \n",
              "3         0  drinking a McDonalds coffee and not understand...\n",
              "4         0  So dissapointed Taylor Swift doesnt have a Twi..."
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_tokens</th>\n",
              "      <th>tweet_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the angel is going to miss the athlete this we...</td>\n",
              "      <td>[the, angel, is, going, to, miss, the, athlete...</td>\n",
              "      <td>[angel, going, miss, athlete, weekend]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>it looks as though shaq is getting traded to c...</td>\n",
              "      <td>[it, looks, as, though, shaq, is, getting, tra...</td>\n",
              "      <td>[looks, though, shaq, getting, traded, clevela...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>constantnonbrandmention constantdate isn't com...</td>\n",
              "      <td>[constantnonbrandmention, constantdate, isn't,...</td>\n",
              "      <td>[constantnonbrandmention, constantdate, coming...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>drinking a mcdonalds coffee and not understand...</td>\n",
              "      <td>[drinking, a, mcdonalds, coffee, and, not, und...</td>\n",
              "      <td>[drinking, mcdonalds, coffee, understanding, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>so dissapointed taylor swift doesnt have a twi...</td>\n",
              "      <td>[so, dissapointed, taylor, swift, doesnt, have...</td>\n",
              "      <td>[dissapointed, taylor, swift, doesnt, twitter]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                 tweet_no_stopwords\n",
              "0         0  ...             [angel, going, miss, athlete, weekend]\n",
              "1         0  ...  [looks, though, shaq, getting, traded, clevela...\n",
              "2         0  ...  [constantnonbrandmention, constantdate, coming...\n",
              "3         0  ...  [drinking, mcdonalds, coffee, understanding, s...\n",
              "4         0  ...     [dissapointed, taylor, swift, doesnt, twitter]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Number of tweets and categories\n",
            "2034 2034\n",
            "\n",
            "Examle of polarity, processed tweet, processed tweet without stopwords\n",
            "0 , ['the', 'angel', 'is', 'going', 'to', 'miss', 'the', 'athlete', 'this', 'weekend'] , ['angel', 'going', 'miss', 'athlete', 'weekend']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HwcGzFAd9_3",
        "colab_type": "text"
      },
      "source": [
        "## Train your own Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W2rN1W0hPhM",
        "colab_type": "text"
      },
      "source": [
        "Twitter process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXWkZ2God9_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eff3289f-e5f1-4e13-cbfe-3963c56ca39d"
      },
      "source": [
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(tweets_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CBOW Model Training Complete.\n",
            "Time taken for training is:0.33940 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCmm7SwBd9_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64c33dbe-7916-47e5-df4c-e845cb0bd25b"
      },
      "source": [
        "#Create document vectors by averaging word vectors.\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 100\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this/count_for_this if count_for_this > 0 else feat_for_this)        \n",
        "    return feats\n",
        "\n",
        "train_vectors = embedding_feats(df['tweet_no_stopwords'].values)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBM6XB4ad9_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "754958aa-f4f3-40a0-de58-313a45cd3206"
      },
      "source": [
        "#Take any classifier (LogisticRegression here)\n",
        "classifier = LogisticRegression(random_state=2020)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, \n",
        "                                                                df['polarity'].values)\n",
        "\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6895874263261297\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      1.00      0.82       351\n",
            "           4       0.00      0.00      0.00       158\n",
            "\n",
            "    accuracy                           0.69       509\n",
            "   macro avg       0.34      0.50      0.41       509\n",
            "weighted avg       0.48      0.69      0.56       509\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viidLnjlhSHL",
        "colab_type": "text"
      },
      "source": [
        "Twitter no process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jKT7Hcmilif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "3ca1081e-0dd0-4a5e-bf03-8267c069e107"
      },
      "source": [
        "datapath = \"Path to repo\"\n",
        "df = pd.read_csv(datapath+\"/practical-nlp/Ch8/Data/sts_gold_tweet.csv\",\n",
        "                 error_bad_lines=False,delimiter=\";\")\n",
        "df = df.dropna(how='any')\n",
        "df.drop(columns=['id'], inplace=True)\n",
        "display(df.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c0ee21d93103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Path to repo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m df = pd.read_csv(datapath+\"/practical-nlp/Ch8/Data/sts_gold_tweet.csv\",\n\u001b[0;32m----> 3\u001b[0;31m                  error_bad_lines=False,delimiter=\";\")\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'any'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File Path to repo/practical-nlp/Ch8/Data/sts_gold_tweet.csv does not exist: 'Path to repo/practical-nlp/Ch8/Data/sts_gold_tweet.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npaLnFeyiQO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CBOW\n",
        "import time\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(tweets_processed,min_count=5, sg=0)\n",
        "end = time.time()\n",
        "\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcmibLVHiQRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create document vectors by averaging word vectors.\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 100\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this/count_for_this if count_for_this > 0 else feat_for_this)        \n",
        "    return feats\n",
        "\n",
        "train_vectors = embedding_feats(df['tweet_no_stopwords'].values)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urXvSSN4iQWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Take any classifier (LogisticRegression here)\n",
        "classifier = LogisticRegression(random_state=2020)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, \n",
        "                                                                df['polarity'].values)\n",
        "\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3de9bO-siQUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}