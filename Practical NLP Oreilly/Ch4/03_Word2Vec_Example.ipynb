{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sVtvH58nb_Hp"
      },
      "source": [
        "# Word2Vec for Text Classification\n",
        "\n",
        "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
        "\n",
        "We will use the sentiment labelled sentences dataset from UCI repository\n",
        "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
        "\n",
        "```cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt```\n",
        "\n",
        "For a pre-trained embedding model, we will use the Google News vectors.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "Let us get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JQX8DAmBb_Hr",
        "colab": {}
      },
      "source": [
        "#basic imports\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "#pre-processing imports\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww39Tp4uNHtT",
        "colab_type": "text"
      },
      "source": [
        "#Word2vector by another lib (DNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88-MfyXJNNZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors #To load the model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') #ignore any generated warnings\n",
        "import numpy as np\n",
        "# load model\n",
        "path='https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/Models/word2vec_cbow.bin?raw=true'\n",
        "model_new = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHDURxxQNNcD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8685dd2-f037-49f8-92b0-623e0c58ca0b"
      },
      "source": [
        "model_new"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa9713a8ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU_H0xfITD_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec295b5c-db02-4ee9-a6e8-11649522af60"
      },
      "source": [
        "# Inspect the model\n",
        "word2vec_vocab_new = model_new.vocab.keys()\n",
        "word2vec_vocab_lower_new = [item.lower() for item in word2vec_vocab_new]\n",
        "print(len(word2vec_vocab_new))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shmk89KITZRw",
        "colab_type": "text"
      },
      "source": [
        "Model nay chi co 161,000 tu, kem hon nhieu model cua google la 3,000,000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8pxR1N6ND-p",
        "colab_type": "text"
      },
      "source": [
        "# Word2vector by googlenew"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC-Zgd1yLyzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0cc65d6d-b673-4806-bb7c-11541746f5a2"
      },
      "source": [
        "#download google new vectors (other tool than word2vector)\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: brew: command not found\n",
            "--2020-08-01 04:02:29--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.237\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.129.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  32.0MB/s    in 50s     \n",
            "\n",
            "2020-08-01 04:03:20 (31.2 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKb-sf9gKE5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7a6e1fd6-53d4-4d9f-bdb2-9a2e9d2209b0"
      },
      "source": [
        "path_to_model = '/content/GoogleNews-vectors-negative300.bin.gz'\n",
        "#Load W2V model. This will take some time. \n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
        "print('done loading Word2Vec')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 53s, sys: 4.09 s, total: 1min 57s\n",
            "Wall time: 1min 57s\n",
            "done loading Word2Vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COUGXAxcb_H5",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "training_data_path = '/content/drive/My Drive/Data/NLP/sentiment_all.txt'\n",
        "#Read text data, cats.\n",
        "#the file path consists of tab separated sentences and cats.\n",
        "texts = []\n",
        "cats = []\n",
        "fh = open(training_data_path)\n",
        "for line in fh:\n",
        "    text, sentiment = line.split(\"\\t\")\n",
        "    texts.append(text)\n",
        "    cats.append(sentiment)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m-WjFyC6b_IE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1c4f087-3f2a-4b31-cc29-ea90494c36c5"
      },
      "source": [
        "# Inspect the model\n",
        "word2vec_vocab = w2v_model.vocab.keys()\n",
        "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
        "print(len(word2vec_vocab))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XEz30Jztb_IP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ffcf2a95-cd9b-4bb4-e0f4-716c8cfd93ba"
      },
      "source": [
        "#Inspect the dataset\n",
        "print(len(cats), len(texts))\n",
        "print(texts[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 3000\n",
            "Good case, Excellent value.\n",
            "1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbVvD0V1PyYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6d0d3bec-4bad-4e8c-9f33-e1482cdca28f"
      },
      "source": [
        "#Preprocessing our models vocabulary to make better visualizations\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFOGaDTwb_Ig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1d991400-d399-4a76-c6e2-3ca6adea020c"
      },
      "source": [
        "#preprocess the text.\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
        "        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "\n",
        "texts_processed = preprocess_corpus(texts)\n",
        "print(len(cats), len(texts_processed))\n",
        "print(texts_processed[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 3000\n",
            "['good', 'case', 'excellent', 'value']\n",
            "1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEXz9twLQ1pL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "95da73c0-94a8-48b4-dce8-bcc1a48b0f8a"
      },
      "source": [
        "texts_processed[:5]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['so', 'way', 'plug', 'us', 'unless', 'i', 'go', 'converter'],\n",
              " ['good', 'case', 'excellent', 'value'],\n",
              " ['great', 'jawbone'],\n",
              " ['tied', 'charger', 'conversations', 'lasting', 'minutes.major', 'problems'],\n",
              " ['the', 'mic', 'great']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fXRiGtY1b_Iq",
        "colab": {}
      },
      "source": [
        "# Creating a feature vector by averaging all embeddings for all sentences\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 300\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists: # trich tung cau sentiments trong toan van ban\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        # tach tung tu trong 1 cau sentiments, chuyen sang vector kich thuoc 300\n",
        "        for token in tokens: #['so', 'way', 'plug', 'us', 'unless', 'i', 'go', 'converter'],\n",
        "            if token in w2v_model: #'so'...\n",
        "                feat_for_this += w2v_model[token] # 300 dimension\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this/count_for_this)    \n",
        "        # cong tat ca token vector cua tung tu trong 1 sentiment va chia tong lay trung binh     \n",
        "    return feats # return tung sentiment da duoc vector (300 dim)\n",
        "\n",
        "train_vectors = embedding_feats(texts_processed)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52AdcnYSVKzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b27f8c4-ac46-4d19-da09-f120649f65ca"
      },
      "source": [
        "print(len(train_vectors))\n",
        "print(len(train_vectors[0]))\n",
        "# train_vectors chua 3000 tu sentiment da duoc convert sang vector 300 dim"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH4co1V1VTu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16bbeb72-37b5-4662-bf54-99a465239780"
      },
      "source": [
        "len(train_vectors[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mr9IaQppb_Ix",
        "colab": {},
        "outputId": "13a84b5c-fde3-49f4-b156-5c2f36592b19"
      },
      "source": [
        "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
        "classifier = LogisticRegression(random_state=1234)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.812\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "         0\n",
            "       0.82      0.80      0.81       374\n",
            "         1\n",
            "       0.80      0.83      0.82       376\n",
            "\n",
            "avg / total       0.81      0.81      0.81       750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k7wjLB8rb_JB"
      },
      "source": [
        "Not bad. With little efforts we got 81% accuracy. Thats a great starting model to have!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8z3tJJJkb_JB",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}