{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QnXzcZvdc-r6"
   },
   "source": [
    "In this notebook we will demostrate how to perform tokenization,stemming,lemmatization and pos_tagging using libraries like [spacy](https://spacy.io/) and [nltk](https://www.nltk.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3xEmJpRc5r8"
   },
   "outputs": [],
   "source": [
    "#This will be our corpus which we will work on\n",
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KHh_33IopPTf",
    "outputId": "37c2db65-d10d-4254-80df-48a3f48713cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
     ]
    }
   ],
   "source": [
    "#lower case the corpus\n",
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3yaGf8RiqgBM",
    "outputId": "5856d53c-63e9-4045-dc53-96bb98fba30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
     ]
    }
   ],
   "source": [
    "#removing digits in the corpus\n",
    "import re\n",
    "corpus = re.sub(r'\\d+','', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v5Q--GItqzfu",
    "outputId": "e0581860-574e-4f1e-f517-29b60b438ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "import string\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zmANqee9rK4N",
    "outputId": "227843ab-d55f-4c9a-adff-cc253eb36450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing trailing whitespaces\n",
    "corpus = corpus.strip()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "colab_type": "code",
    "id": "KMuHZTpy9X_u",
    "outputId": "9ec1f466-2e24-4262-b48c-a6c36f3dcb7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (2.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/etherealenvy/.local/lib/python3.6/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (2.6.0)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/etherealenvy/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.46.1)\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/etherealenvy/miniconda3/envs/practicalnlp/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfJx3MnVj_ph"
   },
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "OUz580k2sMqf",
    "outputId": "5195f746-7e75-4cfa-d763-4856359baf05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/etherealenvy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/etherealenvy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK\n",
      "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords: ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n",
      "\n",
      "Spacy:\n",
      "Tokenized Corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords ['need', 'finalize', 'demo', 'corpus', 'notebook', 'soon', 'ending', 'month', 'notebook', 'run', 'times']\n",
      "Difference between NLTK and spaCy output:\n",
      " {'used', 'done'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)\n",
    "\n",
    "\n",
    "##SPACY \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print(\"Tokenized Corpus:\",text_tokens)\n",
    "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "\n",
    "print(\"Tokenized corpus without stopwords\",tokens_without_sw)\n",
    "\n",
    "\n",
    "print(\"Difference between NLTK and spaCy output:\\n\",\n",
    "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRH_ltkD-HpA"
   },
   "source": [
    "Notice the difference output after stopword removal using nltk and spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGcwD1JlkEao"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ibEpzcv0sdW8",
    "outputId": "f40aa595-f67f-4042-e9bf-e5b4a160d524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(corpus)\n",
    "\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Wy6cwvYkJeR"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "27KvL4ZE-fqJ",
    "outputId": "41dc046e-14dd-4d4f-83ca-2b3af4532593"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/etherealenvy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8uCGA8ukMfQ"
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kZqBxLDz-6cu",
    "outputId": "9b89af94-c433-4099-b1c3-3adefecae984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using spacy:\n",
      "Need : VERB\n",
      "to : PART\n",
      "finalize : VERB\n",
      "the : DET\n",
      "demo : ADJ\n",
      "corpus : NOUN\n",
      "which : DET\n",
      "will : VERB\n",
      "be : VERB\n",
      "used : VERB\n",
      "for : ADP\n",
      "this : DET\n",
      "notebook : NOUN\n",
      "and : CCONJ\n",
      "it : PRON\n",
      "should : VERB\n",
      "be : VERB\n",
      "done : VERB\n",
      "soon : ADV\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      ". : PUNCT\n",
      "It : PRON\n",
      "should : VERB\n",
      "be : VERB\n",
      "done : VERB\n",
      "by : ADP\n",
      "the : DET\n",
      "ending : NOUN\n",
      "of : ADP\n",
      "this : DET\n",
      "month : NOUN\n",
      ". : PUNCT\n",
      "But : CCONJ\n",
      "will : VERB\n",
      "it : PRON\n",
      "? : PUNCT\n",
      "This : DET\n",
      "notebook : NOUN\n",
      "has : VERB\n",
      "been : VERB\n",
      "run : VERB\n",
      "4 : NUM\n",
      "times : NOUN\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      "POS Tagging using NLTK:\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/etherealenvy/nltk_data'\n    - '/home/etherealenvy/miniconda3/envs/practicalnlp/nltk_data'\n    - '/home/etherealenvy/miniconda3/envs/practicalnlp/share/nltk_data'\n    - '/home/etherealenvy/miniconda3/envs/practicalnlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-846410f9a263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# nltk.download('averaged_perceptron_tagger')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POS Tagging using NLTK:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/etherealenvy/nltk_data'\n    - '/home/etherealenvy/miniconda3/envs/practicalnlp/nltk_data'\n    - '/home/etherealenvy/miniconda3/envs/practicalnlp/share/nltk_data'\n    - '/home/etherealenvy/miniconda3/envs/practicalnlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#POS tagging using spacy\n",
    "doc = spacy_model(corpus_original) \n",
    "  \n",
    "print(\"POS Tagging using spacy:\")  \n",
    "# Token and Tag \n",
    "for token in doc: \n",
    "  print(token,\":\", token.pos_)\n",
    "\n",
    "\n",
    "\n",
    "#pos tagging using nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "print(\"POS Tagging using NLTK:\")\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWdmz6lFkpEI"
   },
   "source": [
    "There are various other libraries you can use to perform these common pre-processing steps"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tokenization_Stemming_lemmatization_stopword_postagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
