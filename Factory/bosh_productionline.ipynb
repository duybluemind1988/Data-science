{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Date Exploration (6min == 0.01)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes.\n\nBecause Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods.\n\nIn this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Submissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by:\n\nMCC=(TP∗TN)−(FP∗FN)(TP+FP)(TP+FN)(TN+FP)(TN+FN)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√,\n\n\nwhere TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data Description\n\nThe data for this competition represents measurements of parts as they move through Bosch's production lines. Each part has a unique Id. The goal is to predict which parts will fail quality control (represented by a 'Response' = 1).\n\nThe dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939.\n\nOn account of the large size of the dataset, we have separated the files by the type of feature they contain: numerical, categorical, and finally, a file with date features. The date features provide a timestamp for when each measurement was taken. Each date column ends in a number that corresponds to the previous feature number. E.g. the value of L0_S0_D1 is the time at which L0_S0_F0 was taken.\n\nIn addition to being one of the largest datasets (in terms of number of features) ever hosted on Kaggle, the ground truth for this competition is highly imbalanced. Together, these two attributes are expected to make this a challenging problem.\n\nFile descriptions\n\n- train_numeric.csv - the training set numeric features (this file contains the 'Response' variable)\n- test_numeric.csv - the test set numeric features (you must predict the 'Response' for these Ids)\n- train_categorical.csv - the training set categorical features\n- test_categorical.csv - the test set categorical features\n- train_date.csv - the training set date features\n- test_date.csv - the test set date features\n- sample_submission.csv - a sample submission file in the correct format","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Giai thich dataset:\n1. Moi line se co mot so station, id cua tung station la rieng biet o tung line, do do chi can biet part nam o station nao la duoc roi, sau do se tu truy ra line nao\n2. Moi part khi di qua station se duoc do cac feature tuong ung tai mot thoi gian cu the, do do column se theo ID (part), time (thoi gian do), L3_S36_f3939: part do vao thoi gian 87.2 dang o line 3, station 36, do feature 3939\n3. Tai 1 station part se do nhieu feature khac nhau , thoi gian do tat ca feature cua part do tai station do la nhu nhau","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hi,\nThe timestamps were anonymized in this competition. My motivation was to understand how long is the test/ train period. This would allow to use some intuition for feature engineering.\nMy main question was: what does 0.01 time difference mean? Is it ms, s, m, hour, day? To answer that I tried to find periodic patterns using auto correlation.\n\nI can't help you to answer the how to begin question. Fortunately there are plenty of forum topics with similar questions.\nhttps://www.kaggle.com/forums/f/208/getting-started\nJust google \"kaggle start\" or \"kaggle begin\"\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Let's check the min and max times for each station\ndef get_station_times(dates, withId=False):\n    times = []\n    cols = list(dates.columns)\n    if 'Id' in cols:\n        cols.remove('Id')\n    for feature_name in cols:\n        if withId:\n            df = dates[['Id', feature_name]].copy()\n            df.columns = ['Id', 'time']\n        else:\n            df = dates[[feature_name]].copy()\n            df.columns = ['time']\n        df['station'] = feature_name.split('_')[1][1:]\n        df = df.dropna()\n        times.append(df)\n    return pd.concat(times)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date_part = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', nrows=10000)\nprint(train_date_part.shape)\ntrain_date_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date_part_reprocess=get_station_times(train_date_part, withId=True)\ntrain_date_part_reprocess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_numeric = pd.read_csv('../input/bosch-production-line-performance/train_numeric.csv.zip', nrows=10000)\nprint(train_numeric.shape)\ntrain_numeric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"len(set(train_numeric.Id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_categorical = pd.read_csv('../input/bosch-production-line-performance/train_categorical.csv.zip', nrows=10000)\nprint(train_categorical.shape)\ntrain_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Let's check the min and max times for each station\n# Tach tung cot ra, sau do dropna tung cot, roi append tat ca lai voi nhau\n    \ndates=train_date_part.copy()\nwithId=True\ntimes = []\ncols = list(dates.columns)\nif 'Id' in cols:\n    cols.remove('Id')\nfor feature_name in cols:\n    if withId:\n        df = dates[['Id', feature_name]].copy()\n        df.columns = ['Id', 'time']\n    else:\n        df = dates[[feature_name]].copy()\n        df.columns = ['time']\n    df['line'] = feature_name.split('_')[0][1:]\n    df['station'] = feature_name.split('_')[1][1:]\n    df['feature_number'] = feature_name.split('_')[2][1:]\n    df = df.dropna()\n    print(df.shape)\n    times.append(df)\n    print(len(times))\nstation_times=pd.concat(times)\nstation_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"station_times=station_times.sort_values(by=['Id','station'])\nstation_times['line']=station_times['line'].astype('int64')\nstation_times['station']=station_times['station'].astype('int64')\nstation_times['feature_number']=station_times['feature_number'].astype('int64')\nprint(station_times.dtypes)\nstation_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# How many station in each line ?\nset(station_times[station_times.line==0].station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# How many station in each time ?\nprint(set(station_times[station_times.time==82.24].station))\n# How many part in each time ?\nprint(set(station_times[station_times.time==82.24].Id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"time=82.24\nprint('time: ',time)\nstation_=set(station_times[station_times.time==82.24].station)\nfor j in station_:\n    print('station:', j)\n    print('part: ',set(station_times[(station_times.time==time) & (station_times.station==j)].Id))\n    print('feature_number: ',set(station_times[(station_times.time==time) & (station_times.station==j)].feature_number))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"time=1379.78\nprint('time: ',time)\nstation_=set(station_times[station_times.time==82.24].station)\nfor j in station_:\n    print('station:', j)\n    print('part: ',set(station_times[(station_times.time==time) & (station_times.station==j)].Id))\n    print('feature_number: ',set(station_times[(station_times.time==time) & (station_times.station==j)].feature_number))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"set(station_times[station_times.line==1].station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"set(station_times[station_times.line==2].station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"set(station_times[station_times.line==3].station)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each station is unique in each line, so no need to include line here","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"station_times.line.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"set(station_times.Id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"station_times.station.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"station_times.feature_number.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"part_id=6\npart_filter=station_times[station_times.Id==part_id]\npart_filter_line=set(part_filter.line)\npart_filter_station=set(part_filter.station)\nprint('total_line: ',part_filter_line)\nprint('total_station: ',part_filter_station)\n\nfor i in part_filter_line:\n    print('line:', i)\n    for j in part_filter_station:\n        print('station:', j)\n        print('feature_number: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].feature_number))\n        print('time: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].time))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def part_info(part_id):\n    #part_id=part_id\n    part_filter=station_times[station_times.Id==part_id]\n    part_filter_line=set(part_filter.line)\n    part_filter_station=set(part_filter.station)\n    print('total_line: ',part_filter_line)\n    print('total_station: ',part_filter_station)\n    print('-'*60)\n\n    for i in part_filter_line:\n        print('-'*10)\n        print('line:', i)\n        for j in part_filter_station:\n            print('station:', j)\n            print('feature_number: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].feature_number))\n            print('time: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].time))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"part_info(120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"min_station_times = station_times.groupby(['Id', 'station']).min()['time']\nmax_station_times = station_times.groupby(['Id', 'station']).max()['time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"min_station_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"max_station_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date_part.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Read station times for train and test\ndate_cols = train_date_part.drop('Id', axis=1).count().reset_index().\\\n            sort_values(by=0, ascending=False)\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"date_cols['station'] = date_cols['index'].apply(lambda s: s.split('_')[1])\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"date_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\ndate_cols # selected features\n# remove all duplicate station (with differtion feature measurment each station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# applied these columns to all training data set\ntrain_date = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', usecols=date_cols)\nprint(train_date.shape)\ntrain_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dates=train_date.copy()\nwithId=False\ntimes = []\ncols = list(dates.columns)\nif 'Id' in cols:\n    cols.remove('Id')\nfor feature_name in cols:\n    if withId:\n        df = dates[['Id', feature_name]].copy()\n        df.columns = ['Id', 'time']\n    else:\n        df = dates[[feature_name]].copy()\n        df.columns = ['time']\n    df['line'] = feature_name.split('_')[0][1:]\n    df['station'] = feature_name.split('_')[1][1:]\n    df['feature_number'] = feature_name.split('_')[2][1:]\n    df = df.dropna()\n    #print(df.shape)\n    times.append(df)\n    #print(len(times))\ntrain_station_times=pd.concat(times)\nprint(train_station_times.shape)\ntrain_station_times\n# Do chi giu lai 52 columns nen tong so dong 14 tr, khong qua nhieu, neu giu lai 1000 columns thi\n# con so se rat lon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_time_cnt = train_station_times.groupby('time').count()[['station']].reset_index()\ntrain_time_cnt.columns = ['time', 'cnt']\nprint(train_time_cnt.shape)\ntrain_time_cnt\n# Loc thoi gian testing tung feature ung voi bao nhieu station.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test_date = pd.read_csv('../input/bosch-production-line-performance/test_date.csv.zip', usecols=date_cols)\nprint(test_date.shape)\ntest_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dates=test_date.copy()\nwithId=False\ntimes = []\ncols = list(dates.columns)\nif 'Id' in cols:\n    cols.remove('Id')\nfor feature_name in cols:\n    if withId:\n        df = dates[['Id', feature_name]].copy()\n        df.columns = ['Id', 'time']\n    else:\n        df = dates[[feature_name]].copy()\n        df.columns = ['time']\n    df['line'] = feature_name.split('_')[0][1:]\n    df['station'] = feature_name.split('_')[1][1:]\n    df['feature_number'] = feature_name.split('_')[2][1:]\n    df = df.dropna()\n    #print(df.shape)\n    times.append(df)\n    #print(len(times))\ntest_station_times=pd.concat(times)\nprint(test_station_times.shape)\ntest_station_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test_time_cnt = test_station_times.groupby('time').count()[['station']].reset_index()\ntest_time_cnt.columns = ['time', 'cnt']\nprint(test_time_cnt.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure()\nplt.plot(train_time_cnt['time'].values, train_time_cnt['cnt'].values, 'b.', alpha=0.1, label='train')\nplt.plot(test_time_cnt['time'].values, test_time_cnt['cnt'].values, 'r.', alpha=0.1, label='test')\nplt.title('Original date values')\nplt.ylabel('Number of records')\nplt.xlabel('Time')\nfig.savefig('original_date_values.png', dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print((train_time_cnt['time'].min(), train_time_cnt['time'].max()))\nprint((test_time_cnt['time'].min(), test_time_cnt['time'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few observations:\n\n- Train and test set has the same time period\n- There is a clear periodic pattern\n- The dates are transformed to 0 - 1718 with granularity of 0.01\n- There is a gap in the middle\n\nCould we figure out what does 0.01 mean? Let's check a few auto correlations!","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"time_ticks = np.arange(train_time_cnt['time'].min(), train_time_cnt['time'].max() + 0.01, 0.01)\ntime_ticks = pd.DataFrame({'time': time_ticks})\ntime_ticks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"time_ticks = pd.merge(time_ticks, train_time_cnt, how='left', on='time')\ntime_ticks = time_ticks.fillna(0)\ntime_ticks\n# Dem bao nhieu station lien quan toi specific time trong toan bo data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Autocorrelation\nx = time_ticks['cnt'].values\nmax_lag = 8000\nauto_corr_ks = range(1, max_lag)\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nauto_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(len(auto_corr_ks))\nprint(auto_corr_ks)\nprint(len(x))\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"k=1\nprint('k',k)\nprint(len(x[:-k]))\nprint(len(x[k:]))\nprint(x[:-k])\nprint(x[k:])\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:]))\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:])[0,1])\nprint('corrcoef: \\n',np.array([1]+ np.corrcoef(x[:-k], x[k:])[0,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"k=3\nprint('k',k)\nprint(len(x[:-k]))\nprint(len(x[k:]))\nprint(x[:-k])\nprint(x[k:])\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:]))\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:])[0,1])\nprint('corrcoef: \\n',np.array([1]+ np.corrcoef(x[:-k], x[k:])[0,1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Autocorrelation period 0.01","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(len(auto_corr))\nauto_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(auto_corr, 'k.', label='autocorrelation by 0.01')\nplt.title('Train Sensor Time Auto-correlation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Autocorrelation period 25","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"period = 25\nauto_corr_ks = list(range(period, max_lag, period))\nprint(len(auto_corr_ks))\nprint(auto_corr_ks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":false},"cell_type":"code","source":"auto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nauto_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.plot([0] + auto_corr_ks, auto_corr, 'go', alpha=0.5, label='strange autocorrelation at 0.25')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Autocorrelation period 1675","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"period = 1675\nauto_corr_ks = list(range(period, max_lag, period))\nprint(len(auto_corr_ks))\nprint(auto_corr_ks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"auto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nauto_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.plot([0] + auto_corr_ks, auto_corr, 'ro', markersize=10, alpha=0.5, label='one week = 16.75?')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(auto_corr, 'k.', label='autocorrelation by 0.01')\nplt.title('Train Sensor Time Auto-correlation')\n\nperiod = 25\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'go', alpha=0.5, label='strange autocorrelation at 0.25')\n\nperiod = 1675\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'ro', markersize=10, alpha=0.5, label='one week = 16.75?')\n\nplt.xlabel('k * 0.01 -  autocorrelation lag')\nplt.ylabel('autocorrelation')\nplt.legend(loc=0)\n#fig.savefig('train_time_auto_correlation.png', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest peaks are at approximately 1680 ticks. Let's call it a week ;)\n\nIn each week we could see 7 local maxima ~ days.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_time_cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"week_duration = 1679\ntrain_time_cnt['week_part'] = ((train_time_cnt['time'].values * 100) % week_duration).astype(np.int64)\ntrain_time_cnt\n# Week_part sẽ lặp lại cứ mỗi chu kỳ 1679 đi qua","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(len(set(train_time_cnt.time)))\nprint(len(set(train_time_cnt.week_part)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"x = 32\ny = 15\nprint(x % y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"x = 47\ny = 15\nprint(x % y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(0.01*100 % week_duration)\nprint(10*100 % week_duration)\nprint(1718*100 % week_duration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(train_time_cnt.time.values, train_time_cnt.cnt.values, 'b.',\n         alpha=0.5, label='train count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Aggregate weekly stats\ntrain_week_part = train_time_cnt.groupby(['week_part'])[['cnt']].sum().reset_index()\ntrain_week_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(train_week_part.week_part.values, train_week_part.cnt.values, 'b.',\n         alpha=0.5, label='train count')\n# Gộp toàn bộ count station theo tuần rồi plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"y_train = train_week_part['cnt'].rolling(window=20, center=True).mean().values\ny_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.plot(train_week_part.week_part.values, y_train, 'b-', linewidth=4, alpha=0.5, label='train count smooth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"week_duration = 1679\ntrain_time_cnt['week_part'] = ((train_time_cnt['time'].values * 100) % week_duration).astype(np.int64)\n# Aggregate weekly stats\ntrain_week_part = train_time_cnt.groupby(['week_part'])[['cnt']].sum().reset_index()\nfig = plt.figure()\nplt.plot(train_week_part.week_part.values, train_week_part.cnt.values, 'b.', alpha=0.5, label='train count')\ny_train = train_week_part['cnt'].rolling(window=20, center=True).mean().values\nplt.plot(train_week_part.week_part.values, y_train, 'b-', linewidth=4, alpha=0.5, label='train count smooth')\nplt.title('Relative Part of week')\nplt.ylabel('Number of records')\nplt.xlim(0, 1680)\nfig.savefig('week_duration.png', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 69% failure rate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Station combinations\nWe have seen station 32 has high (4.7%) error rate.\n\nLet's investigate that failure rate with station combinations.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read station and response data\n['S32', 'S33', 'S34'] have the most interesting pattern.\n\nWe read the full train set although only 5 columns.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date_part = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', nrows=10000)\ntrain_date_part.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"date_cols = train_date_part.drop('Id', axis=1).count().reset_index().sort_values(by=0, ascending=False)\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"date_cols['station'] = date_cols['index'].apply(lambda s: s.split('_')[1])\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"STATIONS = ['S32', 'S33', 'S34']\ndate_cols = date_cols[date_cols['station'].isin(STATIONS)]\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"date_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', usecols=['Id'] + date_cols)\nprint(train_date.columns)\nprint(train_date.shape)\ntrain_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"STATIONS = ['S32', 'S33', 'S34']\ntrain_date.columns = ['Id'] + STATIONS\ntrain_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"for station in STATIONS:\n    train_date[station] = 1 * (train_date[station] >= 0)\ntrain_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"response = pd.read_csv('../input/bosch-production-line-performance/train_numeric.csv.zip', usecols=['Id', 'Response'])\nprint(response.shape)\nresponse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train = response.merge(train_date, how='left', on='Id')\nprint(train.shape)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.Response.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DNN WAY","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"nrows=100000 # total full row: 1,183,747\ntrain_date= pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', nrows=nrows)\nprint(train_date.shape)\ntrain_numeric = pd.read_csv('../input/bosch-production-line-performance/train_numeric.csv.zip', nrows=nrows)\nprint(train_numeric.shape)\n#train_categorical = pd.read_csv('../input/bosch-production-line-performance/train_categorical.csv.zip', nrows=nrows)\n#print(train_categorical.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_numeric.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train = train_date.merge(train_numeric, how='left', on='Id')\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(train.Response.value_counts(normalize=True))\nprint(train.Response.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill na with Mean value or use XGBoost can handle nan value","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost from Kaggle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/joconnor/python-xgboost-starter-0-209-public-mcc","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import matthews_corrcoef, roc_auc_score\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# I'm limited by RAM here and taking the first N rows is likely to be\n# a bad idea for the date data since it is ordered.\n# Sample the data in a roundabout way:\ndate_chunks = pd.read_csv(\"../input/bosch-production-line-performance/train_date.csv.zip\", index_col=0, chunksize=100000, dtype=np.float32)\nnum_chunks = pd.read_csv(\"../input/bosch-production-line-performance/train_numeric.csv.zip\", index_col=0,\n                         usecols=list(range(969)), chunksize=100000, dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X = pd.concat([pd.concat([dchunk, nchunk], axis=1).sample(frac=0.05)\n               for dchunk, nchunk in zip(date_chunks, num_chunks)])\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\ny = pd.read_csv(\"../input/bosch-production-line-performance/train_numeric.csv.zip\", index_col=0, usecols=[0,969], dtype=np.float32)\\\n.loc[X.index].values.ravel()\ny\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"len(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"clf = XGBClassifier(base_score=0.005)\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# threshold for a manageable number of features\nplt.hist(clf.feature_importances_[clf.feature_importances_>0])\nimportant_indices = np.where(clf.feature_importances_>0.005)[0]\nprint(important_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# load entire dataset for these features. \n# note where the feature indices are split so we can load the correct ones straight from read_csv\nn_date_features = 1156\nX = np.concatenate([\n    pd.read_csv(\"../input/bosch-production-line-performance/train_date.csv.zip\", index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices < n_date_features] + 1])).values,\n    pd.read_csv(\"../input/bosch-production-line-performance/train_numeric.csv.zip\", index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices >= n_date_features] + 1 - 1156])).values\n], axis=1)\ny = pd.read_csv(\"../input/bosch-production-line-performance/train_numeric.csv.zip\", index_col=0, dtype=np.float32, usecols=[0,969]).values.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.DataFrame(y)[0].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"clf = XGBClassifier(max_depth=5, base_score=0.005)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from collections import Counter\n# count examples in each class\ncounter = Counter(y)\n# estimate scale_pos_weight value\nestimate = counter[0] / counter[1]\nprint('Estimate: %.3f' % estimate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"clf = XGBClassifier(scale_pos_weight=171)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"clf = XGBClassifier(scale_pos_weight=250)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(roc_auc_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/aakashveera/random-forest","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tqdm\nimport gc\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', nrows=10000)\nnumeric = pd.read_csv('../input/bosch-production-line-performance/train_numeric.csv.zip', nrows=10000)\ncategory = pd.read_csv('../input/bosch-production-line-performance/train_categorical.csv.zip', nrows=10000)\n# Mục đích đọc 10000 dòng là để lấy các thông tin về station name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FEATURE ENGINEERING\n\nThe list of numeric features is selected based on the other XGBOOST classifier check the numericclassifier notebook","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feats = ['Id',\n       'L3_S30_F3514', 'L0_S9_F200', 'L3_S29_F3430', 'L0_S11_F314',\n       'L0_S0_F18', 'L3_S35_F3896', 'L0_S12_F350', 'L3_S36_F3918',\n       'L0_S0_F20', 'L3_S30_F3684', 'L1_S24_F1632', 'L0_S2_F48',\n       'L3_S29_F3345', 'L0_S18_F449', 'L0_S21_F497', 'L3_S29_F3433',\n       'L3_S30_F3764', 'L0_S1_F24', 'L3_S30_F3554', 'L0_S11_F322',\n       'L3_S30_F3564', 'L3_S29_F3327', 'L0_S2_F36', 'L0_S9_F180',\n       'L3_S33_F3855', 'L0_S0_F4', 'L0_S21_F477', 'L0_S5_F114',\n       'L0_S6_F122', 'L1_S24_F1122', 'L0_S9_F165', 'L0_S18_F439',\n       'L1_S24_F1490', 'L0_S6_F132', 'L3_S29_F3379', 'L3_S29_F3336',\n       'L0_S3_F80', 'L3_S30_F3749', 'L1_S24_F1763', 'L0_S10_F219',\n 'Response']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = date.drop('Id', axis=1).count()\ndate_cols = length.reset_index().sort_values(by=0, ascending=False)\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stations = sorted(date_cols['index'].str.split('_',expand=True)[1].unique().tolist())\nstations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(stations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols['station'] = date_cols['index'].str.split('_',expand=True)[1]\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Chỉ giữ lại duy nhất các unique station column, tương ứng với feature measurement và line","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = None\nfor chunk in pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip',usecols=['Id'] + date_cols,chunksize=50000,low_memory=False):\n\n    chunk.columns = ['Id'] + stations\n    chunk['start_station'] = -1\n    chunk['end_station'] = -1\n    \n    for s in stations:\n        chunk[s] = 1 * (chunk[s] >= 0)\n        id_not_null = chunk[chunk[s] == 1].Id\n        chunk.loc[(chunk['start_station']== -1) & (chunk.Id.isin(id_not_null)),'start_station'] = int(s[1:])\n        chunk.loc[chunk.Id.isin(id_not_null),'end_station'] = int(s[1:])   \n    data = pd.concat([data, chunk])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['Id','start_station','end_station']]\nusefuldatefeatures = ['Id']+date_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(date_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"usefuldatefeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(chunk.columns.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minmaxfeatures = None\nfor chunk in pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip',usecols=usefuldatefeatures,chunksize=50000,low_memory=False):\n    features = chunk.columns.values.tolist()\n    features.remove('Id')\n    df_mindate_chunk = chunk[['Id']].copy()\n    df_mindate_chunk['mindate'] = chunk[features].min(axis=1).values\n    df_mindate_chunk['maxdate'] = chunk[features].max(axis=1).values\n    df_mindate_chunk['min_time_station'] =  chunk[features].idxmin(axis = 1).apply(lambda s: int(s.split('_')[1][1:]) if s is not np.nan else -1)\n    df_mindate_chunk['max_time_station'] =  chunk[features].idxmax(axis = 1).apply(lambda s: int(s.split('_')[1][1:]) if s is not np.nan else -1)\n    minmaxfeatures = pd.concat([minmaxfeatures, df_mindate_chunk])\n\ndel chunk\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mindate_chunk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minmaxfeatures.sort_values(by=['mindate', 'Id'], inplace=True)\nminmaxfeatures['min_Id_rev'] = -minmaxfeatures.Id.diff().shift(-1)\nminmaxfeatures['min_Id'] = minmaxfeatures.Id.diff()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minmaxfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [['Id']+date_cols,num_feats]\ntraindata = None\ntrainfiles = ['train_date.csv.zip','train_numeric.csv.zip']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,f in enumerate(trainfiles):\n    \n    subset = None\n    \n    for chunk in pd.read_csv('../input/bosch-production-line-performance/' + f,usecols=cols[i],chunksize=100000,low_memory=False):\n        subset = pd.concat([subset, chunk])\n    \n    if traindata is None:\n        traindata = subset.copy()\n    else:\n        traindata = pd.merge(traindata, subset.copy(), on=\"Id\")\n        \ndel subset,chunk\ngc.collect()\ndel cols[1][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata = traindata.merge(minmaxfeatures, on='Id')\ntraindata = traindata.merge(data, on='Id')\ndel minmaxfeatures,data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata.fillna(value=0,inplace=True)\ntraindata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mcc(tp, tn, fp, fn):\n    num = tp * tn - fp * fn\n    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    if den == 0:\n        return 0\n    else:\n        return num / np.sqrt(den)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_mcc(y_true, y_prob):\n    idx = np.argsort(y_prob)\n    y_true_sort = y_true[idx]\n    n = y_true.shape[0]\n    nump = 1.0 * np.sum(y_true) \n    numn = n - nump \n    tp,fp = nump,numn\n    tn,fn = 0.0,0.0\n    best_mcc = 0.0\n    best_id = -1\n    mccs = np.zeros(n)\n    for i in range(n):\n        if y_true_sort[i] == 1:\n            tp -= 1.0\n            fn += 1.0\n        else:\n            fp -= 1.0\n            tn += 1.0\n        new_mcc = mcc(tp, tn, fp, fn)\n        mccs[i] = new_mcc\n        if new_mcc >= best_mcc:\n            best_mcc = new_mcc\n            best_id = i\n    return best_mcc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mcc_eval(y_prob, dtrain):\n    y_true = dtrain.get_label()\n    best_mcc = eval_mcc(y_true, y_prob)\n    return 'MCC', best_mcc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions(suppress=True)\nimport gc\n# lấy random 400,000 sample với Response ==0\ntotal = traindata[traindata['Response']==0].sample(frac=1).head(400000) \n# Sau đó gộp với 6879 sample với Response ==1, trộn ngẫu nhiên toàn bộ mẫu này\ntotal = pd.concat([total,traindata[traindata['Response']==1]]).sample(frac=1)\ntotal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX,y = total.drop(['Response','Id'],axis=1),total['Response']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score,precision_score,plot_precision_recall_curve\nfrom sklearn.metrics import confusion_matrix,classification_report,matthews_corrcoef","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=500,n_jobs=-1,verbose=1,random_state=11)\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(pred,y_test))\nprint(matthews_corrcoef(y_test,pred))\nconfusion_matrix(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_precision_recall_curve(model,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}