{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes.\n\nBecause Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods.\n\nIn this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Submissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by:\n\nMCC=(TP∗TN)−(FP∗FN)(TP+FP)(TP+FN)(TN+FP)(TN+FN)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√,\n\n\nwhere TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data Description\n\nThe data for this competition represents measurements of parts as they move through Bosch's production lines. Each part has a unique Id. The goal is to predict which parts will fail quality control (represented by a 'Response' = 1).\n\nThe dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939.\n\nOn account of the large size of the dataset, we have separated the files by the type of feature they contain: numerical, categorical, and finally, a file with date features. The date features provide a timestamp for when each measurement was taken. Each date column ends in a number that corresponds to the previous feature number. E.g. the value of L0_S0_D1 is the time at which L0_S0_F0 was taken.\n\nIn addition to being one of the largest datasets (in terms of number of features) ever hosted on Kaggle, the ground truth for this competition is highly imbalanced. Together, these two attributes are expected to make this a challenging problem.\n\nFile descriptions\n\n- train_numeric.csv - the training set numeric features (this file contains the 'Response' variable)\n- test_numeric.csv - the test set numeric features (you must predict the 'Response' for these Ids)\n- train_categorical.csv - the training set categorical features\n- test_categorical.csv - the test set categorical features\n- train_date.csv - the training set date features\n- test_date.csv - the test set date features\n- sample_submission.csv - a sample submission file in the correct format","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Giai thich dataset:\n1. Moi line se co mot so station, id cua tung station la rieng biet o tung line, do do chi can biet part nam o station nao la duoc roi, sau do se tu truy ra line nao\n2. Moi part khi di qua station se duoc do cac feature tuong ung tai mot thoi gian cu the, do do column se theo ID (part), time (thoi gian do), L3_S36_f3939: part do vao thoi gian 87.2 dang o line 3, station 36, do feature 3939\n3. Tai 1 station part se do nhieu feature khac nhau , thoi gian do tat ca feature cua part do tai station do la nhu nhau","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_date_part = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', nrows=10000)\nprint(train_date_part.shape)\ntrain_date_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the min and max times for each station\n# Tach tung cot ra, sau do dropna tung cot, roi append tat ca lai voi nhau\n    \ndates=train_date_part.copy()\nwithId=True\ntimes = []\ncols = list(dates.columns)\nif 'Id' in cols:\n    cols.remove('Id')\nfor feature_name in cols:\n    if withId:\n        df = dates[['Id', feature_name]].copy()\n        df.columns = ['Id', 'time']\n    else:\n        df = dates[[feature_name]].copy()\n        df.columns = ['time']\n    df['line'] = feature_name.split('_')[0][1:]\n    df['station'] = feature_name.split('_')[1][1:]\n    df['feature_number'] = feature_name.split('_')[2][1:]\n    df = df.dropna()\n    print(df.shape)\n    times.append(df)\n    print(len(times))\nstation_times=pd.concat(times)\nstation_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"station_times=station_times.sort_values(by=['Id','station'])\nstation_times['line']=station_times['line'].astype('int64')\nstation_times['station']=station_times['station'].astype('int64')\nstation_times['feature_number']=station_times['feature_number'].astype('int64')\nprint(station_times.dtypes)\nstation_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many station in each line ?\nset(station_times[station_times.line==0].station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many station in each time ?\nprint(set(station_times[station_times.time==82.24].station))\n# How many part in each time ?\nprint(set(station_times[station_times.time==82.24].Id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time=82.24\nprint('time: ',time)\nstation_=set(station_times[station_times.time==82.24].station)\nfor j in station_:\n    print('station:', j)\n    print('part: ',set(station_times[(station_times.time==time) & (station_times.station==j)].Id))\n    print('feature_number: ',set(station_times[(station_times.time==time) & (station_times.station==j)].feature_number))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time=1379.78\nprint('time: ',time)\nstation_=set(station_times[station_times.time==82.24].station)\nfor j in station_:\n    print('station:', j)\n    print('part: ',set(station_times[(station_times.time==time) & (station_times.station==j)].Id))\n    print('feature_number: ',set(station_times[(station_times.time==time) & (station_times.station==j)].feature_number))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(station_times[station_times.line==1].station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(station_times[station_times.line==2].station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(station_times[station_times.line==3].station)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each station is unique in each line, so no need to include line here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"station_times.line.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(station_times.Id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"station_times.station.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"station_times.feature_number.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"part_id=6\npart_filter=station_times[station_times.Id==part_id]\npart_filter_line=set(part_filter.line)\npart_filter_station=set(part_filter.station)\nprint('total_line: ',part_filter_line)\nprint('total_station: ',part_filter_station)\n\nfor i in part_filter_line:\n    print('line:', i)\n    for j in part_filter_station:\n        print('station:', j)\n        print('feature_number: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].feature_number))\n        print('time: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].time))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def part_info(part_id):\n    #part_id=part_id\n    part_filter=station_times[station_times.Id==part_id]\n    part_filter_line=set(part_filter.line)\n    part_filter_station=set(part_filter.station)\n    print('total_line: ',part_filter_line)\n    print('total_station: ',part_filter_station)\n    print('-'*60)\n\n    for i in part_filter_line:\n        print('-'*10)\n        print('line:', i)\n        for j in part_filter_station:\n            print('station:', j)\n            print('feature_number: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].feature_number))\n            print('time: ',set(part_filter[(part_filter.line==i) & (part_filter.station==j)].time))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"part_info(120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_station_times = station_times.groupby(['Id', 'station']).min()['time']\nmax_station_times = station_times.groupby(['Id', 'station']).max()['time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_station_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_station_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_date_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_date_part.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read station times for train and test\ndate_cols = train_date_part.drop('Id', axis=1).count().reset_index().\\\n            sort_values(by=0, ascending=False)\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols['station'] = date_cols['index'].apply(lambda s: s.split('_')[1])\ndate_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\ndate_cols # selected features\n# remove all duplicate station (with differtion feature measurment each station)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# applied these columns to all training data set\ntrain_date = pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip', usecols=date_cols)\nprint(train_date.shape)\ntrain_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates=train_date.copy()\nwithId=False\ntimes = []\ncols = list(dates.columns)\nif 'Id' in cols:\n    cols.remove('Id')\nfor feature_name in cols:\n    if withId:\n        df = dates[['Id', feature_name]].copy()\n        df.columns = ['Id', 'time']\n    else:\n        df = dates[[feature_name]].copy()\n        df.columns = ['time']\n    df['line'] = feature_name.split('_')[0][1:]\n    df['station'] = feature_name.split('_')[1][1:]\n    df['feature_number'] = feature_name.split('_')[2][1:]\n    df = df.dropna()\n    #print(df.shape)\n    times.append(df)\n    #print(len(times))\ntrain_station_times=pd.concat(times)\nprint(train_station_times.shape)\ntrain_station_times\n# Do chi giu lai 52 columns nen tong so dong 14 tr, khong qua nhieu, neu giu lai 1000 columns thi\n# con so se rat lon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_time_cnt = train_station_times.groupby('time').count()[['station']].reset_index()\ntrain_time_cnt.columns = ['time', 'cnt']\nprint(train_time_cnt.shape)\ntrain_time_cnt\n# Loc thoi gian testing tung feature ung voi bao nhieu station.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_date = pd.read_csv('../input/bosch-production-line-performance/test_date.csv.zip', usecols=date_cols)\nprint(test_date.shape)\ntest_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates=test_date.copy()\nwithId=False\ntimes = []\ncols = list(dates.columns)\nif 'Id' in cols:\n    cols.remove('Id')\nfor feature_name in cols:\n    if withId:\n        df = dates[['Id', feature_name]].copy()\n        df.columns = ['Id', 'time']\n    else:\n        df = dates[[feature_name]].copy()\n        df.columns = ['time']\n    df['line'] = feature_name.split('_')[0][1:]\n    df['station'] = feature_name.split('_')[1][1:]\n    df['feature_number'] = feature_name.split('_')[2][1:]\n    df = df.dropna()\n    #print(df.shape)\n    times.append(df)\n    #print(len(times))\ntest_station_times=pd.concat(times)\nprint(test_station_times.shape)\ntest_station_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_time_cnt = test_station_times.groupby('time').count()[['station']].reset_index()\ntest_time_cnt.columns = ['time', 'cnt']\nprint(test_time_cnt.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure()\nplt.plot(train_time_cnt['time'].values, train_time_cnt['cnt'].values, 'b.', alpha=0.1, label='train')\nplt.plot(test_time_cnt['time'].values, test_time_cnt['cnt'].values, 'r.', alpha=0.1, label='test')\nplt.title('Original date values')\nplt.ylabel('Number of records')\nplt.xlabel('Time')\nfig.savefig('original_date_values.png', dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((train_time_cnt['time'].min(), train_time_cnt['time'].max()))\nprint((test_time_cnt['time'].min(), test_time_cnt['time'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few observations:\n\n- Train and test set has the same time period\n- There is a clear periodic pattern\n- The dates are transformed to 0 - 1718 with granularity of 0.01\n- There is a gap in the middle\n\nCould we figure out what does 0.01 mean? Let's check a few auto correlations!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"time_ticks = np.arange(train_time_cnt['time'].min(), train_time_cnt['time'].max() + 0.01, 0.01)\ntime_ticks = pd.DataFrame({'time': time_ticks})\ntime_ticks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_ticks = pd.merge(time_ticks, train_time_cnt, how='left', on='time')\ntime_ticks = time_ticks.fillna(0)\ntime_ticks\n# Dem bao nhieu station lien quan toi specific time trong toan bo data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Autocorrelation\nx = time_ticks['cnt'].values\nmax_lag = 8000\nauto_corr_ks = range(1, max_lag)\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nauto_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(auto_corr_ks))\nprint(auto_corr_ks)\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=1\nprint('k',k)\nprint(len(x[:-k]))\nprint(len(x[k:]))\nprint(x[:-k])\nprint(x[k:])\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:]))\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:])[0,1])\nprint('corrcoef: \\n',np.array([1]+ np.corrcoef(x[:-k], x[k:])[0,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=3\nprint('k',k)\nprint(len(x[:-k]))\nprint(len(x[k:]))\nprint(x[:-k])\nprint(x[k:])\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:]))\nprint('corrcoef: \\n',np.corrcoef(x[:-k], x[k:])[0,1])\nprint('corrcoef: \\n',np.array([1]+ np.corrcoef(x[:-k], x[k:])[0,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nplt.plot(auto_corr, 'k.', label='autocorrelation by 0.01')\nplt.title('Train Sensor Time Auto-correlation')\nperiod = 25\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'go', alpha=0.5, label='strange autocorrelation at 0.25')\nperiod = 1675\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'ro', markersize=10, alpha=0.5, label='one week = 16.75?')\nplt.xlabel('k * 0.01 -  autocorrelation lag')\nplt.ylabel('autocorrelation')\nplt.legend(loc=0)\n#fig.savefig('train_time_auto_correlation.png', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest peaks are at approximately 1680 ticks. Let's call it a week ;)\n\nIn each week we could see 7 local maxima ~ days.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}