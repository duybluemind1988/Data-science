{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abnomaly_detection_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZ55GpNfoScuBGSiv63bxG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duybluemind1988/Data-science/blob/master/Abnomaly_detection_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewFxpqZOfGiW"
      },
      "source": [
        "https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbL3punsfMXQ",
        "outputId": "3b92f536-c013-4e66-e4b6-e2a40235d634"
      },
      "source": [
        "!pip install pyod"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyod\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/49/e4a8e94bfe53e3b80eeda468c3b930f7d78209c8184523e84b7529b44aa4/pyod-0.8.5.tar.gz (98kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 40kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 71kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 4.0MB/s \n",
            "\u001b[?25hCollecting combo\n",
            "  Downloading https://files.pythonhosted.org/packages/12/ae/66029dcaa88ccca77f454dbb29c1178c751ec24fc771ed475a992b49a02d/combo-0.1.2.tar.gz\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyod) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pyod) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from pyod) (1.19.4)\n",
            "Requirement already satisfied: numba>=0.35 in /usr/local/lib/python3.6/dist-packages (from pyod) (0.48.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.6/dist-packages (from pyod) (1.1.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from pyod) (1.4.1)\n",
            "Requirement already satisfied: scikit_learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from pyod) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyod) (1.15.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from pyod) (0.10.2)\n",
            "Collecting suod\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/87/9170cabe1b5e10a7d095c0e28f2e30e7c1886a13f063de85d3cfacc06f4b/suod-0.0.4.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyod) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.35->pyod) (51.0.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.35->pyod) (0.31.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->pyod) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->pyod) (0.5.1)\n",
            "Building wheels for collected packages: pyod, combo, suod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-0.8.5-cp36-none-any.whl size=112038 sha256=5fe265493a7acd091bdb5c5d54ae8d8e0565fae339359b67a73bf8604a8cb848\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/2d/37/2c099f1b61c7f47b6954ae429ffea7de5c897e84ec4b876cd2\n",
            "  Building wheel for combo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for combo: filename=combo-0.1.2-cp36-none-any.whl size=42028 sha256=96fc9c217b0c7d6726d6db046052036ca4319fc5552633759d13205e0f83f520\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/d9/bf/d1a371a5f0844cd8a53c04c14daa89974c93f429dda9dceb86\n",
            "  Building wheel for suod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for suod: filename=suod-0.0.4-cp36-none-any.whl size=2167158 sha256=cda96c6701554171066dee528454a7e22767caaa7f6fa19355f56b5554904b64\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/55/e5/a4fca65bba231f6d0115059b589148774b41faea25b3f2aa27\n",
            "Successfully built pyod combo suod\n",
            "Installing collected packages: combo, suod, pyod\n",
            "Successfully installed combo-0.1.2 pyod-0.8.5 suod-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2iXo1qResc8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.auto_encoder import AutoEncoder\n",
        "from pyod.utils.data import generate_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4LLZNxefGL-",
        "outputId": "c18ae717-cabb-4578-8a50-cbe76741f9f8"
      },
      "source": [
        "contamination = 0.1  # percentage of outliers\n",
        "n_train = 500  # number of training points\n",
        "n_test = 500  # number of testing points\n",
        "n_features = 25 # Number of features\n",
        "\n",
        "X_train, y_train, X_test, y_test = generate_data(\n",
        "    n_train=n_train, n_test=n_test,\n",
        "    n_features= n_features, \n",
        "    contamination=contamination,random_state=1234)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyod/utils/data.py:189: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.8.0. Please use behaviour=\"new\", which makes the returned datasets in the order of X_train, X_test, y_train, y_test.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf7iYnSpfSgW"
      },
      "source": [
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X29W7oXCfSlA",
        "outputId": "b1cbb772-4c42-4090-b293-6a642ccf2650"
      },
      "source": [
        "print(X_train.shape,X_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 25) (500, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMeWm03BfSnd"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)\n",
        "X_test = pd.DataFrame(X_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LNK4yRJpfgmy",
        "outputId": "44212da9-2f56-4ede-f3b9-4078fd6ead69"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(2)\n",
        "x_pca = pca.fit_transform(X_train)\n",
        "x_pca = pd.DataFrame(x_pca)\n",
        "x_pca.columns=['PC1','PC2']\n",
        "x_pca"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.374289</td>\n",
              "      <td>-0.290809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.430553</td>\n",
              "      <td>0.508865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.416714</td>\n",
              "      <td>1.039612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.158238</td>\n",
              "      <td>-0.018910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.874365</td>\n",
              "      <td>0.223768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>12.494752</td>\n",
              "      <td>-3.779057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>12.720427</td>\n",
              "      <td>0.451819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>11.830232</td>\n",
              "      <td>-0.179812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>10.370091</td>\n",
              "      <td>3.797218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>10.097291</td>\n",
              "      <td>-2.523826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           PC1       PC2\n",
              "0    -1.374289 -0.290809\n",
              "1    -1.430553  0.508865\n",
              "2    -1.416714  1.039612\n",
              "3    -1.158238 -0.018910\n",
              "4    -0.874365  0.223768\n",
              "..         ...       ...\n",
              "495  12.494752 -3.779057\n",
              "496  12.720427  0.451819\n",
              "497  11.830232 -0.179812\n",
              "498  10.370091  3.797218\n",
              "499  10.097291 -2.523826\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCZQ-9zkfgkJ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(2)\n",
        "x_pca = pca.fit_transform(X_train)\n",
        "x_pca = pd.DataFrame(x_pca)\n",
        "x_pca.columns=['PC1','PC2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkbZmc4ufs5N"
      },
      "source": [
        "The purple points clustering together are the “normal” observations, and the yellow points are the outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ggNq98nzfghl",
        "outputId": "748393e7-b780-4ead-bf06-406e8b169fdc"
      },
      "source": [
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x_pca[\"PC1\"], x_pca[\"PC2\"], c=y_train, alpha=0.8)\n",
        "plt.title('Scatter plot')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8nnBJjLhprC"
      },
      "source": [
        "# Model 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkgxs3Q4hrd0"
      },
      "source": [
        "    Model 1: [25, 2, 2, 25]. The input layer and the output layer has 25 neurons each. There are two hidden layers, each has two neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy0vylEnhjxi",
        "outputId": "3c374d5e-be97-43d9-ee5b-8e58d90d6c73"
      },
      "source": [
        "clf1 = AutoEncoder(hidden_neurons =[25, 2, 2, 25])\n",
        "clf1.fit(X_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 52        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 25)                75        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 25)                650       \n",
            "=================================================================\n",
            "Total params: 2,733\n",
            "Trainable params: 2,733\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "15/15 [==============================] - 2s 27ms/step - loss: 3.6854 - val_loss: 3.1544\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 3.2935 - val_loss: 2.7557\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.8364 - val_loss: 2.5539\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.4778 - val_loss: 2.4183\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.4852 - val_loss: 2.3273\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.3188 - val_loss: 2.2639\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1120 - val_loss: 2.2146\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.3164 - val_loss: 2.1705\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.0116 - val_loss: 2.1334\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.9566 - val_loss: 2.0991\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9823 - val_loss: 2.0676\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1354 - val_loss: 2.0386\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.8946 - val_loss: 2.0116\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8134 - val_loss: 1.9840\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.9363 - val_loss: 1.9592\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.8714 - val_loss: 1.9365\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 1.7946 - val_loss: 1.9158\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7841 - val_loss: 1.8953\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7194 - val_loss: 1.8755\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7796 - val_loss: 1.8560\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6712 - val_loss: 1.8377\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6849 - val_loss: 1.8198\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7790 - val_loss: 1.8023\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8410 - val_loss: 1.7851\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6718 - val_loss: 1.7683\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.7657 - val_loss: 1.7526\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6666 - val_loss: 1.7376\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6922 - val_loss: 1.7237\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6164 - val_loss: 1.7097\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6760 - val_loss: 1.6965\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5765 - val_loss: 1.6836\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5116 - val_loss: 1.6709\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7710 - val_loss: 1.6585\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5155 - val_loss: 1.6471\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.4851 - val_loss: 1.6355\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5667 - val_loss: 1.6234\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6453 - val_loss: 1.6124\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3750 - val_loss: 1.6018\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4260 - val_loss: 1.5911\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4838 - val_loss: 1.5811\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5724 - val_loss: 1.5714\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4214 - val_loss: 1.5625\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4141 - val_loss: 1.5526\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5149 - val_loss: 1.5432\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3378 - val_loss: 1.5344\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5193 - val_loss: 1.5258\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4640 - val_loss: 1.5178\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5401 - val_loss: 1.5093\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4583 - val_loss: 1.5014\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3880 - val_loss: 1.4939\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3939 - val_loss: 1.4861\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3922 - val_loss: 1.4789\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3562 - val_loss: 1.4719\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4487 - val_loss: 1.4650\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3388 - val_loss: 1.4584\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4549 - val_loss: 1.4517\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2920 - val_loss: 1.4455\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4397 - val_loss: 1.4391\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2684 - val_loss: 1.4329\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3096 - val_loss: 1.4267\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1409 - val_loss: 1.4207\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3233 - val_loss: 1.4149\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3604 - val_loss: 1.4092\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.2127 - val_loss: 1.4036\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2173 - val_loss: 1.3982\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2875 - val_loss: 1.3928\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2887 - val_loss: 1.3876\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3977 - val_loss: 1.3824\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2538 - val_loss: 1.3775\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4154 - val_loss: 1.3725\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2849 - val_loss: 1.3678\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2519 - val_loss: 1.3630\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2337 - val_loss: 1.3575\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2542 - val_loss: 1.3526\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1081 - val_loss: 1.3482\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1911 - val_loss: 1.3439\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3153 - val_loss: 1.3396\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3170 - val_loss: 1.3355\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3196 - val_loss: 1.3314\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1044 - val_loss: 1.3275\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2380 - val_loss: 1.3232\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1362 - val_loss: 1.3194\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2008 - val_loss: 1.3155\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3368 - val_loss: 1.3118\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2524 - val_loss: 1.3082\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2757 - val_loss: 1.3046\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3935 - val_loss: 1.3011\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1455 - val_loss: 1.2977\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2889 - val_loss: 1.2943\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2024 - val_loss: 1.2910\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2492 - val_loss: 1.2877\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1587 - val_loss: 1.2844\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1450 - val_loss: 1.2810\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1664 - val_loss: 1.2776\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1335 - val_loss: 1.2746\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0766 - val_loss: 1.2715\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2322 - val_loss: 1.2686\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2517 - val_loss: 1.2656\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2349 - val_loss: 1.2626\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1043 - val_loss: 1.2599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(batch_size=32, contamination=0.1, dropout_rate=0.2, epochs=100,\n",
              "      hidden_activation='relu', hidden_neurons=[25, 2, 2, 25],\n",
              "      l2_regularizer=0.1,\n",
              "      loss=<function mean_squared_error at 0x7f7c9cc44488>,\n",
              "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
              "      random_state=None, validation_size=0.1, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3IT9i3Ohjuq"
      },
      "source": [
        "# Get the outlier scores for the train data\n",
        "y_train_scores = clf1.decision_scores_  \n",
        "y_train_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEe-T4Suhjr-",
        "outputId": "acc14cd3-0cab-4fe5-e10b-68cfec163278"
      },
      "source": [
        "# Predict the anomaly scores\n",
        "y_test_scores = clf1.decision_function(X_test)  # outlier scores\n",
        "y_test_scores = pd.Series(y_test_scores)\n",
        "y_test_scores"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2.269521\n",
              "1       2.787287\n",
              "2       2.775147\n",
              "3       2.932246\n",
              "4       2.739236\n",
              "         ...    \n",
              "495    13.221239\n",
              "496    16.189307\n",
              "497    15.799958\n",
              "498    13.040258\n",
              "499    16.392447\n",
              "Length: 500, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "q4BxQwWNfge_",
        "outputId": "1f8bcf6d-6238-465e-f025-cfac816f3a7d"
      },
      "source": [
        "# Plot it!\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_test_scores, bins='auto')  \n",
        "plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMGoJNNiO_8"
      },
      "source": [
        "if we use a histogram to count the frequency by the anomaly score, we will see the high scores corresponds to low frequency — the evidence of outliers. We choose 4.0 to be the cut point and those >=4.0 to be outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SgugKisfSqH",
        "outputId": "4c46333a-dd29-48cc-ea57-20d53721c83f"
      },
      "source": [
        "df_test = X_test.copy()\n",
        "df_test['score'] = y_test_scores\n",
        "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
        "df_test['cluster'].value_counts()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    450\n",
              "1     50\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "LbSy5xqkiv9_",
        "outputId": "be9a2ad9-e2f9-4f59-bedc-960f248cc84c"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>score</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.490850</td>\n",
              "      <td>0.599752</td>\n",
              "      <td>0.123036</td>\n",
              "      <td>0.124200</td>\n",
              "      <td>0.260621</td>\n",
              "      <td>0.535060</td>\n",
              "      <td>0.054199</td>\n",
              "      <td>-0.593279</td>\n",
              "      <td>0.820037</td>\n",
              "      <td>-0.050190</td>\n",
              "      <td>0.883886</td>\n",
              "      <td>0.107319</td>\n",
              "      <td>0.118136</td>\n",
              "      <td>0.565775</td>\n",
              "      <td>0.479314</td>\n",
              "      <td>0.369343</td>\n",
              "      <td>0.162560</td>\n",
              "      <td>0.083913</td>\n",
              "      <td>-0.344387</td>\n",
              "      <td>0.490514</td>\n",
              "      <td>0.848968</td>\n",
              "      <td>0.919533</td>\n",
              "      <td>1.162120</td>\n",
              "      <td>-0.651528</td>\n",
              "      <td>-0.138860</td>\n",
              "      <td>2.269521</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.953400</td>\n",
              "      <td>-0.669948</td>\n",
              "      <td>0.472268</td>\n",
              "      <td>0.141471</td>\n",
              "      <td>0.449857</td>\n",
              "      <td>0.154775</td>\n",
              "      <td>0.480871</td>\n",
              "      <td>-0.335910</td>\n",
              "      <td>-0.587196</td>\n",
              "      <td>0.809817</td>\n",
              "      <td>0.949703</td>\n",
              "      <td>-0.151841</td>\n",
              "      <td>-0.233898</td>\n",
              "      <td>0.200221</td>\n",
              "      <td>0.412201</td>\n",
              "      <td>-0.903222</td>\n",
              "      <td>-0.158045</td>\n",
              "      <td>1.072411</td>\n",
              "      <td>-0.317343</td>\n",
              "      <td>0.758364</td>\n",
              "      <td>-0.029758</td>\n",
              "      <td>0.332330</td>\n",
              "      <td>0.496692</td>\n",
              "      <td>-0.546290</td>\n",
              "      <td>0.953353</td>\n",
              "      <td>2.787287</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.556078</td>\n",
              "      <td>0.498778</td>\n",
              "      <td>-0.344837</td>\n",
              "      <td>-0.287162</td>\n",
              "      <td>-0.043985</td>\n",
              "      <td>0.089953</td>\n",
              "      <td>0.499733</td>\n",
              "      <td>0.286055</td>\n",
              "      <td>1.407571</td>\n",
              "      <td>0.279362</td>\n",
              "      <td>0.132371</td>\n",
              "      <td>-0.066449</td>\n",
              "      <td>0.718918</td>\n",
              "      <td>0.413829</td>\n",
              "      <td>0.525852</td>\n",
              "      <td>0.293672</td>\n",
              "      <td>1.233307</td>\n",
              "      <td>-0.081703</td>\n",
              "      <td>0.849033</td>\n",
              "      <td>0.074216</td>\n",
              "      <td>-0.752306</td>\n",
              "      <td>0.167411</td>\n",
              "      <td>1.182533</td>\n",
              "      <td>0.495418</td>\n",
              "      <td>1.212759</td>\n",
              "      <td>2.775147</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.406306</td>\n",
              "      <td>0.563676</td>\n",
              "      <td>0.102176</td>\n",
              "      <td>0.843486</td>\n",
              "      <td>0.169333</td>\n",
              "      <td>1.066628</td>\n",
              "      <td>1.366840</td>\n",
              "      <td>0.286360</td>\n",
              "      <td>-0.598536</td>\n",
              "      <td>0.619118</td>\n",
              "      <td>0.868313</td>\n",
              "      <td>0.561261</td>\n",
              "      <td>-0.640273</td>\n",
              "      <td>0.528745</td>\n",
              "      <td>0.743536</td>\n",
              "      <td>0.756899</td>\n",
              "      <td>-0.022941</td>\n",
              "      <td>-0.493134</td>\n",
              "      <td>-0.085806</td>\n",
              "      <td>-0.247288</td>\n",
              "      <td>1.205646</td>\n",
              "      <td>-0.573057</td>\n",
              "      <td>0.988340</td>\n",
              "      <td>0.633948</td>\n",
              "      <td>0.586926</td>\n",
              "      <td>2.932246</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.113211</td>\n",
              "      <td>-0.131414</td>\n",
              "      <td>-0.428576</td>\n",
              "      <td>0.333657</td>\n",
              "      <td>0.511142</td>\n",
              "      <td>0.367135</td>\n",
              "      <td>0.848195</td>\n",
              "      <td>0.460072</td>\n",
              "      <td>0.069612</td>\n",
              "      <td>1.623246</td>\n",
              "      <td>0.824089</td>\n",
              "      <td>-0.266359</td>\n",
              "      <td>-0.133868</td>\n",
              "      <td>0.302390</td>\n",
              "      <td>0.084762</td>\n",
              "      <td>-0.680006</td>\n",
              "      <td>0.808684</td>\n",
              "      <td>0.520798</td>\n",
              "      <td>0.236163</td>\n",
              "      <td>0.881651</td>\n",
              "      <td>-1.025624</td>\n",
              "      <td>-0.053824</td>\n",
              "      <td>0.212217</td>\n",
              "      <td>-0.332923</td>\n",
              "      <td>0.201611</td>\n",
              "      <td>2.739236</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>-0.458204</td>\n",
              "      <td>-1.480846</td>\n",
              "      <td>-1.627822</td>\n",
              "      <td>-0.128910</td>\n",
              "      <td>-4.662887</td>\n",
              "      <td>-1.095524</td>\n",
              "      <td>-2.854105</td>\n",
              "      <td>-1.497142</td>\n",
              "      <td>-1.821412</td>\n",
              "      <td>-0.714612</td>\n",
              "      <td>-1.523932</td>\n",
              "      <td>-2.759798</td>\n",
              "      <td>-3.310791</td>\n",
              "      <td>-3.218165</td>\n",
              "      <td>-2.359555</td>\n",
              "      <td>-0.679159</td>\n",
              "      <td>-1.035494</td>\n",
              "      <td>-1.497838</td>\n",
              "      <td>-3.766005</td>\n",
              "      <td>-3.471172</td>\n",
              "      <td>-4.264616</td>\n",
              "      <td>-2.567784</td>\n",
              "      <td>-0.424206</td>\n",
              "      <td>-0.694795</td>\n",
              "      <td>-3.791009</td>\n",
              "      <td>13.221239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>-1.948558</td>\n",
              "      <td>-3.832460</td>\n",
              "      <td>-4.083134</td>\n",
              "      <td>-1.589382</td>\n",
              "      <td>-3.721350</td>\n",
              "      <td>-0.082364</td>\n",
              "      <td>-1.667298</td>\n",
              "      <td>-4.400630</td>\n",
              "      <td>-4.036945</td>\n",
              "      <td>-3.895880</td>\n",
              "      <td>-4.782561</td>\n",
              "      <td>-3.113285</td>\n",
              "      <td>-0.845943</td>\n",
              "      <td>-3.496209</td>\n",
              "      <td>-2.020959</td>\n",
              "      <td>-2.502335</td>\n",
              "      <td>-3.539393</td>\n",
              "      <td>-4.321991</td>\n",
              "      <td>-4.137475</td>\n",
              "      <td>-1.527440</td>\n",
              "      <td>-1.607514</td>\n",
              "      <td>-3.059639</td>\n",
              "      <td>0.093796</td>\n",
              "      <td>-0.192333</td>\n",
              "      <td>-2.332015</td>\n",
              "      <td>16.189307</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>-4.604396</td>\n",
              "      <td>-3.606876</td>\n",
              "      <td>-0.127779</td>\n",
              "      <td>-0.022257</td>\n",
              "      <td>-3.586979</td>\n",
              "      <td>-4.035472</td>\n",
              "      <td>-3.359836</td>\n",
              "      <td>-0.555483</td>\n",
              "      <td>-1.940565</td>\n",
              "      <td>-0.753478</td>\n",
              "      <td>-0.451402</td>\n",
              "      <td>-3.390210</td>\n",
              "      <td>-4.380781</td>\n",
              "      <td>-3.279315</td>\n",
              "      <td>-4.392723</td>\n",
              "      <td>-4.179270</td>\n",
              "      <td>-1.644594</td>\n",
              "      <td>-3.801497</td>\n",
              "      <td>-4.385973</td>\n",
              "      <td>-0.116137</td>\n",
              "      <td>-1.365493</td>\n",
              "      <td>-1.237924</td>\n",
              "      <td>-1.253790</td>\n",
              "      <td>-1.015664</td>\n",
              "      <td>-4.335684</td>\n",
              "      <td>15.799958</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>-2.016319</td>\n",
              "      <td>-0.507405</td>\n",
              "      <td>-3.842131</td>\n",
              "      <td>-2.746453</td>\n",
              "      <td>-4.258268</td>\n",
              "      <td>-0.491606</td>\n",
              "      <td>-0.407822</td>\n",
              "      <td>-0.602358</td>\n",
              "      <td>-4.325807</td>\n",
              "      <td>-1.950794</td>\n",
              "      <td>-3.462882</td>\n",
              "      <td>-0.175953</td>\n",
              "      <td>-0.658134</td>\n",
              "      <td>-0.996530</td>\n",
              "      <td>-0.355282</td>\n",
              "      <td>-3.454361</td>\n",
              "      <td>-3.177625</td>\n",
              "      <td>-2.231559</td>\n",
              "      <td>-1.201440</td>\n",
              "      <td>-1.686082</td>\n",
              "      <td>-0.430233</td>\n",
              "      <td>-1.541504</td>\n",
              "      <td>-0.279936</td>\n",
              "      <td>-3.677190</td>\n",
              "      <td>-4.068186</td>\n",
              "      <td>13.040258</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>-1.482275</td>\n",
              "      <td>-0.728515</td>\n",
              "      <td>-4.202111</td>\n",
              "      <td>-1.451220</td>\n",
              "      <td>-3.555855</td>\n",
              "      <td>-3.385711</td>\n",
              "      <td>-1.875587</td>\n",
              "      <td>-1.631211</td>\n",
              "      <td>-0.447071</td>\n",
              "      <td>-4.400840</td>\n",
              "      <td>-2.994574</td>\n",
              "      <td>-4.287925</td>\n",
              "      <td>-4.278156</td>\n",
              "      <td>-0.362837</td>\n",
              "      <td>-3.909343</td>\n",
              "      <td>-0.148451</td>\n",
              "      <td>-4.622288</td>\n",
              "      <td>-2.142854</td>\n",
              "      <td>-2.623058</td>\n",
              "      <td>-4.422792</td>\n",
              "      <td>-2.058219</td>\n",
              "      <td>-0.109889</td>\n",
              "      <td>-4.215711</td>\n",
              "      <td>-4.633144</td>\n",
              "      <td>-2.541258</td>\n",
              "      <td>16.392447</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2  ...        24      score  cluster\n",
              "0    0.490850  0.599752  0.123036  ... -0.138860   2.269521        0\n",
              "1    0.953400 -0.669948  0.472268  ...  0.953353   2.787287        0\n",
              "2   -0.556078  0.498778 -0.344837  ...  1.212759   2.775147        0\n",
              "3    0.406306  0.563676  0.102176  ...  0.586926   2.932246        0\n",
              "4    0.113211 -0.131414 -0.428576  ...  0.201611   2.739236        0\n",
              "..        ...       ...       ...  ...       ...        ...      ...\n",
              "495 -0.458204 -1.480846 -1.627822  ... -3.791009  13.221239        1\n",
              "496 -1.948558 -3.832460 -4.083134  ... -2.332015  16.189307        1\n",
              "497 -4.604396 -3.606876 -0.127779  ... -4.335684  15.799958        1\n",
              "498 -2.016319 -0.507405 -3.842131  ... -4.068186  13.040258        1\n",
              "499 -1.482275 -0.728515 -4.202111  ... -2.541258  16.392447        1\n",
              "\n",
              "[500 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "W_LQQqFxfSsp",
        "outputId": "6f6ce532-74fb-45b5-94d7-a6305e9eabc3"
      },
      "source": [
        "df_test.groupby('cluster').mean()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cluster</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.248812</td>\n",
              "      <td>0.235591</td>\n",
              "      <td>0.254013</td>\n",
              "      <td>0.239429</td>\n",
              "      <td>0.239101</td>\n",
              "      <td>0.255499</td>\n",
              "      <td>0.257207</td>\n",
              "      <td>0.24438</td>\n",
              "      <td>0.248004</td>\n",
              "      <td>0.251948</td>\n",
              "      <td>0.240887</td>\n",
              "      <td>0.260996</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.255338</td>\n",
              "      <td>0.259391</td>\n",
              "      <td>0.236949</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>0.251469</td>\n",
              "      <td>0.259140</td>\n",
              "      <td>0.249625</td>\n",
              "      <td>0.235669</td>\n",
              "      <td>0.244511</td>\n",
              "      <td>0.237602</td>\n",
              "      <td>0.246901</td>\n",
              "      <td>0.249373</td>\n",
              "      <td>2.589860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.239305</td>\n",
              "      <td>-2.120321</td>\n",
              "      <td>-2.286113</td>\n",
              "      <td>-2.154863</td>\n",
              "      <td>-2.151912</td>\n",
              "      <td>-2.299489</td>\n",
              "      <td>-2.314860</td>\n",
              "      <td>-2.19942</td>\n",
              "      <td>-2.232040</td>\n",
              "      <td>-2.267535</td>\n",
              "      <td>-2.167980</td>\n",
              "      <td>-2.348960</td>\n",
              "      <td>-2.259004</td>\n",
              "      <td>-2.298042</td>\n",
              "      <td>-2.334521</td>\n",
              "      <td>-2.132539</td>\n",
              "      <td>-2.229637</td>\n",
              "      <td>-2.263223</td>\n",
              "      <td>-2.332263</td>\n",
              "      <td>-2.246622</td>\n",
              "      <td>-2.121021</td>\n",
              "      <td>-2.200595</td>\n",
              "      <td>-2.138417</td>\n",
              "      <td>-2.222110</td>\n",
              "      <td>-2.244355</td>\n",
              "      <td>14.163883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0         1         2  ...        23        24      score\n",
              "cluster                                ...                               \n",
              "0        0.248812  0.235591  0.254013  ...  0.246901  0.249373   2.589860\n",
              "1       -2.239305 -2.120321 -2.286113  ... -2.222110 -2.244355  14.163883\n",
              "\n",
              "[2 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JddhewLcif9o"
      },
      "source": [
        "The following output shows the mean variable values in each cluster. The values of Cluster ‘1’ (the abnormal cluster) is quite different from those of Cluster ‘0’ (the normal cluster). The “score” values show the average distance of those observations to others. A high “score” means that observation is far away from the norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSIAOGZoi4Af"
      },
      "source": [
        "# Model 2— Step 1, 2 — Build the Model & Determine the Cut Point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KtxgfLtxiXV7",
        "outputId": "045a84af-142c-4d7e-e243-15c80ba6aa17"
      },
      "source": [
        "clf2 = AutoEncoder(hidden_neurons =[25, 10,2, 10, 25])\n",
        "clf2.fit(X_train)\n",
        "\n",
        "# Predict the anomaly scores\n",
        "y_test_scores = clf2.decision_function(X_test)  \n",
        "y_test_scores = pd.Series(y_test_scores)\n",
        "\n",
        "# Plot the histogram\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_test_scores, bins='auto')  \n",
        "plt.title(\"Histogram for Model Clf2 Anomaly Scores\")\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 22        \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                30        \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 25)                275       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 25)                650       \n",
            "=================================================================\n",
            "Total params: 3,187\n",
            "Trainable params: 3,187\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "15/15 [==============================] - 2s 34ms/step - loss: 3.6834 - val_loss: 3.1012\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.1202 - val_loss: 2.7327\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.5784 - val_loss: 2.5100\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2738 - val_loss: 2.3359\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.3138 - val_loss: 2.1761\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2890 - val_loss: 2.0285\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.0137 - val_loss: 1.9191\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1186 - val_loss: 1.8338\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7760 - val_loss: 1.7656\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6547 - val_loss: 1.7116\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5756 - val_loss: 1.6653\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5919 - val_loss: 1.6244\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5136 - val_loss: 1.5894\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5817 - val_loss: 1.5568\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5334 - val_loss: 1.5304\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5556 - val_loss: 1.5058\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5289 - val_loss: 1.4825\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2297 - val_loss: 1.4630\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4534 - val_loss: 1.4438\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5357 - val_loss: 1.4251\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2942 - val_loss: 1.4071\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4044 - val_loss: 1.3911\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2704 - val_loss: 1.3773\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3871 - val_loss: 1.3636\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2315 - val_loss: 1.3496\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2785 - val_loss: 1.3388\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2346 - val_loss: 1.3283\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4254 - val_loss: 1.3174\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1722 - val_loss: 1.3078\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1818 - val_loss: 1.2979\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3455 - val_loss: 1.2906\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2416 - val_loss: 1.2830\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2540 - val_loss: 1.2756\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2988 - val_loss: 1.2693\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3964 - val_loss: 1.2635\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3499 - val_loss: 1.2563\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 1.2803 - val_loss: 1.2507\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1920 - val_loss: 1.2446\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1360 - val_loss: 1.2397\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2379 - val_loss: 1.2350\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0175 - val_loss: 1.2293\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1313 - val_loss: 1.2238\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1737 - val_loss: 1.2213\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2033 - val_loss: 1.2164\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2373 - val_loss: 1.2119\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1423 - val_loss: 1.2073\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2933 - val_loss: 1.2049\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0608 - val_loss: 1.2006\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0299 - val_loss: 1.1965\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0544 - val_loss: 1.1936\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1450 - val_loss: 1.1908\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2790 - val_loss: 1.1882\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1954 - val_loss: 1.1853\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0329 - val_loss: 1.1818\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0479 - val_loss: 1.1794\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0686 - val_loss: 1.1772\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0977 - val_loss: 1.1759\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1340 - val_loss: 1.1727\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3126 - val_loss: 1.1718\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1560 - val_loss: 1.1698\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9734 - val_loss: 1.1673\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3048 - val_loss: 1.1658\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1664 - val_loss: 1.1648\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0394 - val_loss: 1.1615\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0167 - val_loss: 1.1591\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0956 - val_loss: 1.1586\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1280 - val_loss: 1.1563\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1232 - val_loss: 1.1570\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 1.0431 - val_loss: 1.1523\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1195 - val_loss: 1.1513\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0668 - val_loss: 1.1485\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0664 - val_loss: 1.1505\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1219 - val_loss: 1.1487\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0914 - val_loss: 1.1468\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0725 - val_loss: 1.1452\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0696 - val_loss: 1.1433\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9881 - val_loss: 1.1414\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0219 - val_loss: 1.1405\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1705 - val_loss: 1.1371\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1161 - val_loss: 1.1380\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9978 - val_loss: 1.1353\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9856 - val_loss: 1.1335\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1138 - val_loss: 1.1341\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0493 - val_loss: 1.1334\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0678 - val_loss: 1.1335\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1024 - val_loss: 1.1311\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0523 - val_loss: 1.1299\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0125 - val_loss: 1.1290\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0590 - val_loss: 1.1280\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0554 - val_loss: 1.1262\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1518 - val_loss: 1.1256\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0355 - val_loss: 1.1280\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0979 - val_loss: 1.1224\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0851 - val_loss: 1.1233\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0495 - val_loss: 1.1217\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1210 - val_loss: 1.1197\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1802 - val_loss: 1.1191\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1171 - val_loss: 1.1177\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0276 - val_loss: 1.1172\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0689 - val_loss: 1.1163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "mAX52g1xiXYX",
        "outputId": "edfe747d-a138-4a93-db6d-8da1faa96aab"
      },
      "source": [
        "df_test = X_test.copy()\n",
        "df_test['score'] = y_test_scores\n",
        "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
        "df_test['cluster'].value_counts()\n",
        "df_test.groupby('cluster').mean()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cluster</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.248812</td>\n",
              "      <td>0.235591</td>\n",
              "      <td>0.254013</td>\n",
              "      <td>0.239429</td>\n",
              "      <td>0.239101</td>\n",
              "      <td>0.255499</td>\n",
              "      <td>0.257207</td>\n",
              "      <td>0.24438</td>\n",
              "      <td>0.248004</td>\n",
              "      <td>0.251948</td>\n",
              "      <td>0.240887</td>\n",
              "      <td>0.260996</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.255338</td>\n",
              "      <td>0.259391</td>\n",
              "      <td>0.236949</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>0.251469</td>\n",
              "      <td>0.259140</td>\n",
              "      <td>0.249625</td>\n",
              "      <td>0.235669</td>\n",
              "      <td>0.244511</td>\n",
              "      <td>0.237602</td>\n",
              "      <td>0.246901</td>\n",
              "      <td>0.249373</td>\n",
              "      <td>2.704472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.239305</td>\n",
              "      <td>-2.120321</td>\n",
              "      <td>-2.286113</td>\n",
              "      <td>-2.154863</td>\n",
              "      <td>-2.151912</td>\n",
              "      <td>-2.299489</td>\n",
              "      <td>-2.314860</td>\n",
              "      <td>-2.19942</td>\n",
              "      <td>-2.232040</td>\n",
              "      <td>-2.267535</td>\n",
              "      <td>-2.167980</td>\n",
              "      <td>-2.348960</td>\n",
              "      <td>-2.259004</td>\n",
              "      <td>-2.298042</td>\n",
              "      <td>-2.334521</td>\n",
              "      <td>-2.132539</td>\n",
              "      <td>-2.229637</td>\n",
              "      <td>-2.263223</td>\n",
              "      <td>-2.332263</td>\n",
              "      <td>-2.246622</td>\n",
              "      <td>-2.121021</td>\n",
              "      <td>-2.200595</td>\n",
              "      <td>-2.138417</td>\n",
              "      <td>-2.222110</td>\n",
              "      <td>-2.244355</td>\n",
              "      <td>13.393525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0         1         2  ...        23        24      score\n",
              "cluster                                ...                               \n",
              "0        0.248812  0.235591  0.254013  ...  0.246901  0.249373   2.704472\n",
              "1       -2.239305 -2.120321 -2.286113  ... -2.222110 -2.244355  13.393525\n",
              "\n",
              "[2 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvUGZJPdjMk4"
      },
      "source": [
        "# Model 3— Step 1,2,3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_YPWV9m4iXbL",
        "outputId": "7c3eac05-4a3e-4101-9969-0084741ec27f"
      },
      "source": [
        "# Step 1: Build the model\n",
        "clf3 = AutoEncoder(hidden_neurons =[25, 15, 10, 2, 10,15, 25])\n",
        "clf3.fit(X_train)\n",
        "\n",
        "# Predict the anomaly scores\n",
        "y_test_scores = clf3.decision_function(X_test)  \n",
        "y_test_scores = pd.Series(y_test_scores)\n",
        "\n",
        "# Step 2: Determine the cut point\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_test_scores, bins='auto')  \n",
        "plt.title(\"Histogram with Model Clf3 Anomaly Scores\")\n",
        "plt.show()\n",
        "\n",
        "df_test = X_test.copy()\n",
        "df_test['score'] = y_test_scores\n",
        "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
        "df_test['cluster'].value_counts()\n",
        "\n",
        "# Step 3: Get the summary statistics by cluster\n",
        "df_test.groupby('cluster').mean()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 25)                650       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 15)                390       \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                160       \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 2)                 22        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                30        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 15)                165       \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 25)                400       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 25)                650       \n",
            "=================================================================\n",
            "Total params: 3,767\n",
            "Trainable params: 3,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "15/15 [==============================] - 2s 43ms/step - loss: 3.9041 - val_loss: 3.5559\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.0681 - val_loss: 3.1359\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.6085 - val_loss: 2.9076\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2884 - val_loss: 2.7239\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.5132 - val_loss: 2.5762\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.0464 - val_loss: 2.4431\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.0400 - val_loss: 2.3115\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9008 - val_loss: 2.2052\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.8577 - val_loss: 2.1126\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.8263 - val_loss: 2.0389\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5659 - val_loss: 1.9778\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4917 - val_loss: 1.9203\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.5200 - val_loss: 1.8767\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.5666 - val_loss: 1.8359\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7107 - val_loss: 1.8017\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.4900 - val_loss: 1.7729\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4909 - val_loss: 1.7424\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2554 - val_loss: 1.7179\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.3076 - val_loss: 1.6921\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3541 - val_loss: 1.6704\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2652 - val_loss: 1.6502\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.4138 - val_loss: 1.6324\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.3464 - val_loss: 1.6153\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.3740 - val_loss: 1.5987\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4784 - val_loss: 1.5831\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2008 - val_loss: 1.5694\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1991 - val_loss: 1.5555\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2195 - val_loss: 1.5434\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0655 - val_loss: 1.5317\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2741 - val_loss: 1.5205\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1699 - val_loss: 1.5106\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.3762 - val_loss: 1.5001\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2681 - val_loss: 1.4911\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1769 - val_loss: 1.4808\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1877 - val_loss: 1.4731\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1424 - val_loss: 1.4664\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 1.2123 - val_loss: 1.4579\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0464 - val_loss: 1.4497\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3343 - val_loss: 1.4431\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1353 - val_loss: 1.4354\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2607 - val_loss: 1.4300\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0910 - val_loss: 1.4258\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0982 - val_loss: 1.4203\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1764 - val_loss: 1.4152\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.0453 - val_loss: 1.4111\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.3138 - val_loss: 1.4060\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0559 - val_loss: 1.4024\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1337 - val_loss: 1.3970\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2381 - val_loss: 1.3930\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.1951 - val_loss: 1.3885\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0765 - val_loss: 1.3856\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0222 - val_loss: 1.3823\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0956 - val_loss: 1.3788\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1079 - val_loss: 1.3750\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0668 - val_loss: 1.3723\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0480 - val_loss: 1.3695\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.9480 - val_loss: 1.3668\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0713 - val_loss: 1.3631\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0309 - val_loss: 1.3607\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0760 - val_loss: 1.3587\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0095 - val_loss: 1.3556\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0456 - val_loss: 1.3534\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.9919 - val_loss: 1.3513\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0679 - val_loss: 1.3493\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0377 - val_loss: 1.3464\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1508 - val_loss: 1.3443\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1123 - val_loss: 1.3426\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0428 - val_loss: 1.3410\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0708 - val_loss: 1.3394\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0525 - val_loss: 1.3378\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1450 - val_loss: 1.3356\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1733 - val_loss: 1.3341\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1205 - val_loss: 1.3326\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1831 - val_loss: 1.3307\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2206 - val_loss: 1.3292\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1625 - val_loss: 1.3275\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.9707 - val_loss: 1.3258\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0812 - val_loss: 1.3240\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.9895 - val_loss: 1.3228\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1467 - val_loss: 1.3214\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0336 - val_loss: 1.3200\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.0784 - val_loss: 1.3188\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0412 - val_loss: 1.3165\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1275 - val_loss: 1.3151\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.8448 - val_loss: 1.3144\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0690 - val_loss: 1.3132\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 1.0738 - val_loss: 1.3121\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1384 - val_loss: 1.3108\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0189 - val_loss: 1.3099\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0367 - val_loss: 1.3089\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0506 - val_loss: 1.3078\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0736 - val_loss: 1.3064\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0263 - val_loss: 1.3055\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1610 - val_loss: 1.3043\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1650 - val_loss: 1.3026\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0773 - val_loss: 1.3016\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1305 - val_loss: 1.3009\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.9787 - val_loss: 1.3004\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0002 - val_loss: 1.2998\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1281 - val_loss: 1.2990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cluster</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.248812</td>\n",
              "      <td>0.235591</td>\n",
              "      <td>0.254013</td>\n",
              "      <td>0.239429</td>\n",
              "      <td>0.239101</td>\n",
              "      <td>0.255499</td>\n",
              "      <td>0.257207</td>\n",
              "      <td>0.24438</td>\n",
              "      <td>0.248004</td>\n",
              "      <td>0.251948</td>\n",
              "      <td>0.240887</td>\n",
              "      <td>0.260996</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.255338</td>\n",
              "      <td>0.259391</td>\n",
              "      <td>0.236949</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>0.251469</td>\n",
              "      <td>0.259140</td>\n",
              "      <td>0.249625</td>\n",
              "      <td>0.235669</td>\n",
              "      <td>0.244511</td>\n",
              "      <td>0.237602</td>\n",
              "      <td>0.246901</td>\n",
              "      <td>0.249373</td>\n",
              "      <td>2.718697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.239305</td>\n",
              "      <td>-2.120321</td>\n",
              "      <td>-2.286113</td>\n",
              "      <td>-2.154863</td>\n",
              "      <td>-2.151912</td>\n",
              "      <td>-2.299489</td>\n",
              "      <td>-2.314860</td>\n",
              "      <td>-2.19942</td>\n",
              "      <td>-2.232040</td>\n",
              "      <td>-2.267535</td>\n",
              "      <td>-2.167980</td>\n",
              "      <td>-2.348960</td>\n",
              "      <td>-2.259004</td>\n",
              "      <td>-2.298042</td>\n",
              "      <td>-2.334521</td>\n",
              "      <td>-2.132539</td>\n",
              "      <td>-2.229637</td>\n",
              "      <td>-2.263223</td>\n",
              "      <td>-2.332263</td>\n",
              "      <td>-2.246622</td>\n",
              "      <td>-2.121021</td>\n",
              "      <td>-2.200595</td>\n",
              "      <td>-2.138417</td>\n",
              "      <td>-2.222110</td>\n",
              "      <td>-2.244355</td>\n",
              "      <td>13.488630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0         1         2  ...        23        24      score\n",
              "cluster                                ...                               \n",
              "0        0.248812  0.235591  0.254013  ...  0.246901  0.249373   2.718697\n",
              "1       -2.239305 -2.120321 -2.286113  ... -2.222110 -2.244355  13.488630\n",
              "\n",
              "[2 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1u2mB26jUaQ"
      },
      "source": [
        "# Aggregate to Achieve Model Stability\n",
        "\n",
        "You may wonder why I go with a great length to produce the three models. Here let me reveal the reason: Although unsupervised techniques are powerful in detecting outliers, they are prone to overfitting and unstable results. The solution is to train multiple models then aggregate the scores. In the aggregation process, you still will follow Step 2 and 3 like before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73tRm02Uke7y"
      },
      "source": [
        "## Average Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdRZNHtRiXd4"
      },
      "source": [
        "# Put all the predictions in a data frame\n",
        "from pyod.models.combination import aom, moa, average, maximization\n",
        "\n",
        "# Put all the predictions in a data frame\n",
        "train_scores = pd.DataFrame({'clf1': clf1.decision_scores_,\n",
        "                             'clf2': clf2.decision_scores_,\n",
        "                             'clf3': clf3.decision_scores_\n",
        "                            })\n",
        "\n",
        "test_scores  = pd.DataFrame({'clf1': clf1.decision_function(X_test),\n",
        "                             'clf2': clf2.decision_function(X_test),\n",
        "                             'clf3': clf3.decision_function(X_test) \n",
        "                            })"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QYiuEHjkjX1o",
        "outputId": "af732f50-f73e-42ce-8ac8-155baa1ac892"
      },
      "source": [
        "train_scores"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clf1</th>\n",
              "      <th>clf2</th>\n",
              "      <th>clf3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.581169</td>\n",
              "      <td>2.747950</td>\n",
              "      <td>2.760979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.504626</td>\n",
              "      <td>2.666813</td>\n",
              "      <td>2.689774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.634121</td>\n",
              "      <td>2.789920</td>\n",
              "      <td>2.799378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.489325</td>\n",
              "      <td>2.590420</td>\n",
              "      <td>2.601639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.227009</td>\n",
              "      <td>2.240596</td>\n",
              "      <td>2.257112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>14.964735</td>\n",
              "      <td>14.099149</td>\n",
              "      <td>14.257848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>15.537908</td>\n",
              "      <td>14.579711</td>\n",
              "      <td>14.842728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>14.778036</td>\n",
              "      <td>14.067678</td>\n",
              "      <td>14.096280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>13.327944</td>\n",
              "      <td>12.602299</td>\n",
              "      <td>12.655603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>12.711807</td>\n",
              "      <td>11.978032</td>\n",
              "      <td>12.021901</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          clf1       clf2       clf3\n",
              "0     2.581169   2.747950   2.760979\n",
              "1     2.504626   2.666813   2.689774\n",
              "2     2.634121   2.789920   2.799378\n",
              "3     2.489325   2.590420   2.601639\n",
              "4     2.227009   2.240596   2.257112\n",
              "..         ...        ...        ...\n",
              "495  14.964735  14.099149  14.257848\n",
              "496  15.537908  14.579711  14.842728\n",
              "497  14.778036  14.067678  14.096280\n",
              "498  13.327944  12.602299  12.655603\n",
              "499  12.711807  11.978032  12.021901\n",
              "\n",
              "[500 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr3hbZ-SjX7H"
      },
      "source": [
        "# Although we did standardization before, it was for the variables.\n",
        "# Now we do the standardization for the decision scores\n",
        "train_scores_norm = StandardScaler().fit_transform(train_scores)\n",
        "test_scores_norm = StandardScaler().fit_transform(test_scores)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPsR8_ecjX96"
      },
      "source": [
        "# Combination by average\n",
        "y_by_average = average(test_scores_norm)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "VqsV5YmRjYAi",
        "outputId": "af9ce6da-e8c9-4f98-f5fc-26d66f032dcc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_by_average, bins='auto')  # arguments are passed to np.histogram\n",
        "plt.title(\"Combination by average\")\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI1sGtGjkXOB"
      },
      "source": [
        "It appears we can identify those >=0.0 as the outliers. Our example identifies 50 outliers (not shown)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA4ZJ8cDkY1u",
        "outputId": "f8233798-6897-45ae-ca4f-034de79f1151"
      },
      "source": [
        "df_test = pd.DataFrame(X_test)\n",
        "df_test['y_by_average_score'] = y_by_average\n",
        "df_test['y_by_average_cluster'] = np.where(df_test['y_by_average_score']<0, 0, 1)\n",
        "df_test['y_by_average_cluster'].value_counts()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    450\n",
              "1     50\n",
              "Name: y_by_average_cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "WjKFLgaOkYzI",
        "outputId": "d5d76787-2090-4806-d4f3-d1228aa7d622"
      },
      "source": [
        "df_test.groupby('y_by_average_cluster').mean()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>y_by_average_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_by_average_cluster</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.248812</td>\n",
              "      <td>0.235591</td>\n",
              "      <td>0.254013</td>\n",
              "      <td>0.239429</td>\n",
              "      <td>0.239101</td>\n",
              "      <td>0.255499</td>\n",
              "      <td>0.257207</td>\n",
              "      <td>0.24438</td>\n",
              "      <td>0.248004</td>\n",
              "      <td>0.251948</td>\n",
              "      <td>0.240887</td>\n",
              "      <td>0.260996</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.255338</td>\n",
              "      <td>0.259391</td>\n",
              "      <td>0.236949</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>0.251469</td>\n",
              "      <td>0.259140</td>\n",
              "      <td>0.249625</td>\n",
              "      <td>0.235669</td>\n",
              "      <td>0.244511</td>\n",
              "      <td>0.237602</td>\n",
              "      <td>0.246901</td>\n",
              "      <td>0.249373</td>\n",
              "      <td>-0.328636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.239305</td>\n",
              "      <td>-2.120321</td>\n",
              "      <td>-2.286113</td>\n",
              "      <td>-2.154863</td>\n",
              "      <td>-2.151912</td>\n",
              "      <td>-2.299489</td>\n",
              "      <td>-2.314860</td>\n",
              "      <td>-2.19942</td>\n",
              "      <td>-2.232040</td>\n",
              "      <td>-2.267535</td>\n",
              "      <td>-2.167980</td>\n",
              "      <td>-2.348960</td>\n",
              "      <td>-2.259004</td>\n",
              "      <td>-2.298042</td>\n",
              "      <td>-2.334521</td>\n",
              "      <td>-2.132539</td>\n",
              "      <td>-2.229637</td>\n",
              "      <td>-2.263223</td>\n",
              "      <td>-2.332263</td>\n",
              "      <td>-2.246622</td>\n",
              "      <td>-2.121021</td>\n",
              "      <td>-2.200595</td>\n",
              "      <td>-2.138417</td>\n",
              "      <td>-2.222110</td>\n",
              "      <td>-2.244355</td>\n",
              "      <td>2.957725</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             0         1  ...        24  y_by_average_score\n",
              "y_by_average_cluster                      ...                              \n",
              "0                     0.248812  0.235591  ...  0.249373           -0.328636\n",
              "1                    -2.239305 -2.120321  ... -2.244355            2.957725\n",
              "\n",
              "[2 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P3LsbXykpxZ"
      },
      "source": [
        "## Maximum of Maximum Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "3GwiolCxkYwo",
        "outputId": "bf673688-935d-472a-a2be-ea9c323da685"
      },
      "source": [
        "# Combination by max\n",
        "y_by_maximization = maximization(test_scores_norm)\n",
        "             \n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_by_maximization, bins='auto')  # arguments are passed to np.histogram\n",
        "plt.title(\"Combination by max\")\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TuLl6U4kYuN",
        "outputId": "12517f64-9a36-48f3-f291-4dfa44346bdd"
      },
      "source": [
        "df_test = pd.DataFrame(X_test)\n",
        "df_test['y_by_maximization_score'] = y_by_maximization\n",
        "df_test['y_by_maximization_cluster'] = np.where(df_test['y_by_maximization_score']<0, 0, 1)\n",
        "df_test['y_by_maximization_cluster'].value_counts()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    450\n",
              "1     50\n",
              "Name: y_by_maximization_cluster, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "smzVaKPekYrV",
        "outputId": "8d1bc950-5c16-449b-94a7-bca2f250344b"
      },
      "source": [
        "df_test.groupby('y_by_maximization_cluster').mean()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>y_by_average_score</th>\n",
              "      <th>y_by_average_cluster</th>\n",
              "      <th>y_by_maximization_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_by_maximization_cluster</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.248812</td>\n",
              "      <td>0.235591</td>\n",
              "      <td>0.254013</td>\n",
              "      <td>0.239429</td>\n",
              "      <td>0.239101</td>\n",
              "      <td>0.255499</td>\n",
              "      <td>0.257207</td>\n",
              "      <td>0.24438</td>\n",
              "      <td>0.248004</td>\n",
              "      <td>0.251948</td>\n",
              "      <td>0.240887</td>\n",
              "      <td>0.260996</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.255338</td>\n",
              "      <td>0.259391</td>\n",
              "      <td>0.236949</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>0.251469</td>\n",
              "      <td>0.259140</td>\n",
              "      <td>0.249625</td>\n",
              "      <td>0.235669</td>\n",
              "      <td>0.244511</td>\n",
              "      <td>0.237602</td>\n",
              "      <td>0.246901</td>\n",
              "      <td>0.249373</td>\n",
              "      <td>-0.328636</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.309781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.239305</td>\n",
              "      <td>-2.120321</td>\n",
              "      <td>-2.286113</td>\n",
              "      <td>-2.154863</td>\n",
              "      <td>-2.151912</td>\n",
              "      <td>-2.299489</td>\n",
              "      <td>-2.314860</td>\n",
              "      <td>-2.19942</td>\n",
              "      <td>-2.232040</td>\n",
              "      <td>-2.267535</td>\n",
              "      <td>-2.167980</td>\n",
              "      <td>-2.348960</td>\n",
              "      <td>-2.259004</td>\n",
              "      <td>-2.298042</td>\n",
              "      <td>-2.334521</td>\n",
              "      <td>-2.132539</td>\n",
              "      <td>-2.229637</td>\n",
              "      <td>-2.263223</td>\n",
              "      <td>-2.332263</td>\n",
              "      <td>-2.246622</td>\n",
              "      <td>-2.121021</td>\n",
              "      <td>-2.200595</td>\n",
              "      <td>-2.138417</td>\n",
              "      <td>-2.222110</td>\n",
              "      <td>-2.244355</td>\n",
              "      <td>2.957725</td>\n",
              "      <td>1</td>\n",
              "      <td>2.979374</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  0  ...  y_by_maximization_score\n",
              "y_by_maximization_cluster            ...                         \n",
              "0                          0.248812  ...                -0.309781\n",
              "1                         -2.239305  ...                 2.979374\n",
              "\n",
              "[2 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpgWXiL6kxZk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnYlrld2kxbv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nfwurYRkxeS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLP_q2eEkxgn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81fUvhWskxjN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}